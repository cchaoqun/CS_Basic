# 总结

## 如何选主

1. 首先是初始化整个的Raft集群, 
   1. 整个集群Raft结点的数组
   2. 日志复制, 应用日志到本地状态机的管道
   3. 选举, 同步日志, 应用日志到本地状态机的计时器
2. 每个结点都有三种可能的状态 跟随者, 候选者, 领主
3. 一开始大家都是跟随者, 选举计时器到时间的时候, 结点会主动发起一个选举
   1. 当前结点的term+1表示进入一个新的纪元
   2. 向其他的raft结点通过管道发送消息, 要求投票给自己
4. 因为每个raft结点的选举计时器时间都是150~300ms之间的随机数, 所以避免了大部分结点同时发起选举,并要求投票, 这样会造成投票的分裂导致不能尽快选出领主
5. 收到要求投票消息的结点会首先判断请求结点的信息
   1. 如果对方结点的term纪元, 最新的日志下标, 日志term纪元 落后于自己, 就不会投票给这个结点, 因为这样保证了最后的领主一定是日志领先于超过一半的结点
   2. 或者当前结点已经投票了就不会再投了, 保证每个term纪元每个结点投票一次 因为只有收到超过半数选票的结点才会当选, 保证了只会产生一个领主
   3. 如果对方都领先于自己, 自己更新term纪元信息, 改变身份为跟随者
6. 如果发起投票的结点收到了超过半数的选票, 就会当选, 
   1. 首先重置自己的选举计时器, 防止刚一当选就因为计时器到时间又开始选举
   2. 向所有结点发送一个心跳, 让其他节点重置选举计时器, 防止其他节点又马上开始新的一轮选举
7. 领主会间断的通过管道发送AppendEntries消息到其他所有的跟随者, 保证自己领主的地位, 因为受到这个消息的结点就会重置自己的选举计时器



## 如何实现心跳机制 和日志复制

- 当某个结点的AppendEntriesTimer到时间, 就会向这个结点发送AppendEntries消息来维持自己的领主地位, 
- 同时, 领主的日志信息也会被发送给结点要求结点复制自己的日志, 结点可以根据收到的领主消息判断
  - 如果领主的term任期落后于自己, 自己不会重置选举计时器, 这样计时器到时就会开始新的选举因为当前的领主已经不是最新的了
  - 如果领主领先, 就重置自己的选举计时器
  - 同时结点会检查自己的日志和领主的日志是否是一致的, 
    - 如果一致, 就会将当前最新的一条日志复制到自己的本地日志集合
    - 不一致说明, 之前有缺失的日志, 拒绝复制, 
      - 因为需要保证每个结点最终日志集合顺序是一致的这样, 当把这些日志都应用到本地状态机以后所有结点的状态都是一样的
      - 结点会根据领主的日志信息, 找到最后一个匹配的日志位置, 然后返回这个位置到nextIndex表示下一条需要发送给该follower 的日志索引
      - 这样领主收到回复的时候, 知道了结点和自己的同步状态, 下次发送这个位置以后的日志让其更新到领主一致的状态



## 如何处理日志不一致

**Raft 强制要求 follower 必须复制 leader 的日志集合来解决不一致问题。**

也就是说，follower 节点上任何与 leader 不一致的日志，都会被 leader 节点上的日志所覆盖。这并不会产生什么问题，因为某些选举上的限制，如果 follower 上的日志与 leader 不一致，那么该日志在 follower 上**一定是未提交的**。未提交的日志并不会应用到状态机，也不会被外部的客户端感知到。

要使得 follower 的日志集合跟自己保持完全一致，leader 必须先找到二者间**最后一次**达成一致的地方。因为一旦这条日志达成一致，在这之前的日志一定也都一致（回忆下前文）。这个确认操作是在 AppendEntries RPC 的一致性检查步骤完成的。

Leader 针对每个 follower 都维护一个 **next index**，表示下一条需要发送给该follower 的日志索引。当一个 leader 刚刚上任时，它初始化所有 next index 值为自己最后一条日志的 index+1。但凡某个 follower 的日志跟 leader 不一致，那么下次 AppendEntries RPC 的一致性检查就会失败。在被 follower 拒绝这次 Append Entries RPC 后，leader 会减少 next index 的值并进行重试。

最终一定会存在一个 next index 使得 leader 和 follower 在这之前的日志都保持一致。极端情况下 next index 为1，表示 follower 没有任何日志与 leader 一致，leader 必须从第一条日志开始同步。

针对每个 follower，一旦确定了 next index 的值，leader 便开始从该 index 同步日志，follower 会删除掉现存的不一致的日志，保留 leader 最新同步过来的。

整个集群的日志会在这个简单的机制下自动趋于一致。此外要注意，**leader 从来不会覆盖或者删除自己的日志**，而是强制 follower 与它保持一致。

这就要求集群票选出的 leader 一定要具备“日志的正确性”，这也就关联到了前文提到的：选举上的限制。





## KVServer如何和Raft一起实现的一致性

- 对于每一个KVServer结点, 都有一个与之对于的Raft结点, 
- 所有的Get/Append/Put请求都会被封装成日志, 由KVServer传递给Raft结点去完成在所有的结点上实现同步
- 所以不能让所有的KVServer都处理请求, 只有KVServer对应的Raft是领主的服务结点才可以处理客户端的请求
- Raft领主收到服务器传递过来的请求开始要求所有的Raft结点复制当前的请求对应的日志
- 复制完成这个请求就可以被提交到本地状态机上, 并持久化到磁盘了
- 通过 Client->Server->Raft 的方式实现了各个服务器结点的状态一致性



## reconfiguration如何实现分片的迁移

- 服务器结点可能会存在三种状态的变化
  - Join 新的服务器结点加入当前集群集合, 可能是之前宕机的服务器重新恢复加入
  - Leave 旧的服务器结点因为网络或宕机离开了当前集群集合
  - move 需要在两个服务器结点之间迁移部分结点
- 都会因为配置的变化导致当前的配置需要更新, 那么shardMaster会先获取之前的旧配置, 配置编号+1
  - Join就需要复制完旧的配置分片分配以后, 将分片的分配包括新的结点在内使得分片在各个结点上的分配尽可能平均达到负载均衡
  - Leave 就需要将离开的服务器结点负责处理的分片平均的分配到剩余的服务器结点
  - move 指定一个新的分片-服务器结点的对应关系
- 均衡分配的过程是把所有当前存在的服务器结点放入一个数据结构存储起来 然后平均的分配分片到这些结点上





## shardMaster如何处理客户端的请求

- 获得负责请求的key所在的分片s的集群id,  s := key2shard(r.Key)
- 如果这个集群当前不负责分片s, 或者 正在转移分片 
  1. kv.shardState[s] == NOTINCHARGE || kv.shardState[s] == TRANSFERRING 
  2. 通过这个请求的 idx term得到这个请求唯一的sigChan ,发送 WRONG_GROUP消息, 删除这个请求对应的sigChan
- 分片正确, 但是正在等待属于这个集群的分片从其他的集群转移过来
  1. kv.shardState[s] == RECEIVING
  2. 还是通过这个请求的sigChan 发送FAILED_REQUEST 因为正在等待分片无法处理请求
- 检查重复请求
  1. r.OpType != GET && r.Seq <= kv.clientSeq\[s][r.Uid] 
  2. 是修改数据库的请求, 并且这个client的Seq<这个client当前最大的Seq, 说明这是一个已经提交过的请求, 
  3. 返回 DUPLICATE_REQUEST 不处理, 并删除这个请求对应的sigChan
- 是合法请求, 更新这个client的最新请求Seq kv.clientSeq\[s][r.Uid] = r.Seq
  1. 根据GET/PUT/APPEND请求执行对应对数据库的操作  kv.shardedData\[s][r.Key]
  2. kv.shardData\[分片index][请求key]
  3. 通过sigChan 返回SUCCESS消息,  删除sigChan



## reconfiguration的时候客户端请求同步到达如何处理

- 客户端请求的这个服务器结点处于正在转移的状态
- 返回要求客户端请求shardMaster最新的配置信息, 去请求当前分片数据迁移后所在的新的服务器结点



## Raft结点之间如何通过RPC通信的











## 如何实现日志复制

## KVServer如何和Raft一起实现的一致性

## 数据库怎么分片的

## shardMaster如何处理客户端的请求

### reconfiguration的时候客户端请求同步到达如何处理

## shardMaster如何分配shard到各个分片

- 将请求参数转化成Byte数组 通过管道发送给对应的结点
- 获得回复后, 将回复的Byte数组 解析获得对应的数据

# 遇到的问题

## RPC通信问题

- raft结点之间的通信是通过RPC, 但是一开始不了解, 没有按照RPC接口的参数要求按顺序传递参数, 导致接收到反序列数据出错, 得不到正确的数据

## 数据库的请求方存在的重复数据请求多次执行

Cleark发送RPC给一个leader , leader提交了这个日志, 但是马上就断线了, 没有回复Cleark, 但是这个提交的日志会被已经复制过的raft都标记为commit 并且apply到本地状态机, 所以事实上这个Op已经作用到了数据库, 但是因为没有回复Cleark, Cleark还会继续发送这个Op到其他的服务器, 

- 我们需要保证这种情况下, 一个相同的Op只会被执行一次, 即当这个Cleark向其他服务器发起同样的请求的时候, 这个请求不会被执行

- 这样要求我们需要对每个客户端的请求做唯一性的确认, 通过请求Id和ClientId构成唯一的标识, 如果两个都相同, 不会被执行第二次