# 操作系统

## 进程与线程

##  进程和线程还有协程 

- 定义

```
进程是资源（CPU、内存等）分配的基本单位，具有一定独立功能的程序关于某个数据集合上的一次运行活动，进程是系统进行资源分配和调度的一个独立单位。
线程是进程的一个实体，是独立运行和独立调度的基本单位（CPU上真正运行的是线程）。线程自己基本上不拥有系统资源，只拥有一点在运行中必不可少的资源(如程序计数器,一组寄存器和栈)，但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源。
```

- 区别

```
进程是资源分配的基本单位；线程是程序执行的基本单位。
进程拥有自己的资源空间，没启动一个进程，系统就会为它分配地址空间；而线程与CPU资源分配无关，多个线程共享同一进程内的资源，使用相同的地址空间。
一个进程可以包含若干个线程。
（1）进程有自己的独立地址空间，线程没有

（2）进程是资源分配的最小单位，线程是CPU调度的最小单位

（3）进程和线程通信方式不同(线程之间的通信比较方便。同一进程下的线程共享数据（比如全局变量，静态变量），通过这些数据来通信不仅快捷而且方便，当然如何处理好这些访问的同步与互斥正是编写多线程程序的难点。而进程之间的通信只能通过进程通信的方式进行。)

（4）进程上下文切换开销大，线程开销小

（5）一个进程挂掉了不会影响其他进程，而线程挂掉了会影响其他线程

（6）对进程操作一般开销都比较大，对线程开销就小了

```

![1625481112040](面经.assets/1625481112040.png)

![1625481184670](面经.assets/1625481184670.png)

- 优劣

```
线程之间的通信更方便，同一进程下的线程共享全局变量、静态变量等数据，而进程之间的通信需要以通信的方式（Inter Process Communication，IPC)进行。不过如何处理好同步与互斥是编写多线程程序的难点。
线程的调度与切换比进程快很多，同时创建一个线程的开销也比进程要小很多。
但是多进程程序更健壮，多线程程序只要有一个线程死掉，整个进程也死掉了，而一个进程死掉并不会对另外一个进程造成影响，因为进程有自己独立的地址空间。
```

- 通信

```
进程可以通过管道、套接字、信号交互、共享内存、消息队列等等进行通信；而线程本身就会共享内存，指针指向同一个内容，交互很容易。
```





- 进程

```
进程是系统资源分配的最小单位, 系统由一个个进程(程序)组成
一般情况下，包括文本区域（text region）、数据区域（data region）和堆栈（stack region）。

文本区域存储处理器执行的代码
数据区域存储变量和进程执行期间使用的动态分配的内存；
堆栈区域存储着活动过程调用的指令和本地变量。

因此进程的创建和销毁都是相对于系统资源,所以是一种比较昂贵的操作。

一个进程至少具有5种基本状态：初始态、就绪状态、等待（阻塞）状态、执行状态、终止状态。
初始状态：进程刚被创建，由于其他进程正占有CPU资源，所以得不到执行，只能处于初始状态。
就绪状态：只有处于就绪状态的经过调度才能到执行状态
等待状态：进程等待某件事件完成
执行状态：任意时刻处于执行状态的进程只能有一个（对于单核CPU来讲）。
停止状态：进程结束

进程是抢占式的争夺CPU运行自身,而CPU单核的情况下同一时间只能执行一个进程的代码,但是多进程的实现则是通过CPU飞快的切换不同进程,因此使得看上去就像是多个进程在同时进行.

操作系统对进程的控制和管理通过PCB(Processing Control Block)，PCB通常是系统内存占用区中的一个连续存区，它存放着操作系统用于描述进程情况及控制进程运行所需的全部信息(包括：进程标识号,进程状态,进程优先级,文件系统指针以及各个寄存器的内容等)，

无论是在多核还是单核系统中，一个CPU看上去都像是在并发的执行多个进程，这是通过处理器在进程间切换来实现的。
操作系统对把CPU控制权在不同进程之间交换执行的机制称为上下文切换（context switch），即保存当前进程的上下文，恢复新进程的上下文，然后将CPU控制权转移到新进程，新进程就会从上次停止的地方开始。因此，进程是轮流使用CPU的，CPU被若干进程共享，使用某种调度算法来决定何时停止一个进程，并转而为另一个进程提供服务。

通信问题: 由于进程间是隔离的,各自拥有自己的内存内存资源, 因此相对于线程比较安全, 所以不同进程之间的数据只能通过 IPC(Inter-Process Communication) 进行通信共享.
由于进程拥有自己独占的虚拟地址空间，CPU通过地址翻译将虚拟地址转换成真实的物理地址，每个进程只能访问自己的地址空间。因此，在没有其他机制（进程间通信）的辅助下，进程之间是无法共享数据的


```

- 线程

```
线程属于进程
线程-也是操作系统提供的抽象概念，是程序执行中一个单一的顺序控制流程，是程序执行流的最小单元，是处理器调度和分派的基本单位。一个进程可以有一个或多个线程，同一进程中的多个线程将共享该进程中的全部系统资源，如虚拟地址空间，文件描述符和信号处理等等。但同一进程中的多个线程有各自的调用栈和线程本地存储

通信问题:   进程相当于一个容器,而线程而是运行在容器里面的,因此对于容器内的东西,线程是共同享有的,因此线程间的通信可以直接通过全局变量进行通信,但是由此带来的例如多个线程读写同一个地址变量的时候则将带来不可预期的后果,因此这时候引入了各种锁的作用,例如互斥锁等。

同时多线程是不安全的,当一个线程崩溃了,会导致整个进程也崩溃了,即其他线程也挂了,
但多进程而不会,一个进程挂了,另一个进程依然照样运行。

系统利用PCB来完成对进程的控制和管理。同样，系统为线程分配一个线程控制块TCB（Thread Control Block）,将所有用于控制和管理线程的信息记录在线程的控制块中，TCB中通常包括：

线程标志符 一组寄存器 线程运行状态 优先级 线程专有存储区 信号屏蔽

和进程一样，线程同样至少具有五种状态：初始态、就绪状态、等待（阻塞）状态、执行状态和终止状态

```

- 进程 VS 线程

```
进程是资源的分配和调度的独立单元。进程拥有完整的虚拟地址空间，当发生进程切换时，不同的进程拥有不同的虚拟地址空间。而同一进程的多个线程共享同一地址空间（不同进程之间的线程无法共享）

线程是CPU调度的基本单元，一个进程包含若干线程（至少一个线程）。
线程比进程小，基本上不拥有系统资源。线程的创建和销毁所需要的时间比进程小很多
由于线程之间能够共享地址空间，因此，需要考虑同步和互斥操作
一个线程的意外终止会影响整个进程的正常运行，但是一个进程的意外终止不会影响其他的进程的运行。因此，多进程程序安全性更高。

```



- 协程

```
协程是属于线程的。协程程序是在线程里面跑的，因此协程又称微线程和纤程等
协没有线程的上下文切换消耗。协程的调度切换是用户(程序员)手动切换的,因此更加灵活,因此又叫用户空间线程.
原子操作性。由于协程是用户调度的，所以不会出现执行一半的代码片段被强制中断了，因此无需原子操作锁。

协程（Coroutine，又称微线程）是一种比线程更加轻量级的存在，协程不是被操作系统内核所管理，而完全是由程序所控制。协程与线程以及进程的关系见下图所示。

协程可以比作子程序，但执行过程中，子程序内部可中断，然后转而执行别的子程序，在适当的时候再返回来继续执行。协程之间的切换不需要涉及任何系统调用或任何阻塞调用
协程只在一个线程中执行，是子程序之间的切换，发生在用户态上。而且，线程的阻塞状态是由操作系统内核来完成，发生在内核态上，因此协程相比线程节省了线程创建和切换的开销
协程中不存在同时写变量冲突，因此，也就不需要用来守卫关键区块的同步性原语，比如互斥锁、信号量等，并且不需要来自操作系统的支持。

协程适用于IO阻塞且需要大量并发的场景，当发生IO阻塞，由协程的调度器进行调度，通过将数据流yield掉，并且记录当前栈上的数据，阻塞完后立刻再通过线程恢复协程栈，并把阻塞的结果放到这个线程上去运行。


协程（Coroutine，又称微线程）是一种比线程更加轻量级的存在，协程不是被操作系统内核所管理，而完全是由程序所控制。协程与线程以及进程的关系见下图所示。

协程可以比作子程序，但执行过程中，子程序内部可中断，然后转而执行别的子程序，在适当的时候再返回来继续执行。协程之间的切换不需要涉及任何系统调用或任何阻塞调用
协程只在一个线程中执行，是子程序之间的切换，发生在用户态上。而且，线程的阻塞状态是由操作系统内核来完成，发生在内核态上，因此协程相比线程节省了线程创建和切换的开销
协程中不存在同时写变量冲突，因此，也就不需要用来守卫关键区块的同步性原语，比如互斥锁、信号量等，并且不需要来自操作系统的支持。

协程适用于IO阻塞且需要大量并发的场景，当发生IO阻塞，由协程的调度器进行调度，通过将数据流yield掉，并且记录当前栈上的数据，阻塞完后立刻再通过线程恢复协程栈，并把阻塞的结果放到这个线程上去运行。


```



##  线程和进程的通信方式以及两者区别 

```
进程可以通过管道、套接字、信号交互、共享内存、消息队列等等进行通信；而线程本身就会共享内存，指针指向同一个内容，交互很容易。
```



## 什么用线程什么时候用进程（举例子 )

1．耗时的操作使用线程，提高应用程序响应

2．并行操作时使用线程，如C/S架构的服务器端并发线程响应用户的请求。

3．多CPU系统中，使用线程提高CPU利用率

##  进程上下文切换、为啥进程开销大 

```
线程也有自己的资源，比如栈，私有数据等等。说他使用而不拥有资源指的是使用的是进程的打开文件句柄，进程的全局数据，进程的地址空间等等,这些都属于进程，而不属于线程，进程内个线程共享。 
进程切换比线程切换开销大是因为进程切换时要切页表，而且往往伴随着页调度，因为进程的数据段代码段要换出去，以便把将要执行的进程的内容换进来。本来进程的内容就是线程的超集。而且线程只需要保存线程的上下文（相关寄存器状态和栈的信息）就好了，动作很小

进程切换分两步：
1.切换页目录以使用新的地址空间
2.切换内核栈和硬件上下文

对于linux来说，线程和进程的最大区别就在于地址空间，对于线程切换，第1步是不需要做的，第2是进程和线程切换都要做的。

切换的性能消耗：
1、线程上下文切换和进程上下问切换一个最主要的区别是线程的切换虚拟内存空间依然是相同的，但是进程切换是不同的。这两种上下文切换的处理都是通过操作系统内核来完成的。内核的这种切换过程伴随的最显著的性能损耗是将寄存器中的内容切换出。
2、另外一个隐藏的损耗是上下文的切换会扰乱处理器的缓存机制。简单的说，一旦去切换上下文，处理器中所有已经缓存的内存地址一瞬间都作废了。还有一个显著的区别是当你改变虚拟内存空间的时候，处理的页表缓冲（processor's Translation Lookaside Buffer (TLB)）被全部刷新，这将导致内存的访问在一段时间内相当的低效。但是在线程的切换中，不会出现这个问题。

切换开销
最显著的性能损耗是将保存寄存器中的内容
CPU高速缓存失效
页表查找是一个很慢的过程，因此通常使用Cache来缓存常用的地址映射，这样可以加速页表查找，这个cache就是TLB.当进程切换后页表也要进行切换，页表切换后TLB就失效了，cache失效导致命中率降低，那么虚拟地址转换为物理地址就会变慢，表现出来的就是程序运行会变慢


```



## 上下文切换

```
1.进程的上下文切换
各个进程之间是共享 CPU 资源的，在不同的时候进程之间需要切换，让不同的进程可以在 CPU 执行，那么这个一个进程切换到另一个进程运行，称为进程的上下文切换。
进程是由内核管理和调度的，所以进程的切换只能发生在内核态。
所以，进程的上下文切换不仅包含了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源。
通常，会把交换的信息保存在进程的 PCB，当要运行另外一个进程的时候，我们需要从这个进程的 PCB 取出上下文，然后恢复到 CPU 中，这使得这个进程可以继续执行

2.CPU的上下文切换：
CPU 上下文切换就是先把前一个任务的 CPU 上下文**（CPU 寄存器和程序计数器）**保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。
系统内核会存储保持下来的上下文信息，当此任务再次被分配给 CPU 运行时，CPU 会重新加载这些上下文，这样就能保证任务原来的状态不受影响，让任务看起来还是连续运行。

3.线程的上下文切换
这还得看线程是不是属于同一个进程：
当两个线程不是属于同一个进程，则切换的过程就跟进程上下文切换一样；
当两个线程是属于同一个进程，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据；
所以，线程的上下文切换相比进程，开销要小很多

```



## 内存管理

##  虚拟内存和物理内存的区别和关系？ 

```
虚拟地址是操作系统为每个进程分配的自己的独立的地址空间,
而物理内存是物理意义上的内存, CPU通过操作物理内存来实现程序的运行
因为直接操作物理内存会带来程序编写的困难如两个程序同时对一个物理内存地址进行操作一个覆盖了另一个, 导致了程序的崩溃, 那么分配每个进程单独的地址空间使得进程间的地址隔离开来互不干扰
这时就需要一个MMU内存映射单元将每个进程的虚拟地址映射到对应的物理内存地址上, 这样CPU就可以操作虚拟地址对应的物理内存地址了
内存存在分页分段机制
分段根据逻辑上把内存分成 堆 栈 数据 代码四个段, 虚拟地址由 段选择子和段偏移量构成, 通过段选择子在段表中获得段在物理内存中的段基地址值 + 段偏移量获得真实的物理内存地址从而完成虚拟地址到物理内存的映射
在分页机制下，虚拟地址分为两部分，页号和页内偏移。页号作为页表的索引，页表包含物理页每页所在物理内存的基地址，这个基地址与页内偏移的组合就形成了物理内存地址
```





##  内存分页分段机制 

### 虚拟内存 

如果你是电子相关专业的，肯定在大学里捣鼓过单片机。

单片机是没有操作系统的，所以每次写完代码，都需要借助工具把程序烧录进去，这样程序才能跑起来。

另外，**单片机的 CPU 是直接操作内存的「物理地址」**。

![img](https://user-gold-cdn.xitu.io/2020/6/30/17303f81d0757226?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

在这种情况下，要想在内存中同时运行两个程序是不可能的。如果第一个程序在 2000 的位置写入一个新的值，将会擦掉第二个程序存放在相同位置上的所有内容，所以同时运行两个程序是根本行不通的，这两个程序会立刻崩溃。

> 操作系统是如何解决这个问题呢？

这里关键的问题是这两个程序都引用了绝对物理地址，而这正是我们最需要避免的。

我们可以把进程所使用的地址「隔离」开来，即让操作系统为每个进程分配独立的一套「**虚拟地址**」，人人都有，大家自己玩自己的地址就行，互不干涉。但是有个前提每个进程都不能访问物理地址，至于虚拟地址最终怎么落到物理内存里，对进程来说是透明的，操作系统已经把这些都安排的明明白白了。

![进程的中间层](https://user-gold-cdn.xitu.io/2020/6/30/17303f81d37e28e2?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)进程的中间层

**操作系统会提供一种机制，将不同进程的虚拟地址和不同内存的物理地址映射起来。**

如果程序要访问虚拟地址的时候，由操作系统转换成不同的物理地址，这样不同的进程运行的时候，写入的是不同的物理地址，这样就不会冲突了。

于是，这里就引出了两种地址的概念：

- 我们程序所使用的内存地址叫做**虚拟内存地址**（*Virtual Memory Address*）
- 实际存在硬件里面的空间地址叫**物理内存地址**（*Physical Memory Address*）。

操作系统引入了虚拟内存，进程持有的虚拟地址会通过 CPU 芯片中的内存管理单元（MMU）的映射关系，来转换变成物理地址，然后再通过物理地址访问内存，如下图所示：

![虚拟地址寻址](https://user-gold-cdn.xitu.io/2020/6/30/17303f81d586b421?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)虚拟地址寻址

> 操作系统是如何管理虚拟地址与物理地址之间的关系？

主要有两种方式，分别是**内存分段和内存分页**，分段是比较早提出的，我们先来看看内存分段。

------

### 内存分段 

程序是由若干个逻辑分段组成的，如可由代码分段、数据分段、栈段、堆段组成。**不同的段是有不同的属性的，所以就用分段（\*Segmentation\*）的形式把这些段分离出来。**

> 分段机制下，虚拟地址和物理地址是如何映射的？

分段机制下的虚拟地址由两部分组成，**段选择子**和**段内偏移量**。

![内存分段-寻址的方式](https://user-gold-cdn.xitu.io/2020/6/30/17303f81d3b6a20b?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)内存分段-寻址的方式

- **段选择子**就保存在段寄存器里面。段选择子里面最重要的是**段号**，用作段表的索引。**段表**里面保存的是这个**段的基地址、段的界限和特权等级**等。
- 虚拟地址中的**段内偏移量**应该位于 0 和段界限之间，如果段内偏移量是合法的，就将段基地址加上段内偏移量得到物理内存地址。

在上面，知道了虚拟地址是通过**段表**与物理地址进行映射的，分段机制会把程序的虚拟地址分成 4 个段，每个段在段表中有一个项，在这一项找到段的基地址，再加上偏移量，于是就能找到物理内存中的地址，如下图：

![内存分段-虚拟地址与物理地址](https://user-gold-cdn.xitu.io/2020/6/30/17303f81ffd38048?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)内存分段-虚拟地址与物理地址

如果要访问段 3 中偏移量 500 的虚拟地址，我们可以计算出物理地址为，段 3 基地址 7000 + 偏移量 500 = 7500。

分段的办法很好，解决了程序本身不需要关心具体的物理内存地址的问题，但它也有一些不足之处：

- 第一个就是**内存碎片**的问题。
- 第二个就是**内存交换的效率低**的问题。

接下来，说说为什么会有这两个问题。

> 我们先来看看，分段为什么会产生内存碎片的问题？

我们来看看这样一个例子。假设有 1G 的物理内存，用户执行了多个程序，其中：

- 游戏占用了 512MB 内存
- 浏览器占用了 128MB 内存
- 音乐占用了 256 MB 内存。

这个时候，如果我们关闭了浏览器，则空闲内存还有 1024 - 512 - 256 = 256MB。

如果这个 256MB 不是连续的，被分成了两段 128 MB 内存，这就会导致没有空间再打开一个 200MB 的程序。

![内存碎片的问题](https://user-gold-cdn.xitu.io/2020/6/30/17303f820070d348?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)内存碎片的问题

这里的内存碎片的问题共有两处地方：

- 外部内存碎片，也就是产生了多个不连续的小物理内存，导致新的程序无法被装载；
- 内部内存碎片，程序所有的内存都被装载到了物理内存，但是这个程序有部分的内存可能并不是很常使用，这也会导致内存的浪费；

针对上面两种内存碎片的问题，解决的方式会有所不同。

解决外部内存碎片的问题就是**内存交换**。

可以把音乐程序占用的那 256MB 内存写到硬盘上，然后再从硬盘上读回来到内存里。不过再读回的时候，我们不能装载回原来的位置，而是紧紧跟着那已经被占用了的 512MB 内存后面。这样就能空缺出连续的 256MB 空间，于是新的 200MB 程序就可以装载进来。

这个内存交换空间，在 Linux 系统里，也就是我们常看到的 Swap 空间，这块空间是从硬盘划分出来的，用于内存与硬盘的空间交换。

> 再来看看，分段为什么会导致内存交换效率低的问题？

对于多进程的系统来说，用分段的方式，内存碎片是很容易产生的，产生了内存碎片，那不得不重新 `Swap` 内存区域，这个过程会产生性能瓶颈。

因为硬盘的访问速度要比内存慢太多了，每一次内存交换，我们都需要把一大段连续的内存数据写到硬盘上。

所以，**如果内存交换的时候，交换的是一个占内存空间很大的程序，这样整个机器都会显得卡顿。**

为了解决内存分段的内存碎片和内存交换效率低的问题，就出现了内存分页。

------

### 内存分页 

分段的好处就是能产生连续的内存空间，但是会出现内存碎片和内存交换的空间太大的问题。

要解决这些问题，那么就要想出能少出现一些内存碎片的办法。另外，当需要进行内存交换的时候，让需要交换写入或者从磁盘装载的数据更少一点，这样就可以解决问题了。这个办法，也就是**内存分页**（*Paging*）。

**分页是把整个虚拟和物理内存空间切成一段段固定尺寸的大小**。这样一个连续并且尺寸固定的内存空间，我们叫**页**（*Page*）。在 Linux 下，每一页的大小为 `4KB`。

虚拟地址与物理地址之间通过**页表**来映射，如下图：

![内存映射](https://user-gold-cdn.xitu.io/2020/6/30/17303f8200e320ea?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)内存映射

页表实际上存储在 CPU 的**内存管理单元** （*MMU*） 中，于是 CPU 就可以直接通过 MMU，找出要实际要访问的物理内存地址。

而当进程访问的虚拟地址在页表中查不到时，系统会产生一个**缺页异常**，进入系统内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运行。

> 分页是怎么解决分段的内存碎片、内存交换效率低的问题？

由于内存空间都是预先划分好的，也就不会像分段会产生间隙非常小的内存，这正是分段会产生内存碎片的原因。而**采用了分页，那么释放的内存都是以页为单位释放的，也就不会产生无法给进程使用的小内存。**

如果内存空间不够，操作系统会把其他正在运行的进程中的「最近没被使用」的内存页面给释放掉，也就是暂时写在硬盘上，称为**换出**（*Swap Out*）。一旦需要的时候，再加载进来，称为**换入**（*Swap In*）。所以，一次性写入磁盘的也只有少数的一个页或者几个页，不会花太多时间，**内存交换的效率就相对比较高。**

![换入换出](https://user-gold-cdn.xitu.io/2020/6/30/17303f820144d6cf?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)换入换出

更进一步地，分页的方式使得我们在加载程序的时候，不再需要一次性都把程序加载到物理内存中。我们完全可以在进行虚拟内存和物理内存的页之间的映射之后，并不真的把页加载到物理内存里，而是**只有在程序运行中，需要用到对应虚拟内存页里面的指令和数据时，再加载到物理内存里面去。**

> 分页机制下，虚拟地址和物理地址是如何映射的？

在分页机制下，虚拟地址分为两部分，**页号**和**页内偏移**。页号作为页表的索引，**页表**包含物理页每页所在**物理内存的基地址**，这个基地址与页内偏移的组合就形成了物理内存地址，见下图。

![内存分页寻址](https://user-gold-cdn.xitu.io/2020/6/30/17303f8206624181?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)内存分页寻址

总结一下，对于一个内存地址转换，其实就是这样三个步骤：

- 把虚拟内存地址，切分成页号和偏移量；
- 根据页号，从页表里面，查询对应的物理页号；
- 直接拿物理页号，加上前面的偏移量，就得到了物理内存地址。

下面举个例子，虚拟内存中的页通过页表映射为了物理内存中的页，如下图：

![虚拟页与物理页的映射](https://user-gold-cdn.xitu.io/2020/6/30/17303f82280197cf?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)虚拟页与物理页的映射

这看起来似乎没什么毛病，但是放到实际中操作系统，这种简单的分页是肯定是会有问题的。

> 简单的分页有什么缺陷吗？

有空间上的缺陷。

因为操作系统是可以同时运行非常多的进程的，那这不就意味着页表会非常的庞大。

在 32 位的环境下，虚拟地址空间共有 4GB，假设一个页的大小是 4KB（2^12），那么就需要大约 100 万 （2^20） 个页，每个「页表项」需要 4 个字节大小来存储，那么整个 4GB 空间的映射就需要有 `4MB` 的内存来存储页表。

这 4MB 大小的页表，看起来也不是很大。但是要知道每个进程都是有自己的虚拟地址空间的，也就说都有自己的页表。

那么，`100` 个进程的话，就需要 `400MB` 的内存来存储页表，这是非常大的内存了，更别说 64 位的环境了。

#### 多级页表

要解决上面的问题，就需要采用的是一种叫作**多级页表**（*Multi-Level Page Table*）的解决方案。

在前面我们知道了，对于单页表的实现方式，在 32 位和页大小 `4KB` 的环境下，一个进程的页表需要装下 100 多万个「页表项」，并且每个页表项是占用 4 字节大小的，于是相当于每个页表需占用 4MB 大小的空间。

我们把这个 100 多万个「页表项」的单级页表再分页，将页表（一级页表）分为 `1024` 个页表（二级页表），每个表（二级页表）中包含 `1024` 个「页表项」，形成**二级分页**。如下图所示：

![二级分页](https://user-gold-cdn.xitu.io/2020/6/30/17303f823070a77a?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)二级分页

> 你可能会问，分了二级表，映射 4GB 地址空间就需要 4KB（一级页表）+ 4MB（二级页表）的内存，这样占用空间不是更大了吗？

当然如果 4GB 的虚拟地址全部都映射到了物理内存上的话，二级分页占用空间确实是更大了，但是，我们往往不会为一个进程分配那么多内存。

其实我们应该换个角度来看问题，还记得计算机组成原理里面无处不在的**局部性原理**么？

每个进程都有 4GB 的虚拟地址空间，而显然对于大多数程序来说，其使用到的空间远未达到 4GB，因为会存在部分对应的页表项都是空的，根本没有分配，对于已分配的页表项，如果存在最近一定时间未访问的页表，在物理内存紧张的情况下，操作系统会将页面换出到硬盘，也就是说不会占用物理内存。

如果使用了二级分页，一级页表就可以覆盖整个 4GB 虚拟地址空间，但**如果某个一级页表的页表项没有被用到，也就不需要创建这个页表项对应的二级页表了，即可以在需要时才创建二级页表**。做个简单的计算，假设只有 20% 的一级页表项被用到了，那么页表占用的内存空间就只有 4KB（一级页表） + 20% * 4MB（二级页表）= `0.804MB`，这对比单级页表的 `4MB` 是不是一个巨大的节约？

那么为什么不分级的页表就做不到这样节约内存呢？我们从页表的性质来看，保存在内存中的页表承担的职责是将虚拟地址翻译成物理地址。假如虚拟地址在页表中找不到对应的页表项，计算机系统就不能工作了。所以**页表一定要覆盖全部虚拟地址空间，不分级的页表就需要有 100 多万个页表项来映射，而二级分页则只需要 1024 个页表项**（此时一级页表覆盖到了全部虚拟地址空间，二级页表在需要时创建）。

我们把二级分页再推广到多级页表，就会发现页表占用的内存空间更少了，这一切都要归功于对局部性原理的充分应用。

对于 64 位的系统，两级分页肯定不够了，就变成了四级目录，分别是：

- 全局页目录项 PGD（*Page Global Directory*）；
- 上层页目录项 PUD（*Page Upper Directory*）；
- 中间页目录项 PMD（*Page Middle Directory*）；
- 页表项 PTE（*Page Table Entry*）；

![四级目录](https://user-gold-cdn.xitu.io/2020/6/30/17303f82332af6af?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)四级目录

#### TLB

多级页表虽然解决了空间上的问题，但是虚拟地址到物理地址的转换就多了几道转换的工序，这显然就降低了这俩地址转换的速度，也就是带来了时间上的开销。

程序是有局部性的，即在一段时间内，整个程序的执行仅限于程序中的某一部分。相应地，执行所访问的存储空间也局限于某个内存区域。

![程序的局部性](https://user-gold-cdn.xitu.io/2020/6/30/17303f82448236b6?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)程序的局部性

我们就可以利用这一特性，把最常访问的几个页表项存储到访问速度更快的硬件，于是计算机科学家们，就在 CPU 芯片中，加入了一个专门存放程序最常访问的页表项的 Cache，这个 Cache 就是 TLB（*Translation Lookaside Buffer*） ，通常称为页表缓存、转址旁路缓存、快表等。

![地址转换](https://user-gold-cdn.xitu.io/2020/6/30/17303f8248ebf887?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)地址转换

在 CPU 芯片里面，封装了内存管理单元（*Memory Management Unit*）芯片，它用来完成地址转换和 TLB 的访问与交互。

有了 TLB 后，那么 CPU 在寻址时，会先查 TLB，如果没找到，才会继续查常规的页表。

TLB 的命中率其实是很高的，因为程序最常访问的页就那么几个。

------

### 段页式内存管理 

内存分段和内存分页并不是对立的，它们是可以组合起来在同一个系统中使用的，那么组合起来后，通常称为**段页式内存管理**。

![段页式地址空间](https://user-gold-cdn.xitu.io/2020/6/30/17303f8267c2cc87?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)段页式地址空间

段页式内存管理实现的方式：

- 先将程序划分为多个有逻辑意义的段，也就是前面提到的分段机制；
- 接着再把每个段划分为多个页，也就是对分段划分出来的连续空间，再划分固定大小的页；

这样，地址结构就由**段号、段内页号和页内位移**三部分组成。

用于段页式地址变换的数据结构是每一个程序一张段表，每个段又建立一张页表，段表中的地址是页表的起始地址，而页表中的地址则为某页的物理页号，如图所示：

![段页式管理中的段表、页表与内存的关系](https://user-gold-cdn.xitu.io/2020/6/30/17303f826f5479f2?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)段页式管理中的段表、页表与内存的关系

段页式地址变换中要得到物理地址须经过三次内存访问：

- 第一次访问段表，得到页表起始地址；
- 第二次访问页表，得到物理页号；
- 第三次将物理页号与页内位移组合，得到物理地址。

可用软、硬件相结合的方法实现段页式地址变换，这样虽然增加了硬件成本和系统开销，但提高了内存的利用率。

### 总结 

为了在多进程环境下，使得进程之间的内存地址不受影响，相互隔离，于是操作系统就为每个进程独立分配一套**虚拟地址空间**，每个程序只关心自己的虚拟地址就可以，实际上大家的虚拟地址都是一样的，但分布到物理地址内存是不一样的。作为程序，也不用关心物理地址的事情。

每个进程都有自己的虚拟空间，而物理内存只有一个，所以当启用了大量的进程，物理内存必然会很紧张，于是操作系统会通过**内存交换**技术，把不常使用的内存暂时存放到硬盘（换出），在需要的时候再装载回物理内存（换入）。

那既然有了虚拟地址空间，那必然要把虚拟地址「映射」到物理地址，这个事情通常由操作系统来维护。

那么对于虚拟地址与物理地址的映射关系，可以有**分段**和**分页**的方式，同时两者结合都是可以的。

内存分段是根据程序的逻辑角度，分成了**栈段、堆段、数据段、代码段**等，这样可以分离出不同属性的段，同时是一块连续的空间。但是每个段的大小都不是统一的，这就会导致内存碎片和内存交换效率低的问题。

于是，就出现了内存分页，把虚拟空间和物理空间分成大小固定的页，如在 Linux 系统中，每一页的大小为 `4KB`。由于分了页后，就不会产生细小的内存碎片。同时在内存交换的时候，写入硬盘也就一个页或几个页，这就大大提高了内存交换的效率。

再来，为了解决简单分页产生的页表过大的问题，就有了**多级页表**，它解决了空间上的问题，但这就会导致 CPU 在寻址的过程中，需要有很多层表参与，加大了时间上的开销。于是根据程序的**局部性原理**，在 CPU 芯片中加入了 **TLB**，负责缓存最近常被访问的页表项，大大提高了地址的转换速度。










##  堆和栈的区别 以及在操作系统中堆和栈的具体使用 

```
内存中的栈区处于相对较高的地址以地址的增长方向为上的话，栈地址是向下增长的。

栈中分配局部变量空间，堆区是向上增长的用于分配程序员申请的内存空间。另外还有静态区是分配静态变量，全局变量空间的；只读区是分配常量和程序代码空间的；以及其他一些分区。

0.申请方式和回收方式不同
不知道你是否有点明白了。

堆和栈的第一个区别就是申请方式不同：栈（英文名称是stack）是系统自动分配空间的，例如我们定义一个 char a；系统会自动在栈上为其开辟空间。而堆（英文名称是heap）则是程序员根据需要自己申请的空间，例如malloc（10）；开辟十个字节的空间。

由于栈上的空间是自动分配自动回收的，所以栈上的数据的生存周期只是在函数的运行过程中，运行后就释放掉，不可以再访问。而堆上的数据只要程序员不释放空间，就一直可以访问到，不过缺点是一旦忘记释放会造成内存泄露。还有其他的一些区别我认为网上的朋友总结的不错这里转述一下：

1.申请后系统的响应
栈：只要栈的剩余空间大于所申请空间，系统将为程序提供内存，否则将报异常提示栈溢出。

堆：首先应该知道操作系统有一个记录空闲内存地址的链表，当系统收到程序的申请时，会遍历该链表，寻找第一个空间大于所申请空间的堆结点，然后将该结点从空闲结点链表中删除，并将该结点的空间分配给程序，另外，对于大多数系统，会在这块内存空间中的首地址处记录本次分配的大小，这样，代码中的 delete语句才能正确的释放本内存空间。另外，由于找到的堆结点的大小不一定正好等于申请的大小，系统会自动的将多余的那部分重新放入空闲链表中。 
也就是说堆会在申请后还要做一些后续的工作这就会引出申请效率的问题。

2.申请效率的比较

根据第0点和第1点可知。
栈：由系统自动分配，速度较快。但程序员是无法控制的。
堆：是由new分配的内存，一般速度比较慢，而且容易产生内存碎片,不过用起来最方便。

3.申请大小的限制
栈：在Windows下,栈是向低地址扩展的数据结构，是一块连续的内存的区域。这句话的意思是栈顶的地址和栈的最大容量是系统预先规定好的，在 WINDOWS下，栈的大小是2M（也有的说是1M，总之是一个编译时就确定的常数），如果申请的空间超过栈的剩余空间时，将提示overflow。因此，能从栈获得的空间较小。 

堆：堆是向高地址扩展的数据结构，是不连续的内存区域。这是由于系统是用链表来存储的空闲内存地址的，自然是不连续的，而链表的遍历方向是由低地址向高地址。堆的大小受限于计算机系统中有效的虚拟内存。由此可见，堆获得的空间比较灵活，也比较大。

4.堆和栈中的存储内容
由于栈的大小有限，所以用子函数还是有物理意义的，而不仅仅是逻辑意义。

栈： 在函数调用时，第一个进栈的是主函数中函数调用后的下一条指令（函数调用语句的下一条可执行语句）的地址，然后是函数的各个参数，在大多数的C编译器中，参数是由右往左入栈的，然后是函数中的局部变量。注意静态变量是不入栈的。 
当本次函数调用结束后，局部变量先出栈，然后是参数，最后栈顶指针指向最开始存的地址，也就是主函数中的下一条指令，程序由该点继续运行。 

堆：一般是在堆的头部用一个字节存放堆的大小。堆中的具体内容有程序员安排。
```

##  操作系统调度算法和饥饿问题 

```
先来先到
	计算密集型和IO密集型进程同时执行, IO可能需要很长时间才能执行完, 但是如果调度算法可以每10ms抢占计算密集型进程, IO可也很快完成, 不会对计算密集型产生太大延迟
最短作业优先
	所有作业都可以同时运行的情况下才是最优的
最短剩余时间优先
	当前运行进程会为刚进来的剩余时间更短的进程让出CPU
轮转调度 (20-50ms)
	每个进程分配一个时间片, 时间片结束将CPU分配给另一个进程, 如果时间片结束前阻塞或者结束立即切换
优先级调度
	防止当前高优先级的进程无限进行下去, 每个时钟滴答会降低当前进程的优先级,如果导致了当前进程优先级低于其他进程, 切换 给其他进程运行的机会
多级队列
	为CPU密集型进程设置较长的时间片比频繁的分给他们短时间片更高效, 设置优先级类, 属于最高优先级类的进程运行一个时间片, 属于次高的运行2个时间片, 降低一级, 时间片时间加倍, 当一个进程用完分配的时间片后, 移动到下一类, 下次可运行的时间加倍
最短进程优先
	根据过去的行为, 估算运行时间最短的那一个
保证调度
	n个进程, 每个获取1/nCPU
彩票
	需要调度, 抽彩票, 拥有该彩票的进程获得该资源
公平分享
	每个用户不管进程多少, 都获得1/用户数的CPU
```

## DMA

https://blog.csdn.net/as480133937/article/details/104927922?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162579960116780261915487%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=162579960116780261915487&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-104927922.pc_search_result_control_group&utm_term=DMA&spm=1018.2226.3001.4187

## 文件管理

https://juejin.cn/post/6850037269835808782#heading-86

## inode

https://blog.csdn.net/haiross/article/details/39157885?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162556888416780269864750%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=162556888416780269864750&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-5-39157885.pc_search_result_control_group&utm_term=%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F+%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F+inode&spm=1018.2226.3001.4187

### 软连接

-  创建一个硬链接后，testfile的inode count增加了一个。而且testfile和testfile.hard这两个的Inode number是一样的。这个硬链接就是重新创建了一个文件名对应到原文件的Inode。实质就是在Directory中增加了一个新的对应关系。通过这个例子，你是不是更清楚了，这个Inode count的含义了。他就是指，一个Inode对应了多少个文件名。 

- 般情况下，文件名和inode号码是"一一对应"关系，每个inode号码对应一个文件名。但是，Unix/Linux系统允许，多个文件名指向同一个inode号码。这意味着，可以用不同的文件名访问同样的内容；对文件内容进行修改，会影响到所有文件名；但是，删除一个文件名，不影响另一个文件名的访问。这种情况就被称为"硬链接"（hard link）。**

  

### 硬链接

-  软链接是重新建立了一个文件，而文件是指向到原文件，而不是指向原Inode。当然他会占用掉 inode 与 block。当我们删除了源文件后，链接文件不能独立存在，虽然仍保留文件名，但我们却不能查看软链接文件的内容了。但软链接是可以跨文件系统，而且是可以链接目录。他就相当于windows系统下的快捷方式一样 
- 文件A和文件B的inode号码虽然不一样，但是文件A的内容是文件B的路径。读取文件A时，系统会自动将访问者导向文件B。因此，无论打开哪一个文件，最终读取的都是文件B。这时，文件A就称为文件B的"软链接"（soft link）或者"符号链接（symbolic link）。这意味着，文件A依赖于文件B而存在，如果删除了文件B，打开文件A就会报错："No such file or directory"。这是软链接与硬链接最大的不同：文件A指向文件B的文件名，而不是文件B的inode号码，文件B的inode"链接数"不会因此发生变化。

### 总结

1.  一个Inode对应一个文件，而一个文件根据其大小，会占用多块blocks。
2. 更为准确的来说，一个文件只对应一个Inode。因为硬链接其实不是创建新文件，只是在Directory中写入了新的对应关系而已。
3. 当我们删除文件的时候，只是把Inode标记为可用，文件在block中的内容是没有被清除的，只有在有新的文件需要占用block的时候，才会被覆盖。 



硬链接：其实就是同一个文件具有多个别名，具有相同inode，而dentry不同。

​       \1. 文件具有相同的inode和data block；

​       \2. 只能对已存在的文件进行创建；

​       \3. 不同交叉文件系统进行硬链接的创建

​       \4. 不能对目录进行创建，只能对文件创建硬链接

​       \5. 删除一个硬链接并不影响其他具有相同inode号的文件；

软链接：软链接具有自己的inode，即具有自己的文件，只是这个文件中存放的内容是另一个文件的路径名。因此软链接具有自己的inode号以及用户数据块。

​       \1. 软链接有自己的文件属性及权限等；

​       \2. 软链接可以对不存在的文件或目录创建；

​       \3. 软链接可以交叉文件系统；

​       \4. 软链接可以对文件或目录创建；

​       \5. 创建软链接时，链接计数i_nlink不会增加；

​       \6. 删除软链接不会影响被指向的文件，但若指向的原文件被删除，则成死链接，但重新创建指向 的路径即可恢复为正常的软链接，只是源文件的内容可能变了。

## IO









## 锁

##  死锁如何避免

```
一、死锁的定义：
多个进行相互等待对方资源，在得到所有资源继续运行之前，都不会释放自己已有的资源，这样造成了循环等待的现象，称为死锁。

二、产生死锁的四大必要条件：
①资源互斥/资源不共享
每个资源要么已经分配给了一个进程，要么是可用的，只有这两种状态，资源不可以被共享使用，所以所谓的互斥是指：资源不共享，如果被使用，只能被一个进程使用。

②占有和等待/请求并保持
已经得到资源的进程还能继续请求新的资源，所以个人觉得叫占有并请求也许更好理解。

③资源不可剥夺
当一个资源分配给了一个进程后，其它需要该资源的进程不能强制性获得该资源，除非该资源的当前占有者显示地释放该资源。

④环路等待
死锁发生时，系统中一定有由两个或两个以上的进程组成的一条环路，环路上的每个进程都在等待下一个进程所占有的资源。

三、防止死锁的方法
1、破坏互斥条件
方法：如果允许系统资源都能共享使用，则系统不会进入死锁状态。
缺点：有些资源根本不能同时访问，如打印机等临界资源只能互斥使用。所以，破坏互斥条件而预防死锁的方法不太可行，而且在有的场合应该保护这种互斥性。

2、破坏请求并保持条件
方法：釆用预先静态分配方法，即进程在运行前一次申请完它所需要的全部资源，在它的资源未满足前，不把它投入运行。一旦投入运行后，这些资源就一直归它所有，也不再提出其他资源请求，这样就可以保证系统不会发生死锁。
缺点： 系统资源被严重浪费，其中有些资源可能仅在运行初期或运行快结束时才使用，甚至根本不使用。而且还会导致“饥饿”现象，当由于个别资源长期被其他进程占用时，将致使等待该资源的进程迟迟不能开始运行。

3、破坏不可剥夺条件
方法：当一个已保持了某些不可剥夺资源的进程，请求新的资源而得不到满足时，它必须释放已经保持的所有资源，待以后需要时再重新申请。这意味着，一个进程已占有的资源会被暂时释放，或者说是被剥夺了，或从而破坏了不可剥夺条件。
缺点：该策略实现起来比较复杂，释放已获得的资源可能造成前一阶段工作的失效，反复地申请和释放资源会增加系统开销，降低系统吞吐量。这种方法常用于状态易于保存和恢复的资源，如CPU的寄存器及内存资源，一般不能用于打印机之类的资源。

4、破坏循环等待条件
方法：为了破坏循环等待条件，可釆用顺序资源分配法。首先给系统中的资源编号，规定每个进程，必须按编号递增的顺序请求资源，同类资源一次申请完。也就是说，只要进程提出申请分配资源Ri，则该进程在以后的资源申请中，只能申请编号大于Ri的资源。
缺点：这种方法存在的问题是，编号必须相对稳定，这就限制了新类型设备的增加；尽管在为资源编号时已考虑到大多数作业实际使用这些资源的顺序，但也经常会发生作业使用资源的顺序与系统规定顺序不同的情况，造成资源的浪费；此外，这种按规定次序申请资源的方法，也必然会给用户的编程带来麻烦。

六、死锁的解除

1、资源剥夺法
挂起某些死锁进程，并抢占它的资源，将这些资源分配给其他的死锁进程。但应防止被挂起的进程长时间得不到资源，而处于资源匮乏的状态。
2、撤销进程法
强制撤销部分、甚至全部死锁进程并剥夺这些进程的资源。撤销的原则可以按进程优先级和撤销进程代价的高低进行。
3、进程回退法
让一（多）个进程回退到足以回避死锁的地步，进程回退时自愿释放资源而不是被剥夺。要求系统保持进程的历史信息，设置还原点。
```









##  epoll底层，水平，边缘 

### ANS1

设想一个场景：有100万用户同时与一个进程保持着TCP连接，而每一时刻只有几十个或几百个TCP连接是活跃的(接收TCP包)，也就是说在每一时刻进程只需要处理这100万连接中的一小部分连接。那么，如何才能高效的处理这种场景呢？进程是否在每次询问操作系统收集有事件发生的TCP连接时，把这100万个连接告诉操作系统，然后由操作系统找出其中有事件发生的几百个连接呢？实际上，在Linux2.4版本以前，那时的select或者poll事件驱动方式是这样做的。

  这里有个非常明显的问题，即在某一时刻，进程收集有事件的连接时，其实这100万连接中的大部分都是没有事件发生的。因此如果每次收集事件时，都把100万连接的套接字传给操作系统(这首先是用户态内存到内核态内存的大量复制)，而由操作系统内核寻找这些连接上有没有未处理的事件，将会是巨大的资源浪费，然后select和poll就是这样做的，因此它们最多只能处理几千个并发连接。而epoll不这样做，它在Linux内核中申请了一个简易的文件系统，把原先的一个select或poll调用分成了3部分：

int epoll_create(int size);  
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);  
int epoll_wait(int epfd, struct epoll_event *events,int maxevents, int timeout);  

1. 调用epoll_create建立一个epoll对象(在epoll文件系统中给这个句柄分配资源)；

2. 调用epoll_ctl向epoll对象中添加这100万个连接的套接字；

3. 调用epoll_wait收集发生事件的连接。

  这样只需要在进程启动时建立1个epoll对象，并在需要的时候向它添加或删除连接就可以了，因此，在实际收集事件时，epoll_wait的效率就会非常高，因为调用epoll_wait时并没有向它传递这100万个连接，内核也不需要去遍历全部的连接。

一、epoll原理详解
  当某一进程调用epoll_create方法时，Linux内核会创建一个eventpoll结构体，这个结构体中有两个成员与epoll的使用方式密切相关，如下所示：

struct eventpoll {
　　...
　　/*红黑树的根节点，这棵树中存储着所有添加到epoll中的事件，
　　也就是这个epoll监控的事件*/
　　struct rb_root rbr;
　　/*双向链表rdllist保存着将要通过epoll_wait返回给用户的、满足条件的事件*/
　　struct list_head rdllist;
　　...
};
  我们在调用epoll_create时，内核除了帮我们在epoll文件系统里建了个file结点，在内核cache里建了个红黑树用于存储以后epoll_ctl传来的socket外，还会再建立一个rdllist双向链表，用于存储准备就绪的事件，当epoll_wait调用时，仅仅观察这个rdllist双向链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。所以，epoll_wait非常高效。

  所有添加到epoll中的事件都会与设备(如网卡)驱动程序建立回调关系，也就是说相应事件的发生时会调用这里的回调方法。这个回调方法在内核中叫做ep_poll_callback，它会把这样的事件放到上面的rdllist双向链表中。

  在epoll中对于每一个事件都会建立一个epitem结构体，如下所示：

struct epitem {
　　...
　　//红黑树节点
　　struct rb_node rbn;
　　//双向链表节点
　　struct list_head rdllink;
　　//事件句柄等信息
　　struct epoll_filefd ffd;
　　//指向其所属的eventepoll对象
　　struct eventpoll *ep;
　　//期待的事件类型
　　struct epoll_event event;
　　...
}; // 这里包含每一个事件对应着的信息。

  当调用epoll_wait检查是否有发生事件的连接时，只是检查eventpoll对象中的rdllist双向链表是否有epitem元素而已，如果rdllist链表不为空，则这里的事件复制到用户态内存（使用共享内存提高效率）中，同时将事件数量返回给用户。因此epoll_waitx效率非常高。epoll_ctl在向epoll对象中添加、修改、删除事件时，从rbr红黑树中查找事件也非常快，也就是说epoll是非常高效的，它可以轻易地处理百万级别的并发连接。


【总结】：

  一颗红黑树，一张准备就绪句柄链表，少量的内核cache，就帮我们解决了大并发下的socket处理问题。

执行epoll_create()时，创建了红黑树和就绪链表；

执行epoll_ctl()时，如果增加socket句柄，则检查在红黑树中是否存在，存在立即返回，不存在则添加到树干上，然后向内核注册回调函数，用于当中断事件来临时向准备就绪链表中插入数据；

执行epoll_wait()时立刻返回准备就绪链表里的数据即可。


二、epoll的两种触发模式
  epoll有EPOLLLT和EPOLLET两种触发模式，LT是默认的模式，ET是“高速”模式。

LT（水平触发）模式下，只要这个文件描述符还有数据可读，每次 epoll_wait都会返回它的事件，提醒用户程序去操作；

ET（边缘触发）模式下，在它检测到有 I/O 事件时，通过 epoll_wait 调用会得到有事件通知的文件描述符，对于每一个被通知的文件描述符，如可读，则必须将该文件描述符一直读到空，让 errno 返回 EAGAIN 为止，否则下次的 epoll_wait 不会返回余下的数据，会丢掉事件。如果ET模式不是非阻塞的，那这个一直读或一直写势必会在最后一次阻塞。

  还有一个特点是，epoll使用“事件”的就绪通知方式，通过epoll_ctl注册fd，一旦该fd就绪，内核就会采用类似callback的回调机制来激活该fd，epoll_wait便可以收到通知。



【epoll为什么要有EPOLLET触发模式？】：

  如果采用EPOLLLT模式的话，系统中一旦有大量你不需要读写的就绪文件描述符，它们每次调用epoll_wait都会返回，这样会大大降低处理程序检索自己关心的就绪文件描述符的效率.。而采用EPOLLET这种边缘触发模式的话，当被监控的文件描述符上有可读写事件发生时，epoll_wait()会通知处理程序去读写。如果这次没有把数据全部读写完(如读写缓冲区太小)，那么下次调用epoll_wait()时，它不会通知你，也就是它只会通知你一次，直到该文件描述符上出现第二次可读写事件才会通知你！！！这种模式比水平触发效率高，系统不会充斥大量你不关心的就绪文件描述符。

【总结】：

ET模式（边缘触发）只有数据到来才触发，不管缓存区中是否还有数据，缓冲区剩余未读尽的数据不会导致epoll_wait返回；

LT 模式（水平触发，默认）只要有数据都会触发，缓冲区剩余未读尽的数据会导致epoll_wait返回。


### ANS2

select、poll、epoll是Linux平台下的IO多路复用技术，适合用来管理大量的文件描述符，但是这些系统调用本身是阻塞的，而他们管理的socket描述符其实是可以阻塞，也可以非阻塞的，但是大部分情况下设置为非阻塞的要更好一些，效率会更高一些。因此，他们并不是真正的异步IO。是伪异步的。

1、select

首先，select的缺点1：是select管理的描述符的数量在不重新编译内核的情况下是一个固定的值：1024，当然，重新编译了Linux内核之后，这个数值可以继续增大到用户的需求，但是这是相对来说比较麻烦的一件事。

其次。select的缺点2：是select对于socket描述符的管理方式，因为Linux内核对select的实现方式为每次返回前都要对所有的描述符进行一遍遍历，然后将有事件发生的socket描述符放到描述符集合里，然后将这个描述符集合返回。这种情况对于描述符的数量不是很大的时候还是可以的，但是当描述符达到数十万，甚至上百万的时候，select的效率就会急剧的降低，因为这样的轮询机制会造成大量的浪费和资源开销。因为每一次的轮询都要将这些所有的socket描述符从用户态拷贝到内核态，在内核态，进行轮询，查看是否有事件发生，这是select的底层需要做的。而这些拷贝完全是可以避免的。

2、poll

poll的实现机制和select是一样的，也是采用轮询机制来查看有事件发生的socket描述符，所以效率也是很低，但是poll对select有一项改进就是能够监视的描述符是任意大小的而不是局限在一个较小的数值上(当然这个描述符的大小也是需要操作系统来支持的)。

综上：在总结一下，select与poll的实现机制基本是一样的，只不过函数不同，参数不同，但是基本流程是相同的；

1、复制用户数据到内核空间

2、估计超时时间

3、遍历每个文件并调用f_op->poll()取得文件状态

4、遍历完成检查状态

如果有就绪的文件(描述符对应的还是文件，这里就当成是描述符就可以)则跳转到5，

如果有信号产生则重新启动poll或者select，否则挂起进程并等待超时或唤醒超时或再次遍历每个文件的状态

5、将所有文件的就绪状态复制到用户空间

6、清理申请的资源



3、epoll

epoll改进了select的两个缺点，使用了三个数据结构从而能够在管理大量的描述符的情况下，对系统资源的使用并没有急剧的增加，而只是对内存的使用有所增加（毕竟存储大量的描述符的数据结构会占用大量内存）。

epoll在实现上的三个核心点是：1、红黑树，2、rdlist(就绪描述符链表)接下来一一解释为什么会高效；

1、红黑树是用来存储这些描述符的，因为红黑树的特性，就是良好的插入，查找，删除性能O(lgN)。

     当内核初始化epoll的时候（当调用epoll_create的时候内核也是个epoll描述符创建了一个文件，毕竟在Linux中一切都是文件，而epoll面对的是一个特殊的文件，和普通文件不同），会开辟出一块内核高速cache区，这块区域用来存储我们要监管的所有的socket描述符，当然在这里面存储一定有一个数据结构，这就是红黑树，由于红黑树的接近平衡的查找，插入，删除能力，在这里显著的提高了对描述符的管理。

2、rdlist   就绪描述符链表这是一个双链表，epoll_wait()函数返回的也是这个就绪链表。

     当内核创建了红黑树之后，同时也会建立一个双向链表rdlist，用于存储准备就绪的描述符，当调用epoll_wait的时候在timeout时间内，只是简单的去管理这个rdlist中是否有数据，如果没有则睡眠至超时，如果有数据则立即返回并将链表中的数据赋值到events数组中。这样就能够高效的管理就绪的描述符，而不用去轮询所有的描述符。所以当管理的描述符很多但是就绪的描述符数量很少的情况下如果用select来实现的话效率可想而知，很低，但是epoll的话确实是非常适合这个时候使用。
    
      对与rdlist的维护：当执行epoll_ctl时除了把socket描述符放入到红黑树中之外，还会给内核中断处理程序注册一个回调函数，告诉内核，当这个描述符上有事件到达（或者说中断了）的时候就调用这个回调函数。这个回调函数的作用就是将描述符放入到rdlist中，所以当一个socket上的数据到达的时候内核就会把网卡上的数据复制到内核，然后把socket描述符插入就绪链表rdlist中。

补充：epoll的工作模式ET和LT

都知道epoll有两个工作模式，ET和LT，其中ET模式是高速模式，叫做边缘触发模式，LT模式是默认模式，叫做水平触发模式。

这两种工作模式的区别在于：

当工作在ET模式下，如果一个描述符上有数据到达，然后读取这个描述符上的数据如果没有将数据全部读完的话，当下次epoll_wait返回的时候这个描述符里的数据就再也读取不到了，因为这个描述符不会再次触发返回，也就没法去读取，所以对于这种模式下对一个描述符的数据的正确读取方式是用一个死循环一直读，读到么有数据可读的情况下才可以认为是读取结束。

而工作在LT模式下，这种情况就不会发生，如果对一个描述符的数据没有读取完成，那么下次当epoll_wait返回的时候会继续触发，也就可以继续获取到这个描述符，从而能够接着读。

那么这两种模式的实现方式是什么样的?

基于以上的数据结构是怎么实现这种工作模式的呢？

实现原理：当一个socket描述符的中断事件发生，内核会将数据从网卡复制到内核，同时将socket描述符插入到rdlist中，此时如果调用了epoll_wait会把rdlist中的就绪的socekt描述符复制到用户空间，然后清理掉这个rdlist中的数据，最后epoll_wait还会再次检查这些socket描述符，如果是工作在LT模式下，并且这些socket描述符上还有数据没有读取完成，那么L就会再次把没有读完的socket描述符放入到rdlist中，所以再次调用epoll_wait的时候是会再次触发的，而ET模式是不会这么干的。

ET模式在物理实现上是基于电平的高低变化来工作的，就是从高电平变成低电平，或者从低电平变成高电平的这个上升沿或者下降沿才会触发，也就是状态变化导致触发，而当一个描述符上数据未读完的时候这个状态是不会发生变化的，所以触发不了，LT模式是在只有出现高电平的时候才会触发。

高电平和低电平：

LT水平触发：

EPOLLIN的触发事件：当输入缓冲区为空-->低电平，当输入缓冲区不为空-->高电平

高电平的时候触发EPOLLIN事件，如果没有把缓冲区的数据读取完，下次还会触发的，因为始终是高电平

EPOLLOUT的触发事件：当发送缓冲区满-->低电平，当发送缓冲区不满-->高电平

高电平的时候触发EPOLLOUT事件，所以在一开始的时候不要关注EPOLLOUT时间，因为发送缓冲区是不满的所以会导致CPU忙等待，每次都触发。什么时候关注EPOLLOUT事件呢? 当write的时候没有写完全，因为发送缓冲区满了，这个时候才关注EPOLLOUT事件直到下次把所有数据都发送完毕了，才取消EPOLLOUT事件

ET边缘触发：

EPOLLIN事件发生的条件:

有数据到来(输入缓冲区初始为空，为低电平，有数据到来变成了高电平)

EPOLLout事件发生的条件:

内核发送缓冲区不满(当发送缓冲区出现满之后为低电平，然后内核发送出去了部分数据后变成了不满，也就是高电平)



## 说说NIO（select,poll,epoll） 







# Java

## 基础

 Java里的锁 

- 答：Synchronized、Volatile、ReentrantLock

- 追问：ReentrantLock和Synchronized的区别

  - > **补充**
    > 两者的共同点：
    >
    > 1. 都是用来协调多线程对共享对象、变量的访问
    > 2. 都是可重入锁，同一线程可以多次获得同一个锁
    > 3. 都保证了可见性和互斥性
    >    两者的不同点：
    > 4. ReentrantLock 显示的获得、释放锁，synchronized 隐式获得释放锁
    > 5. ReentrantLock 可响应中断、可轮回，synchronized 是不可以响应中断的，为处理锁的不可用性提供了更高的灵活性
    > 6. ReentrantLock 是 API 级别的，synchronized 是 JVM 级别的
    > 7. ReentrantLock 可以实现公平锁
    > 8. ReentrantLock 通过 Condition 可以绑定多个条件
    > 9. 底层实现不一样， synchronized 是同步阻塞，使用的是悲观并发策略，lock 是同步非阻塞，采用的是乐观并发策略
    > 10. Lock 是一个接口，而 synchronized 是 Java 中的关键字，synchronized 是内置的语言实现。
    > 11. synchronized 在发生异常时，会自动释放线程占有的锁，因此不会导致死锁现象发生；
    >     而 Lock 在发生异常时，如果没有主动通过 unLock()去释放锁，则很可能造成死锁现象，
    >     因此使用 Lock 时需要在 finally 块中释放锁。
    > 12. Lock 可以让等待锁的线程响应中断，而 synchronized 却不行，使用 synchronized 时，
    >     等待的线程会一直等待下去，不能够响应中断。
    > 13. 通过 Lock 可以知道有没有成功获取锁，而 synchronized 却无法办到。
    > 14. Lock 可以提高多个线程进行读操作的效率，既就是实现读写锁等

  - 如果希望加锁的时候，超过一段时间没有得到，会自动放弃，应该用哪种？（ReentrantLock）

- ArrayList是线程安全的么？多线程操作下会发生什么？

  - 多线程并发情况下出现ConcurrentModificationException并发修改异常，应该是这个！

- == 和 equals() 区别

- HashMap、TreeMap区别
  - 追问：HashMap能不能根据添加的顺序进行遍历？
    - 回不能，追问：能不能从底层实现原理解释一下，蒙的……说了一堆hash冲突的问题

- HashMap为什么用[红黑树](https://www.nowcoder.com/jump/super-jump/word?word=红黑树)，查找效率是多少

- C语言和Java语言的核心区别，有什么特点？
  - C面向过程，Java面向对象
  - C需要自己管理内存，Java自动实现（挖了个坑）
  - C不能跨平台，Java可以跨平台

-  Java资源（内存）管理是怎么管理的？ 

> Java的内存管理就是对象的分配和释放问题。（两部分）
>
> 分配 ：内存的分配是由程序完成的，程序员需要通过关键字new 为每个对象申请内存空间 (基本类型除外)，所有的对象都在堆 (Heap)中分配空间。
>
> 释放 ：对象的释放是由垃圾回收机制决定和执行的，这样做确实简化了程序员的工作。但同时，它也加重了JVM的工作。因为，GC为了能够正确释放对象，GC必须监控每一个对象的运行状态，包括对象的申请、引用、被引用、赋值等，GC都需要进行监控。

-  final关键字定义一个变量或对象，不可变是什么不可变？ 

-  Java中的集合，哪些线程安全，哪些不安全？ 
  - 说了Map、Collection（List、Set）
  - HashMap、Hashtable、HashSet等
  - HashMap不安全、Hashtable安全、ConcurrentHashMap安全、ArrayList不安全、Vector安全（弃用）
-  HashMap和ConcurrentHashMap底层有哪些不一样的？ 
  - HashMap：数组+[链表]()（jdk1.7）+[红黑树]()（jdk1.8）
  - ConcurrentHashMap：Segement+HashEntry（jdk1.7），数组+[链表]()+[红黑树]()（jdk1.8）
-  Java多线程里创建线程池，提供哪些接口？ 
  - newFixedThreadPool 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待
  - newScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行\
  - newSingleThreadExecutor 创建一个单线程化的线程池，它只会唯一的工作线程来执行任务，保证所有任务按照指定顺序（FIFO，LIFO，优先级）执行

## 多线程

```
https://www.jianshu.com/p/40d4c7aebd66
```



线程池

讲了线程池的参数与它的概念还有运行过程









###  synchronized 

- 从JVM规范中可以看到Synchonized在JVM里的实现原理，JVM基于进入和退出Monitor对
  象来实现方法同步和代码块同步，但两者的实现细节不一样。代码块同步是使用monitorenter
  和monitorexit指令实现的，而方法同步是使用另外一种方式实现的，细节在JVM规范里并没有
  详细说明。但是，方法的同步同样可以使用这两个指令来实现。
- monitorenter指令是在编译后插入到同步代码块的开始位置，而monitorexit是插入到方法结
  束处和异常处，JVM要保证每个monitorenter必须有对应的monitorexit与之配对。任何对象都有
  一个monitor与之关联，当且一个monitor被持有后，它将处于锁定状态。线程执行到monitorenter
  指令时，将会尝试获取对象所对应的monitor的所有权，即尝试获得对象的锁。

#### Java对象头

- synchronized用的锁是存在Java对象头里的。
- Java对象头里的Mark Word里默认存储对象的HashCode、分代年龄和锁标记位。



#### 锁的升级与对比

- 锁一共有4种状态，级别从低到高依次是：无锁状态、偏向锁状态、轻量级锁状
  态和重量级锁状态，这几个状态会随着竞争情况逐渐升级。锁可以升级但不能降级，意味着偏
  向锁升级成轻量级锁后不能降级成偏向锁。这种锁升级却不能降级的策略，目的是为了提高
  获得锁和释放锁的效率

##### 偏向锁

- HotSpot [1] 的作者经过研究发现，大多数情况下，锁不仅不存在多线程竞争，而且总是由同
  一线程多次获得，为了让线程获得锁的代价更低而引入了偏向锁。
- 当一个线程访问同步块并获取锁时，
  - 会在对象头和栈帧中的锁记录里存储锁偏向的线程ID，
  - 以后该线程在进入和退出同步块时不需要进行CAS操作来加锁和解锁，只需简单地测试一下对象头的Mark Word里是否存储着指向当前线程的偏向锁。
  - 如果测试成功，表示线程已经获得了锁。
  - 如果测试失败，则需要再测试一下Mark Word中偏向锁的标识是否设置成1（表示当前是偏向锁）：
  - 如果没有设置，则使用CAS竞争锁；
  - 如果设置了，则尝试使用CAS将对象头的偏向锁指向当前线程。
- （1）偏向锁的撤销
  - 偏向锁使用了一种等到竞争出现才释放锁的机制，所以当其他线程尝试竞争偏向锁时，
    持有偏向锁的线程才会释放锁。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有正
    在执行的字节码）。它会首先暂停拥有偏向锁的线程，然后检查持有偏向锁的线程是否活着，
    如果线程不处于活动状态，则将对象头设置成无锁状态；如果线程仍然活着，拥有偏向锁的栈
    会被执行，遍历偏向对象的锁记录，栈中的锁记录和对象头的Mark Word要么重新偏向于其他
    线程，要么恢复到无锁或者标记对象不适合作为偏向锁，最后唤醒暂停的线程。图2-1中的线
    程1演示了偏向锁初始化的流程，线程2演示了偏向锁撤销的流程。

##### 轻量级锁

- （1）轻量级锁加锁
  - 线程在执行同步块之前，JVM会先在当前线程的栈桢中创建用于存储锁记录的空间，并
    将对象头中的Mark Word复制到锁记录中，官方称为Displaced Mark Word。然后线程尝试使用
    CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失
    败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。
- （2）轻量级锁解锁
  - 轻量级解锁时，会使用原子的CAS操作将Displaced Mark Word替换回到对象头，如果成
    功，则表示没有竞争发生。如果失败，表示当前锁存在竞争，锁就会膨胀成重量级锁。图2-2是
    两个线程同时争夺锁，导致锁膨胀的流程图。

##### 锁的升级流程

- 每⼀个线程在准备获取共享资源时： 第⼀步，检查MarkWord⾥⾯是不是放的⾃⼰
  的ThreadId ,如果是，表示当前线程是处于 “偏向锁” 。
- 第⼆步，如果MarkWord不是⾃⼰的ThreadId，锁升级，这时候，⽤CAS来执⾏切
  换，新的线程根据MarkWord⾥⾯现有的ThreadId，通知之前线程暂停，之前线程
  将Markword的内容置为空。
- 第三步，两个线程都把锁对象的HashCode复制到⾃⼰新建的⽤于存储锁的记录空
  间，接着开始通过CAS操作， 把锁对象的MarKword的内容修改为⾃⼰新建的记录
  空间的地址的⽅式竞争MarkWord。
- 第四步，第三步中成功执⾏CAS的获得资源，失败的则进⼊⾃旋 。
- 第五步，⾃旋的线程在⾃旋过程中，成功获得资源(即之前获的资源的线程执⾏完
  成并释放了共享资源)，则整个状态依然处于 轻量级锁的状态，如果⾃旋失败 。
- 第六步，进⼊重量级锁的状态，这个时候，⾃旋的线程进⾏阻塞，等待之前线程执
  ⾏完成并唤醒⾃⼰。

####  synchronized修饰普通同步方法和静态同步方法的区别 

- 对于普通同步方法，锁是当前实例对象。
- 对于静态同步方法，锁是当前类的Class对象。
- 对于同步方法块，锁是Synchonized括号里配置的对象

### Volatile

- volatile是轻量级的synchronized，它在多处理器开发中保证了共享变量的“可见性”。可见性的意思是当一个线程修改一个共享变量时，另外一个线程能读到这个修改的值。如果volatile变量修饰符使用恰当的话，它比synchronized的使用和执行成本更低，因为它不会引起线程上下文的切换和调度
- 有volatile变量修饰的共享变量进行写操作的时候会多出一行汇编代码
  - 1）将当前处理器缓存行的数据写回到系统内存。
  - 2）这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效。
  - 为了提高处理速度，处理器不直接和内存进行通信，而是先将系统内存的数据读到内部缓存（L1，L2或其他）后再进行操作，但操作完不知道何时会写到内存。如果对声明了volatile的变量进行写操作，JVM就会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据写回到系统内存。
  - 但是，就算写回到内存，如果其他处理器缓存的值还是旧的，再执行计算操作就会有问题。所以，在多处理器下，为了保证各个处理器的缓存是一致的，就会实现缓存一致性协议，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存里。
- volatile的两条实现原则
  - 1）Lock前缀指令会引起处理器缓存回写到内存。Lock前缀指令导致在执行指令期间，声
    言处理器的LOCK#信号。在多处理器环境中，LOCK#信号确保在声言该信号期间，处理器可以
    独占任何共享内存 [2] 。但是，在最近的处理器里，LOCK＃信号一般不锁总线，而是锁缓存，毕
    竟锁总线开销的比较大。在8.1.4节有详细说明锁定操作对处理器缓存的影响，对于Intel486和
    Pentium处理器，在锁操作时，总是在总线上声言LOCK#信号。但在P6和目前的处理器中，如果
    访问的内存区域已经缓存在处理器内部，则不会声言LOCK#信号。相反，它会锁定这块内存区
    域的缓存并回写到内存，并使用缓存一致性机制来确保修改的原子性，此操作被称为“缓存锁
    定”，缓存一致性机制会阻止同时修改由两个以上处理器缓存的内存区域数据。
  - 2）一个处理器的缓存回写到内存会导致其他处理器的缓存无效。IA-32处理器和Intel 64处
    理器使用MESI（修改、独占、共享、无效）控制协议去维护内部缓存和其他处理器缓存的一致
    性。在多核处理器系统中进行操作的时候，IA-32和Intel 64处理器能嗅探其他处理器访问系统
    内存和它们的内部缓存。处理器使用嗅探技术保证它的内部缓存、系统内存和其他处理器的
    缓存的数据在总线上保持一致。例如，在Pentium和P6 family处理器中，如果通过嗅探一个处理
    器来检测其他处理器打算写内存地址，而这个地址当前处于共享状态，那么正在嗅探的处理
    器将使它的缓存行无效，在下次访问相同内存地址时，强制执行缓存行填充。



### 乐观锁 悲观锁

- 锁可以从不同的⻆度分类。其中，乐观锁和悲观锁是⼀种分类⽅式。
  - 悲观锁：
    - 悲观锁就是我们常说的锁。对于悲观锁来说，它总是认为每次访问共享资源时会发
      ⽣冲突，所以必须对每次数据操作加上锁，以保证临界区的程序同⼀时间只能有⼀
      个线程在执⾏。
  - 乐观锁：
    - 乐观锁⼜称为“⽆锁”，顾名思义，它是乐观派。乐观锁总是假设对共享资源的访问
      没有冲突，线程可以不停地执⾏，⽆需加锁也⽆需等待。⽽⼀旦多个线程发⽣冲
      突，乐观锁通常是使⽤⼀种称为CAS的技术来保证线程执⾏的安全性。
    - 由于⽆锁操作中没有锁的存在，因此不可能出现死锁的情况，也就是说乐观锁天⽣
      免疫死锁。
    - 乐观锁多⽤于“读多写少“的环境，避免频繁加锁影响性能；⽽悲观锁多⽤于”写多读
      少“的环境，避免频繁失败和重试影响性能。

###  Java如何实现线程安全 

（synchronized，ReentrantLock，AtomicInteger，ThreadLocal，CAS）

### 原子操作

- （1）使用总线锁保证原子性
  - 总线锁就是使用处理器提供的一个LOCK＃信号，当一个处理器在总线上输出此信号时，其他处理器的请求将被阻塞住，那么该处理器可以独占共享内存。
- （2）使用缓存锁保证原子性
  - 第二个机制是通过缓存锁定来保证原子性。在同一时刻，我们只需保证对某个内存地址
    的操作是原子性即可，但总线锁定把CPU和内存之间的通信锁住了，这使得锁定期间，其他处
    理器不能操作其他内存地址的数据，所以总线锁定的开销比较大，目前处理器在某些场合下
    使用缓存锁定代替总线锁定来进行优化。
  - 频繁使用的内存会缓存在处理器的L1、L2和L3高速缓存里，那么原子操作就可以直接在
    处理器内部缓存中进行，并不需要声明总线锁，在Pentium 6和目前的处理器中可以使用“缓存
    锁定”的方式来实现复杂的原子性。所谓“缓存锁定”是指内存区域如果被缓存在处理器的缓存
    行中，并且在Lock操作期间被锁定，那么当它执行锁操作回写到内存时，处理器不在总线上声
    言LOCK＃信号，而是修改内部的内存地址，并允许它的缓存一致性机制来保证操作的原子
    性，因为缓存一致性机制会阻止同时修改由两个以上处理器缓存的内存区域数据，当其他处
    理器回写已被锁定的缓存行的数据时，会使缓存行无效，在如图2-3所示的例子中，当CPU1修
    改缓存行中的i时使用了缓存锁定，那么CPU2就不能同时缓存i的缓存行。
- 但是有两种情况下处理器不会使用缓存锁定。
  - 第一种情况是：当操作的数据不能被缓存在处理器内部，或操作的数据跨多个缓存行
    （cache line）时，则处理器会调用总线锁定。
  - 第二种情况是：有些处理器不支持缓存锁定

###  CAS

#### Java如何实现原子操作

- 通过锁和循环CAS的方式来实现原子操作
- JVM中的CAS操作正是利用了处理器提供的CMPXCHG指令实现的。自旋CAS实现的基本思路就是循环进行CAS操作直到成功为止，

#### CAS实现原子操作的三大问题

##### ABA问题，

- 因为CAS需要在操作值的时候，检查值有没有发生变化，如果没有发生变化则更新，但是如果一个值原来是A，变成了B，又变成了A，那么使用CAS进行检查时会发现它的值没有发生变化，但是实际上却变化了。
- ABA问题的解决思路就是使用版本号。在变量前面追加上版本号，每次变量更新的时候把版本号加1，那么A→B→A就会变成1A→2B→3A。
- JDK的Atomic包里提供了一个类AtomicStampedReference来解决ABA问题。这个类的compareAndSet方法的作用是首先检查当前引用是否等于预期引用，并且检查当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。

##### 循环时间长开销大

- 自旋CAS如果长时间不成功，会给CPU带来非常大的执行开销。如果JVM能支持处理器提供的pause指令，那么效率会有一定的提升。
- pause指令有两个作用：
- 第一，它可以延迟流水线执行指令（de-pipeline），使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零；
- 第二，它可以避免在退出循环的时候因内存顺序冲突（Memory Order Violation）而引起CPU流水线被清空（CPU Pipeline Flush），从而提高CPU的执行效率。

##### 只能保证一个共享变量的原子操作。

- 当对一个共享变量执行操作时，我们可以使用循环CAS的方式来保证原子操作，但是对多个共享变量操作时，循环CAS就无法保证操作的原子性，这个时候就可以用锁。
- 还有一个取巧的办法，就是把多个共享变量合并成一个共享变量来操作。比如，有两个共享变量i＝2，j=a，合并一下ij=2a，然后用CAS来操作ij

##### 使用锁机制实现原子操作

- 锁机制保证了只有获得锁的线程才能够操作锁定的内存区域。JVM内部实现了很多种锁
  机制，有偏向锁、轻量级锁和互斥锁。有意思的是除了偏向锁，JVM实现锁的方式都用了循环
  CAS，即当一个线程想进入同步块的时候使用循环CAS的方式来获取锁，当它退出同步块的时
  候使用循环CAS释放锁。

#### CAS如何解决ABA问题

- 版本号或者时间戳



### 内存屏障

- 硬件层⾯，内存屏障分两种：读屏障（Load Barrier）和写屏障（Store Barrier）。内存屏障有两个作⽤：
  1. 阻⽌屏障两侧的指令重排序；
  2. 强制把写缓冲区/⾼速缓存中的脏数据等写回主内存，或者让缓存中相应的数据
  失效。

- 编译器在⽣成字节码时，会在指令序列中插⼊内存屏障来禁⽌特定类型的处理器重
  排序。编译器选择了⼀个⽐较保守的JMM内存屏障插⼊策略，这样可以保证在任何
  处理器平台，任何程序中都能得到正确的volatile内存语义。这个策略是：
  - 在每个volatile写操作前插⼊⼀个StoreStore屏障；
  - 在每个volatile写操作后插⼊⼀个StoreLoad屏障；
  - 在每个volatile读操作后插⼊⼀个LoadLoad屏障；
  - 在每个volatile读操作后再插⼊⼀个LoadStore屏障。
  - LoadLoad屏障：对于这样的语句Load1; LoadLoad; Load2，在Load2及后
    续读取操作要读取的数据被访问前，保证Load1要读取的数据被读取完毕。
  - StoreStore屏障：对于这样的语句Store1; StoreStore; Store2，在Store2及
    后续写⼊操作执⾏前，保证Store1的写⼊操作对其它处理器可⻅。
  - LoadStore屏障：对于这样的语句Load1; LoadStore; Store2，在Store2及后
    续写⼊操作被刷出前，保证Load1要读取的数据被读取完毕。
  - StoreLoad屏障：对于这样的语句Store1; StoreLoad; Load2，在Load2及后
    续所有读取操作执⾏前，保证Store1的写⼊对所有处理器可⻅。它的开销是
    四种屏障中最⼤的（冲刷写缓冲器，清空⽆效化队列）。在⼤多数处理器的
    实现中，这个屏障是个万能屏障，兼具其它三种内存屏障的功能
  - ![1626075498121](面经.assets/1626075498121.png)

### ThreadLocal

- ThreadLocal，即线程变量，是一个以ThreadLocal对象为键、任意对象为值的存储结构。这
  个结构被附带在线程上，也就是说一个线程可以根据一个ThreadLocal对象查询到绑定在这个
  线程上的一个值。
- 可以通过set(T)方法来设置一个值，在当前线程下再通过get()方法获取到原先设置的值











### Lock

- 锁是用来控制多个线程访问共享资源的方式，一般来说，一个锁能够防止多个线程同时
  访问共享资源（但是有些锁可以允许多个线程并发的访问共享资源，比如读写锁）。在Lock接
  口出现之前，Java程序是靠synchronized关键字实现锁功能的，而Java SE 5之后，并发包中新增
  了Lock接口（以及相关实现类）用来实现锁功能，它提供了与synchronized关键字类似的同步功
  能，只是在使用时需要显式地获取和释放锁。虽然它缺少了（通过synchronized块或者方法所提
  供的）隐式获取释放锁的便捷性，但是却拥有了锁获取与释放的可操作性、可中断的获取锁以
  及超时获取锁等多种synchronized关键字所不具备的同步特性。



### 队列同步AQS

- 用来构建锁或者其他同步组件的基础框架，它使用了一个int成员变量表示同步状态，通过内置的FIFO队列来完成资源获取线程的排队工作
- 同步器的主要使用方式是继承，子类通过继承同步器并实现它的抽象方法来管理同步状
  态，在抽象方法的实现过程中免不了要对同步状态进行更改，这时就需要使用同步器提供的3
  个方法（getState()、setState(int newState)和compareAndSetState(int expect,int update)）来进行操
  作，因为它们能够保证状态的改变是安全的
- 锁是面向使用者的，它定义了使用者与锁交
  互的接口（比如可以允许两个线程并行访问），隐藏了实现细节；
- 同步器面向的是锁的实现者，
  它简化了锁的实现方式，屏蔽了同步状态管理、线程的排队、等待与唤醒等底层操作。锁和同
  步器很好地隔离了使用者和实现者所需关注的领域。

#### 队列同步器的接口

- 使用者需要继承同步器并重写指定的
  方法，随后将同步器组合在自定义同步组件的实现中，并调用同步器提供的模板方法，而这些
  模板方法将会调用使用者重写的方法
- 重写同步器指定的方法时，需要使用同步器提供的如下3个方法来访问或修改同步状态。
  - getState()：获取当前同步状态。
  - setState(int newState)：设置当前同步状态。
  - compareAndSetState(int expect,int update)：使用CAS设置当前状态，该方法能够保证状态
    设置的原子性。
- 同步器可重写的方法
  - ![1625995837926](面经.assets/1625995837926.png)
- 同步器提供的模板方法
  - ![1625995825671](面经.assets/1625995825671.png)
- 同步器提供的模板方法基本上分为3类：独占式获取与释放同步状态、共享式获取与释放
  同步状态和查询同步队列中的等待线程情况

#### 队列同步器的实现

##### 同步队列

- 同步器依赖内部的同步队列（一个FIFO双向队列）来完成同步状态的管理，当前线程获取
  同步状态失败时，同步器会将当前线程以及等待状态等信息构造成为一个节点（Node）并将其
  加入同步队列，同时会阻塞当前线程，当同步状态释放时，会把首节点中的线程唤醒，使其再
  次尝试获取同步状态。

##### 结点

- 节点是构成同步队列的基础，同步器拥有首节点（head）和尾节点（tail），没有成功获取同步状态的线程将会成为节点加入该队列的尾部，
- ![1625995945965](面经.assets/1625995945965.png)
- 同步器包含了两个节点类型的引用，一个指向头节点，而另一个指向尾节点。
  试想一下，当一个线程成功地获取了同步状态（或者锁），其他线程将无法获取到同步状态，转
  而被构造成为节点并加入到同步队列中，而这个加入队列的过程必须要保证线程安全，因此
  同步器提供了一个基于CAS的设置尾节点的方法：compareAndSetTail(Node expect,Node
  update)，它需要传递当前线程“认为”的尾节点和当前节点，只有设置成功后，当前节点才正式
  与之前的尾节点建立关联
- 同步队列遵循FIFO，首节点是获取同步状态成功的节点，首节点的线程在释放同步状态
  时，将会唤醒后继节点，而后继节点将会在获取同步状态成功时将自己设置为首节点
- 设置首节点是通过获取同步状态成功的线程来完成的，由于只有一个线程能
  够成功获取到同步状态，因此设置头节点的方法并不需要使用CAS来保证，它只需要将首节
  点设置成为原首节点的后继节点并断开原首节点的next引用即可。

##### 独占式同步状态获取与释放

- 通过调用同步器的acquire(int arg)方法可以获取同步状态，该方法对中断不敏感，也就是
  由于线程获取同步状态失败后进入同步队列中，后续对线程进行中断操作时，线程不会从同
  步队列中移出
- tryAcquire()
  - 首先调用自定义同步器实现的tryAcquire(int arg)方法，该方法
    保证线程安全的获取同步状态，如果同步状态获取失败，则构造同步节点（独占式
    Node.EXCLUSIVE，同一时刻只能有一个线程成功获取同步状态）并通过addWaiter(Node node)
    方法将该节点加入到同步队列的尾部，最后调用acquireQueued(Node node,int arg)方法，使得该
    节点以“死循环”的方式获取同步状态。如果获取不到则阻塞节点中的线程，而被阻塞线程的
    唤醒主要依靠前驱节点的出队或阻塞线程被中断来实现。
- compareAndSetTail(Node expect,Node update)方法来确保节点能够被线程安全添加。ReentrantLock
- enq(final Node node)方法中，同步器通过“死循环”来保证节点的正确添加，在“死循
  环”中只有通过CAS将节点设置成为尾节点之后，当前线程才能从该方法返回，否则，当前线
  程不断地尝试设置。可以看出，enq(final Node node)方法将并发添加节点的请求通过CAS变
  得“串行化”了。
- 节点进入同步队列之后，就进入了一个自旋的过程，每个节点（或者说每个线程）都在自
  省地观察，当条件满足，获取到了同步状态，就可以从这个自旋过程中退出，否则依旧留在这
  个自旋过程中（并会阻塞节点的线程）
- 在acquireQueued(final Node node,int arg)方法中，当前线程在“死循环”中尝试获取同步状
  态，而只有前驱节点是头节点才能够尝试获取同步状态，这是为什么？
  - 第一，头节点是成功获取到同步状态的节点，而头节点的线程释放了同步状态之后，将会
    唤醒其后继节点，后继节点的线程被唤醒后需要检查自己的前驱节点是否是头节点。
  - 第二，维护同步队列的FIFO原则。

##### 共享式同步状态获取与释放

- 共享式获取与独占式获取最主要的区别在于同一时刻能否有多个线程同时获取到同步状
  态。以文件的读写为例，如果一个程序在对文件进行读操作，那么这一时刻对于该文件的写操
  作均被阻塞，而读操作能够同时进行。写操作要求对资源的独占式访问，而读操作可以是共享
  式访问
- 在acquireShared(int arg)方法中，同步器调用tryAcquireShared(int arg)方法尝试获取同步状
  态，tryAcquireShared(int arg)方法返回值为int类型，当返回值大于等于0时，表示能够获取到同
  步状态。
- 因此，在共享式获取的自旋过程中，成功获取到同步状态并退出自旋的条件就是
  tryAcquireShared(int arg)方法返回值大于等于0。可以看到，在doAcquireShared(int arg)方法的自
  旋过程中，如果当前节点的前驱为头节点时，尝试获取同步状态，如果返回值大于等于0，表示
  该次获取同步状态成功并从自旋过程中退出。

#### 可重入锁是什么，非可重入锁又是什么 

- 可重入锁
  - 任意线程在获取到锁之后能够再次获取该锁而不会被锁所阻塞，
    - 线程再次获取锁。锁需要去识别获取锁的线程是否为当前占据锁的线程，如果是，则再次成功获取。
    - 锁的最终释放。线程重复n次获取了锁，随后在第n次释放该锁后，其他线程能够获取到该锁。
    - 锁的最终释放要求锁对于获取进行计数自增，计数表示当前锁被重复获取的次数，而锁被释放时，计数自减，当计数等于0时表示锁已经成功释放。
    - 成功获取锁的线程再次获取锁，只是增加了同步状态值，这也就要求ReentrantLock在释放同步状态时减少同步状态值
    - 如果该锁被获取了n次，那么前(n-1)次tryRelease(int releases)方法必须返回false，而只有同
      步状态完全释放了，才能返回true。可以看到，该方法将同步状态是否为0作为最终释放的条
      件，当同步状态为0时，将占有线程设置为null，并返回true，表示释放成功。
  - 该锁的还支持获取锁时的公平和非公平性选择
    - 如果在绝对时间上，先对锁进行获取的请求一定先被满足，那么这个锁是公平的，反之，是不公平的。
    - 公平的获取锁，也就是等待时间最长的线程最优先获取锁，也可以说锁获取是顺序的
    - 公平锁能够减少“饥饿”发生的概率，等待越久的请求越是能够得到优先满足。
    - tryAcquire()方法不同的点在于唯一不同的位置为判断条件多了hasQueuedPredecessors()方法，即加入了同步队列中当前节点是否有前驱节点的判断，如果该方法返回true，则表示有线程比当前线程更早地请求获取锁，因此需要等待前驱线程获取并释放锁之后才能继续获取锁。
    - **为什么会出现线程连续获取锁的情况呢**？
      - 回顾nonfairTryAcquire(int acquires)方法，当一个线程请求锁时，只要获取了同步状态即成功获取锁。在这个前提下，刚释放锁的线程再次获取同步状态的几率会非常大
    - **非公平性锁可能使线程“饥饿”，为什么它又被设定成默认的实现呢**？
      - 如果把每次不同线程获取到锁定义为1次切换，公平性锁在测试中进行了10次切换，而非
        公平性锁只有5次切换，这说明非公平性锁的开销更小
      - 公平性锁保证了锁的获取按照FIFO原则，而代价是进行大量的线程切换。
      - 非公平性锁虽然可能造成线程“饥饿”，但极少的线程切换，保证了其更大的吞吐量。
  - synchronized关键字隐式的支持重进入
- 非可重入锁
  - 当一个线程调用Mutex的lock()方法获取锁之后，如果再次调用lock()方法，则该线程将会被自己所阻塞

### 读写锁 ReentrantReadWriteLock

- 读写锁的自定义同步器需要在同步状态（一个整型变量）上维护多个读线程和一个写线程的状态，使得该状态的设计成为读写锁实现的关键

- 读写锁将变量切分成了两个部分，高16位表示读，低16位表示写，

- 当前同步状态表示一个线程已经获取了写锁，且重进入了两次，同时也连续获取了两次读锁。读写锁是如何迅速确定读和写各自的状态呢？
  - 答案是通过位运算。
  - 假设当前同步状态值为S，写状态等于S&0x0000FFFF（将高16位全部抹去），
  - 读状态等于S>>>16（无符号补0右移16位）。
  - 当写状态增加1时，等于S+1，当读状态增加1时，等于S+(1<<16)，也就是S+0x00010000。
  - S不等于0时，当写状态（S&0x0000FFFF）等于0时，则读状态（S>>>16）大于0，即读锁已被获取。

- readLock()
  - 读锁是一个支持重进入的共享锁，它能够被多个线程同时获取，在没有其他写线程访问（或者写状态为0）时，读锁总会被成功地获取，而所做的也只是（线程安全的）增加读状态。如果当前线程已经获取了读锁，则增加读状态。如果当前线程在获取读锁时，写锁已被其他线程获取，则进入等待状态
  - 读状态是所有线程获取读锁次数的总和，而每个线程各自获取读锁的次数只能选择保存在ThreadLocal中
- writeLock()
  - 写锁是一个支持重进入的排它锁。如果当前线程已经获取了写锁，则增加写状态。如果当前线程在获取写锁时，读锁已经被获取（读状态不为0）或者该线程不是已经获取写锁的线程，则当前线程进入等待状态
  - 该方法除了重入条件（当前线程为获取了写锁的线程）之外，增加了一个读锁是否存在的判断。如果存在读锁，则写锁不能被获取，原因在于：读写锁要确保写锁的操作对读锁可见，如果允许读锁在已被获取的情况下对写锁的获取，那么正在运行的其他读线程就无法感知到当前写线程的操作。因此，只有等待其他读线程都释放了读锁，写锁才能被当前线程获取，而写锁一旦被获取，则其他读写线程的后续访问均被阻塞。
  - 写锁的释放与ReentrantLock的释放过程基本类似，每次释放均减少写状态，当写状态为0时表示写锁已被释放，从而等待的读写线程能够继续访问读写锁，同时前次写线程的修改对后续读写线程可见。
- 降级锁
  - 锁降级指的是写锁降级成为读锁。把持住（当前拥有的）写锁，再获取到读锁，随后释放（先前拥有的）写锁的过程
  - 锁降级中读锁的获取是否必要呢？答案是必要的。主要是为了保证数据的可见性，如果当前线程不获取读锁而是直接释放写锁，假设此刻另一个线程（记作线程T）获取了写锁并修改了数据，那么当前线程无法感知线程T的数据更新。如果当前线程获取读锁，即遵循锁降级的步骤，则线程T将会被阻塞，直到当前线程使用数据并释放读锁之后，线程T才能获取写锁进行数据更新。



### Condition接口

- Condition接口也提供了类似Object的监视器方法，与Lock配合可以实现等待/通知模式，
- Condition定义了等待/通知两种类型的方法，当前线程调用这些方法时，需要提前获取到Condition对象关联的锁。Condition对象是由Lock对象（调用Lock对象的newCondition()方法）创建出来的，换句话说，Condition是依赖Lock对象的。
- Condition的使用方式比较简单，需要注意在调用方法前获取锁，
- 一般都会将Condition对象作为成员变量。当调用await()方法后，当前线程会释放锁并在此等待，而其他线程调用Condition对象的signal()方法，通知当前线程后，当前线程才从await()方法返回，并且在返回前已经获取了锁。

#### Condition的实现

##### 等待队列

- 等待队列是一个FIFO的队列，在队列中的每个节点都包含了一个线程引用，该线程就是
  在Condition对象上等待的线程，如果一个线程调用了Condition.await()方法，那么该线程将会
  释放锁、构造成节点加入等待队列并进入等待状态。事实上，节点的定义复用了同步器中节点
  的定义，也就是说，同步队列和等待队列中节点类型都是同步器的静态内部类
  AbstractQueuedSynchronizer.Node。
- 一个Condition包含一个等待队列，Condition拥有首节点（firstWaiter）和尾节点
  （lastWaiter）。当前线程调用Condition.await()方法，将会以当前线程构造节点，并将节点从尾部
  加入等待队列
- Condition拥有首尾节点的引用，而新增节点只需要将原有的尾节点nextWaiter
  指向它，并且更新尾节点即可。上述节点引用更新的过程并没有使用CAS保证，原因在于调用
  await()方法的线程必定是获取了锁的线程，也就是说该过程是由锁来保证线程安全的。

##### 等待

- 调用Condition的await()方法（或者以await开头的方法），会使当前线程进入等待队列并释
  放锁，同时线程状态变为等待状态。当从await()方法返回时，当前线程一定获取了Condition相
  关联的锁。
- 如果从队列（同步队列和等待队列）的角度看await()方法，当调用await()方法时，相当于同步队列的首节点（获取了锁的节点）移动到Condition的等待队列中。然后释放同步状态，唤醒同步队列中的后继节点，然后当前线程会进入等待状态。(同步队列的首节点并不会直接加入等待队列，而是通过addConditionWaiter()方
  法把当前线程构造成一个新的节点并将其加入等待队列中。)
- 当等待队列中的节点被唤醒，则唤醒节点的线程开始尝试获取同步状态。如果不是通过其他线程调用Condition.signal()方法唤醒，而是对等待线程进行中断，则会抛出InterruptedException。

##### 通知

- 调用Condition的signal()方法，将会唤醒在等待队列中等待时间最长的节点（首节点），在
  唤醒节点之前，会将节点移到同步队列中。
- 通过调用同步器的enq(Node node)方法，等待队列中的头节点线程安全地移动到同步队
  列。当节点移动到同步队列后，当前线程再使用LockSupport唤醒该节点的线程。
- 被唤醒后的线程，将从await()方法中的while循环中退出（isOnSyncQueue(Node node)方法
  返回true，节点已经在同步队列中），进而调用同步器的acquireQueued()方法加入到获取同步状
  态的竞争中。
- 成功获取同步状态（或者说锁）之后，被唤醒的线程将从先前调用的await()方法返回，此
  时该线程已经成功地获取了锁。
- Condition的signalAll()方法，相当于对等待队列中的每个节点均执行一次signal()方法，效
  果就是将等待队列中所有节点全部移动到同步队列中，并唤醒每个节点的线程。



### ConcurrentHashMap

#### 为什么使用ConcurrentHashMap

- 并发编程中使用HashMap可能导致程序死循环。而使用线程安全的HashTable效率又非
  常低下，基于以上两个原因，便有了ConcurrentHashMap的登场机会
- 线程不安全的HashMap
  - HashMap在并发执行put操作时会引起死循环，是因为多线程会导致HashMap的Entry链表
    形成环形数据结构，一旦形成环形数据结构，Entry的next节点永远不为空，就会产生死循环获
    取Entry。
- 效率低下的HashTable
  - HashTable容器使用synchronized来保证线程安全，但在线程竞争激烈的情况下HashTable
    的效率非常低下。因为当一个线程访问HashTable的同步方法，其他线程也访问HashTable的同
    步方法时，会进入阻塞或轮询状态。如线程1使用put进行元素添加，线程2不但不能使用put方
    法添加元素，也不能使用get方法来获取元素，所以竞争越激烈效率越低
- ConcurrentHashMap的锁分段技术可有效提升并发访问率
  - 假如容器里有多把锁，每一把锁用于锁容器其中一部分数据，那么
    当多线程访问容器里不同数据段的数据时，线程间就不会存在锁竞争，从而可以有效提高并
    发访问效率，这就是ConcurrentHashMap所使用的锁分段技术。首先将数据分成一段一段地存
    储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数
    据也能被其他线程访问

#### ConcurrentHashMap结构

- ConcurrentHashMap是由Segment数组结构和HashEntry数组结构组成。Segment是一种可重
  入锁（ReentrantLock），在ConcurrentHashMap里扮演锁的角色；HashEntry则用于存储键值对数
  据。一个ConcurrentHashMap里包含一个Segment数组。Segment的结构和HashMap类似，是一种
  数组和链表结构。一个Segment里包含一个HashEntry数组，每个HashEntry是一个链表结构的元
  素，每个Segment守护着一个HashEntry数组里的元素，当对HashEntry数组的数据进行修改时，
  必须首先获得与它对应的Segment锁

![1625997188009](面经.assets/1625997188009.png)

#### ConcurrentHashMap的初始化

ConcurrentHashMap初始化方法是通过initialCapacity、loadFactor和concurrencyLevel等几个
参数来初始化segment数组、段偏移量segmentShift、段掩码segmentMask和每个segment里的
HashEntry数组来实现的

##### 初始化segments数组

- segments数组的长度ssize是通过concurrencyLevel计算得出的。为了能
  通过按位与的散列算法来定位segments数组的索引，必须保证segments数组的长度是2的N次方
  （power-of-two size），所以必须计算出一个大于或等于concurrencyLevel的最小的2的N次方值
  来作为segments数组的长度。假如concurrencyLevel等于14、15或16，ssize都会等于16，即容器里
  锁的个数也是16。
- concurrencyLevel的最大值是65535，这意味着segments数组的长度最大为65536，
  对应的二进制是16位。

##### 初始化segmentShift和segmentMask

- 这两个全局变量需要在定位segment时的散列算法里使用，sshift等于ssize从1向左移位的
  次数，在默认情况下concurrencyLevel等于16，1需要向左移位移动4次，所以sshift等于4。
  segmentShift用于定位参与散列运算的位数，segmentShift等于32减sshift，所以等于28，这里之所
  以用32是因为ConcurrentHashMap里的hash()方法输出的最大数是32位的，后面的测试中我们
  可以看到这点。segmentMask是散列运算的掩码，等于ssize减1，即15，掩码的二进制各个位的
  值都是1。因为ssize的最大长度是65536，所以segmentShift最大值是16，segmentMask最大值是
  65535，对应的二进制是16位，每个位都是1。

##### 初始化每个segment

- 输入参数initialCapacity是ConcurrentHashMap的初始化容量，loadfactor是每个segment的负
  载因子，在构造方法里需要通过这两个参数来初始化数组中的每个segment。
- 上面代码中的变量cap就是segment里HashEntry数组的长度，它等于initialCapacity除以ssize
  的倍数c，如果c大于1，就会取大于等于c的2的N次方值，所以cap不是1，就是2的N次方。
  segment的容量threshold＝（int）cap*loadFactor，默认情况下initialCapacity等于16，loadfactor等于
  0.75，通过运算cap等于1，threshold等于零

##### 定位Segment

- 既然ConcurrentHashMap使用分段锁Segment来保护不同段的数据，那么在插入和获取元素
  的时候，必须先通过散列算法定位到Segment。可以看到ConcurrentHashMap会首先使用
  Wang/Jenkins hash的变种算法对元素的hashCode进行一次再散列。
- 之所以进行再散列，目的是减少散列冲突，使元素能够均匀地分布在不同的Segment上，
  从而提高容器的存取效率



#### ConcurrentHashMap的操作

##### get操作

- Segment的get操作实现非常简单和高效。先经过一次再散列，然后使用这个散列值通过散
  列运算定位到Segment，再通过散列算法定位到元素
- get操作的高效之处在于整个get过程不需要加锁，除非读到的值是空才会加锁重读。
  - 原因是它的get方法里将要使用的共享变量都定义成volatile类型，
    - 如用于统计当前Segement大小的count字段和用于存储值的HashEntry的value。定义成volatile的变量，能够在线程之间保持可见性，能够被多线程同时读，并且保证不会读到过期的值，但是只能被单线程写（有一种情况可以被多线程写，就是写入的值不依赖于原值），在get操作里只需要读不需要写共享变量count和value，所以可以不用加锁。
    - 之所以不会读到过期的值，是因为根据Java内存模型的happen before原则，对volatile字段的写入操作先于读操作，即使两个线程同时修改和获取volatile变量，get操作也能拿到最新的值，这是用volatile替换锁的经典应用场景

##### put操作

- 由于put方法里需要对共享变量进行写入操作，所以为了线程安全，在操作共享变量时必
  须加锁。put方法首先定位到Segment，然后在Segment里进行插入操作。插入操作需要经历两个
  步骤，第一步判断是否需要对Segment里的HashEntry数组进行扩容，第二步定位添加元素的位
  置，然后将其放在HashEntry数组里
- 是否需要扩容
  - 在插入元素前会先判断Segment里的HashEntry数组是否超过容量（threshold），如果超过阈
    值，则对数组进行扩容。值得一提的是，Segment的扩容判断比HashMap更恰当，因为HashMap
    是在插入元素后判断元素是否已经到达容量的，如果到达了就进行扩容，但是很有可能扩容
    之后没有新元素插入，这时HashMap就进行了一次无效的扩容。
- 如何扩容
  - 在扩容的时候，首先会创建一个容量是原来容量两倍的数组，然后将原数组里的元素进
    行再散列后插入到新的数组里。为了高效，ConcurrentHashMap不会对整个容器进行扩容，而只
    对某个segment进行扩容。

##### size操作

Segment里的全局变量count是一个volatile变量



### ConcurrentLinkedQueue

- 在并发编程中，有时候需要使用线程安全的队列。如果要实现一个线程安全的队列有两
  种方式：一种是使用阻塞算法，另一种是使用非阻塞算法。使用阻塞算法的队列可以用一个锁
  （入队和出队用同一把锁）或两个锁（入队和出队用不同的锁）等方式来实现。非阻塞的实现方
  式则可以使用循环CAS的方式来实现
- ConcurrentLinkedQueue是一个基于链接节点的无界线程安全队列，它采用先进先出的规
  则对节点进行排序，当我们添加一个元素的时候，它会添加到队列的尾部；当我们获取一个元
  素时，它会返回队列头部的元素。它采用了“wait-free”算法（即CAS算法）来实现

#### ConcurrentLinkedQueue的结构

- ConcurrentLinkedQueue由head节点和tail节点组成，每个节点（Node）由节点元素（item）和
  指向下一个节点（next）的引用组成，节点与节点之间就是通过这个next关联起来，从而组成一
  张链表结构的队列。默认情况下head节点存储的元素为空，tail节点等于head节点。

#### 入队列

- 入队列就是将入队节点添加到队列的尾部。

- 发现入队主要做两件事情：
  - 第一是将入队节点设置成当前队列尾节点的下一个节点；
  - 第二是更新tail节点，如果tail节点的next节点不为空，则将入队节点设置成tail节点，如果tail节点的next节点为空，则将入队节点设置成tail的next节点，所以tail节点不总是尾节点
- 如果有一个线程正在入队，那么它必须先获取尾节点，然后设置尾节点的下一个节点为入队节点，但这时可能有另外一个线程插队了，那么队列的尾节点就会发生变化，这时当前线程要暂停入队操作，然后重新获取尾节点
- 从源代码角度来看，整个入队过程主要做两件事情第一是定位出尾节点；第二是使用CAS算法将入队节点设置成尾节点的next节点，如不成功则重试
- 定位尾节点
  - tail节点并不总是尾节点，所以每次入队都必须先通过tail节点来找到尾节点。
  - 尾节点可能是tail节点，也可能是tail节点的next节点。代码中循环体中的第一个if就是判断tail是否有next节点，有则表示next节点可能是尾节点。
  - 获取tail节点的next节点需要注意的是p节点等于p的next节点的情况，只有一种可能就是p节点和p的next节点都等于空，表示这个队列刚初始化，正准备添加节点，所以需要返回head节点
- 设置入队结点为尾结点
  - p.casNext（null，n）方法用于将入队节点设置为当前队列尾节点的next节点，如果p是null，
    表示p是当前队列的尾节点，如果不为null，表示有其他线程更新了尾节点，则需要重新获取当
    前队列的尾节点。
- Hops
  - 让tail节点永远作为队列的尾节点，这样实现代码量非常少，而且逻辑清晰和易懂。但是，
    这么做有个缺点，每次都需要使用循环CAS更新tail节点。如果能减少CAS更新tail节点的次
    数，就能提高入队的效率，
  - 所以doug lea使用hops变量来控制并减少tail节点的更新频率，并不是每次节点入队后都将tail节点更新成尾节点，而是当tail节点和尾节点的距离大于等于常量HOPS的值（默认等于1）时才更新tail节点，tail和尾节点的距离越长，使用CAS更新tail节点的次数就会越少，
  - 但是距离越长带来的负面效果就是每次入队时定位尾节点的时间就越长，因为循环体需要多循环一次来定位出尾节点，但是这样仍然能提高入队的效率，因为从本质上来看它通过增加对volatile变量的读操作来减少对volatile变量的写操作，而对volatile变量的写操作开销要远远大于读操作，所以入队效率会有所提升。
- 入队方法永远返回true，所以不要通过返回值判断入队是否成功。

#### 出队列

- 出队列的就是从队列里返回一个节点元素，并清空该节点对元素的引

- 并不是每次出队时都更新head节点，当head节点里有元素时，直接弹出head
  节点里的元素，而不会更新head节点。只有当head节点里没有元素时，出队操作才会更新head
  节点。这种做法也是通过hops变量来减少使用CAS更新head节点的消耗，从而提高出队效率。
- 首先获取头节点的元素，然后判断头节点元素是否为空，如果为空，表示另外一个线程已
  经进行了一次出队操作将该节点的元素取走，如果不为空，则使用CAS的方式将头节点的引
  用设置成null，如果CAS成功，则直接返回头节点的元素，如果不成功，表示另外一个线程已经
  进行了一次出队操作更新了head节点，导致元素发生了变化，需要重新获取头节点。

### 阻塞队列

- 阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。这两个附加的操作支持阻塞
  的插入和移除方法。

- 支持阻塞的插入方法：意思是当队列满时，队列会阻塞插入元素的线程，直到队列不满。
- 支持阻塞的移除方法：意思是在队列为空时，获取元素的线程会等待队列变为非空

- 阻塞队列常用于生产者和消费者的场景，生产者是向队列里添加元素的线程，消费者是
  从队列里取元素的线程。阻塞队列就是生产者用来存放元素、消费者用来获取元素的容器。

- 在阻塞队列不可用时，这两个附加操作提供了4种处理方式
  - 抛出异常：当队列满时，如果再往队列里插入元素，会抛出IllegalStateException（"Queue
    full"）异常。当队列空时，从队列里获取元素会抛出NoSuchElementException异常。
  - 返回特殊值：当往队列插入元素时，会返回元素是否插入成功，成功返回true。如果是移
    除方法，则是从队列里取出一个元素，如果没有则返回null。
  - 一直阻塞：当阻塞队列满时，如果生产者线程往队列里put元素，队列会一直阻塞生产者
    线程，直到队列可用或者响应中断退出。当队列空时，如果消费者线程从队列里take元素，队
    列会阻塞住消费者线程，直到队列不为空。
  - 超时退出：当阻塞队列满时，如果生产者线程往队列里插入元素，队列会阻塞生产者线程
    一段时间，如果超过了指定的时间，生产者线程就会退出。

#### Java里的阻塞队列

- ArrayBlockingQueue：一个由数组结构组成的有界阻塞队列。
  - ArrayBlockingQueue是一个用数组实现的有界阻塞队列。此队列按照先进先出（FIFO）的原
    则对元素进行排序。
  - 默认情况下不保证线程公平的访问队列，所谓公平访问队列是指阻塞的线程，可以按照
    阻塞的先后顺序访问队列，即先阻塞线程先访问队列
- LinkedBlockingQueue：一个由链表结构组成的有界阻塞队列。
  - LinkedBlockingQueue是一个用链表实现的有界阻塞队列。此队列的默认和最大长度为
    Integer.MAX_VALUE。此队列按照先进先出的原则对元素进行排序。
- PriorityBlockingQueue：一个支持优先级排序的无界阻塞队列。
  - PriorityBlockingQueue是一个支持优先级的无界阻塞队列。默认情况下元素采取自然顺序
    升序排列。也可以自定义类实现compareTo()方法来指定元素排序规则，或者初始化
    PriorityBlockingQueue时，指定构造参数Comparator来对元素进行排序。需要注意的是不能保证
    同优先级元素的顺序。
- DelayQueue：一个使用优先级队列实现的无界阻塞队列。
  - DelayQueue是一个支持延时获取元素的无界阻塞队列。队列使用PriorityQueue来实现。队
    列中的元素必须实现Delayed接口，在创建元素时可以指定多久才能从队列中获取当前元素。
    只有在延迟期满时才能从队列中提取元素
  - DelayQueue非常有用，可以将DelayQueue运用在以下应用场景。
  - 缓存系统的设计：可以用DelayQueue保存缓存元素的有效期，使用一个线程循环查询
    DelayQueue，一旦能从DelayQueue中获取元素时，表示缓存有效期到了。
  - 定时任务调度：使用DelayQueue保存当天将会执行的任务和执行时间，一旦从
    DelayQueue中获取到任务就开始执行，比如TimerQueue就是使用DelayQueue实现的。
- SynchronousQueue：一个不存储元素的阻塞队列。
  - SynchronousQueue是一个不存储元素的阻塞队列。每一个put操作必须等待一个take操作，
    否则不能继续添加元素。
  - 它支持公平访问队列。默认情况下线程采用非公平性策略访问队列。使用以下构造方法
    可以创建公平性访问的SynchronousQueue，如果设置为true，则等待的线程会采用先进先出的
    顺序访问队列。
  - SynchronousQueue可以看成是一个传球手，负责把生产者线程处理的数据直接传递给消费
    者线程。队列本身并不存储任何元素，非常适合传递性场景。SynchronousQueue的吞吐量高于
    LinkedBlockingQueue和ArrayBlockingQueue。
- LinkedTransferQueue：一个由链表结构组成的无界阻塞队列。
  - LinkedTransferQueue是一个由链表结构组成的无界阻塞TransferQueue队列。相对于其他阻
    塞队列，LinkedTransferQueue多了tryTransfer和transfer方法。
  - transfer方法
    - 如果当前有消费者正在等待接收元素（消费者使用take()方法或带时间限制的poll()方法
      时），transfer方法可以把生产者传入的元素立刻transfer（传输）给消费者。如果没有消费者在等
      待接收元素，transfer方法会将元素存放在队列的tail节点，并等到该元素被消费者消费了才返
      回
  - tryTransfer方法
    - tryTransfer方法是用来试探生产者传入的元素是否能直接传给消费者。如果没有消费者等
      待接收元素，则返回false。和transfer方法的区别是tryTransfer方法无论消费者是否接收，方法
      立即返回，而transfer方法是必须等到消费者消费了才返回。
      对于带有时间限制的tryTransfer（E e，long timeout，TimeUnit unit）方法，试图把生产者传入
      的元素直接传给消费者，但是如果没有消费者消费该元素则等待指定的时间再返回，如果超
      时还没消费元素，则返回false，如果在超时时间内消费了元素，则返回true。
- LinkedBlockingDeque：一个由链表结构组成的双向阻塞队列。
  - LinkedBlockingDeque是一个由链表结构组成的双向阻塞队列。所谓双向队列指的是可以
    从队列的两端插入和移出元素。双向队列因为多了一个操作队列的入口，在多线程同时入队
    时，也就减少了一半的竞争。相比其他的阻塞队列，LinkedBlockingDeque多了addFirst、
    addLast、offerFirst、offerLast、peekFirst和peekLast等方法，以First单词结尾的方法，表示插入、
    获取（peek）或移除双端队列的第一个元素。以Last单词结尾的方法，表示插入、获取或移除双
    端队列的最后一个元素。另外，插入方法add等同于addLast，移除方法remove等效于
    removeFirst。但是take方法却等同于takeFirst，

#### 阻塞队列的实现原理

- 使用通知模式实现。所谓通知模式，就是当生产者往满的队列里添加元素时会阻塞住生
  产者，当消费者消费了一个队列中的元素后，会通知生产者当前队列可用
- 当往队列里插入一个元素时，如果队列不可用，那么阻塞生产者主要通过
  LockSupport.park（this）来实现。



### Atomic

- Atomic包里一共提供了13个类，属于4种类型的原子更
  新方式，分别是原子更新基本类型、原子更新数组、原子更新引用和原子更新属性（字段）。
  Atomic包里的类基本都是使用Unsafe实现的包装类。

#### 原子更新基本类型类

- AtomicBoolean：原子更新布尔类型。AtomicInteger：原子更新整型。AtomicLong：原子更新长整型。

- AtomicInteger的常用方法如下

  - int addAndGet（int delta）：以原子方式将输入的数值与实例中的值（AtomicInteger里的value）相加，并返回结果。

  - boolean compareAndSet（int expect，int update）：如果输入的数值等于预期值，则以原子方
    式将该值设置为输入的值。

  - int getAndIncrement()：以原子方式将当前值加1，注意，这里返回的是自增前的值。

    - ```java
      public final int getAndIncrement() {
          for (;;) {
              int current = get();
              int next = current + 1;
              if (compareAndSet(current, next))
              return current;
          }
      }
      ```

    - 源码中for循环体的第一步先取得AtomicInteger里存储的数值，第二步对AtomicInteger的当
      前数值进行加1操作，关键的第三步调用compareAndSet方法来进行原子更新操作，该方法先检
      查当前数值是否等于current，等于意味着AtomicInteger的值没有被其他线程修改过，则将
      AtomicInteger的当前数值更新成next的值，如果不等compareAndSet方法会返回false，程序会进
      入for循环重新进行compareAndSet操作。

  - void lazySet（int newValue）：最终会设置成newValue，使用lazySet设置值后，可能导致其他
    线程在之后的一小段时间内还是可以读到旧的值。关于该方法的更多信息可以参考并发编程
    网翻译的一篇文章《AtomicLong.lazySet是如何工作的？》，文章地址是“http://ifeve.com/how-
    does-atomiclong-lazyset-work/”。

  - int getAndSet（int newValue）：以原子方式设置为newValue的值，并返回旧值。

- Unsafe只提供了3种CAS方法：

  - compareAndSwapObject、compareAndSwapInt compareAndSwapLong
  - 再看AtomicBoolean源码，发现它是先把Boolean转换成整型，再使用compareAndSwapInt进行CAS，所以原子更新char、float和double变量也可以用类似的思路来实现

#### 原子更新数组

- AtomicIntegerArray：原子更新整型数组里的元素。
- AtomicLongArray：原子更新长整型数组里的元素。
- AtomicReferenceArray：原子更新引用类型数组里的元素。
- AtomicIntegerArray类主要是提供原子的方式更新数组里的整型，其常用方法如下。
  - int addAndGet（int i，int delta）：以原子方式将输入值与数组中索引i的元素相加。
  - boolean compareAndSet（int i，int expect，int update）：如果当前值等于预期值，则以原子
    方式将数组位置i的元素设置成update值。
- 数组value通过构造方法传递进去，然后AtomicIntegerArray会将当前数组
  复制一份，所以当AtomicIntegerArray对内部的数组元素进行修改时，不会影响传入的数组。

#### 原子更新引用类型

- 原子更新基本类型的AtomicInteger，只能更新一个变量，如果要原子更新多个变量，就需
  要使用这个原子更新引用类型提供的类。Atomic包提供了以下3个类。
  - AtomicReference：原子更新引用类型。
  - AtomicReferenceFieldUpdater：原子更新引用类型里的字段。
  - AtomicMarkableReference：原子更新带有标记位的引用类型。可以原子更新一个布尔类
    型的标记位和引用类型。构造方法是AtomicMarkableReference（V initialRef，boolean
    initialMark）。

#### 原子更新字段类

- AtomicIntegerFieldUpdater：原子更新整型的字段的更新器。
- AtomicLongFieldUpdater：原子更新长整型字段的更新器。
- AtomicStampedReference：原子更新带有版本号的引用类型。该类将整数值与引用关联起
  来，可用于原子的更新数据和数据的版本号，可以解决使用CAS进行原子更新时可能出现的
  ABA问题。
- 要想原子地更新字段类需要两步
  - 第一步，因为原子更新字段类都是抽象类，每次使用的时候必须使用静态方法newUpdater()创建一个更新器，并且需要设置想要更新的类和属性。
  - 第二步，更新类的字段（属性）必须使用public volatile修饰符

####  AtomicInteger的原理

（UnSafe类，底层是一句CPU指令，避免了并发问题） 



###  线程池

- 好处
  - 第一：降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。
  - 第二：提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。
  - 第三：提高线程的可管理性。线程是稀缺资源，如果无限制地创建，不仅会消耗系统资源，
    还会降低系统的稳定性，使用线程池可以进行统一分配、调优和监控

#### 线程池的实现原理

- ##### 线程池的处理流程

  - 1）线程池判断核心线程池里的线程是否都在执行任务。如果不是，则创建一个新的工作
    线程来执行任务。如果核心线程池里的线程都在执行任务，则进入下个流程。
  - 2）线程池判断工作队列是否已经满。如果工作队列没有满，则将新提交的任务存储在这
    个工作队列里。如果工作队列满了，则进入下个流程。
  - 3）线程池判断线程池的线程是否都处于工作状态。如果没有，则创建一个新的工作线程
    来执行任务。如果已经满了，则交给饱和策略来处理这个任务。
  - ![1626004827324](面经.assets/1626004827324.png)

- ##### ThreadPoolExecutor执行execute()方法

  - 如果当前运行的线程少于corePoolSize，则创建新线程来执行任务（注意，执行这一步骤
    需要获取全局锁）。
  - 如果运行的线程等于或多于corePoolSize，则将任务加入BlockingQueue。
  - 如果无法将任务加入BlockingQueue（队列已满），则创建新的线程来处理任务（注意，执
    行这一步骤需要获取全局锁）。
  - 如果创建新线程将使当前运行的线程超出maximumPoolSize，任务将被拒绝，并调用
    RejectedExecutionHandler.rejectedExecution()方法。
  - ThreadPoolExecutor采取上述步骤的总体设计思路，是为了在执行execute()方法时，尽可能
    地避免获取全局锁（那将会是一个严重的可伸缩瓶颈）。在ThreadPoolExecutor完成预热之后
    （当前运行的线程数大于等于corePoolSize），几乎所有的execute()方法调用都是执行步骤2，而
    步骤2不需要获取全局锁。
  - ![1626004885747](面经.assets/1626004885747.png)

##### 源码分析：

```java
/**
     * Executes the given task sometime in the future.  The task
     * may execute in a new thread or in an existing pooled thread.
     *
     * If the task cannot be submitted for execution, either because this
     * executor has been shutdown or because its capacity has been reached,
     * the task is handled by the current {@code RejectedExecutionHandler}.
     *
     * @param command the task to execute
     * @throws RejectedExecutionException at discretion of
     *         {@code RejectedExecutionHandler}, if the task
     *         cannot be accepted for execution
     * @throws NullPointerException if {@code command} is null
     */
    public void execute(Runnable command) {
        if (command == null)
            throw new NullPointerException();
        /*
         * Proceed in 3 steps:
         *
         * 1. If fewer than corePoolSize threads are running, try to
         * start a new thread with the given command as its first
         * task.  The call to addWorker atomically checks runState and
         * workerCount, and so prevents false alarms that would add
         * threads when it shouldn't, by returning false.
         *
         * 2. If a task can be successfully queued, then we still need
         * to double-check whether we should have added a thread
         * (because existing ones died since last checking) or that
         * the pool shut down since entry into this method. So we
         * recheck state and if necessary roll back the enqueuing if
         * stopped, or start a new thread if there are none.
         *
         * 3. If we cannot queue task, then we try to add a new
         * thread.  If it fails, we know we are shut down or saturated
         * and so reject the task.
         */
        int c = ctl.get();
        // 如果线程数小于基本线程数，则创建线程并执行当前任务
        if (workerCountOf(c) < corePoolSize) {
            if (addWorker(command, true))
                return;
            c = ctl.get();
        }
        // 如线程数大于等于基本线程数或线程创建失败，则将当前任务放到工作队列中
        if (isRunning(c) && workQueue.offer(command)) {
            int recheck = ctl.get();
            if (! isRunning(recheck) && remove(command))
                reject(command);
            // 如果线程池不处于运行中或任务无法放入队列，并且当前线程数量小于最大允许的线程数量，
            // 则创建一个线程执行任务。
            else if (workerCountOf(recheck) == 0)
                addWorker(null, false);
        }
        
        else if (!addWorker(command, false))
            reject(command);
    }
```

- 工作线程：线程池创建线程时，会将线程封装成工作线程Worker，Worker在执行完任务
  后，还会循环获取工作队列里的任务来执行。



#### 线程池的使用

##### 线程池的创建

- 我们可以通过ThreadPoolExecutor来创建一个线程池。
- 创建一个线程池时需要输入几个参数，如下。
  - 1）corePoolSize（线程池的基本大小）：当提交一个任务到线程池时，线程池会创建一个线
    程来执行任务，即使其他空闲的基本线程能够执行新任务也会创建线程，等到需要执行的任
    务数大于线程池基本大小时就不再创建。如果调用了线程池的prestartAllCoreThreads()方法，
    线程池会提前创建并启动所有基本线程。
  - 2）runnableTaskQueue（任务队列）：用于保存等待执行的任务的阻塞队列。可以选择以下几
    个阻塞队列。
    - ArrayBlockingQueue：是一个基于数组结构的有界阻塞队列，此队列按FIFO（先进先出）原
      则对元素进行排序。
    - LinkedBlockingQueue：一个基于链表结构的阻塞队列，此队列按FIFO排序元素，吞吐量通
      常要高于ArrayBlockingQueue。静态工厂方法Executors.newFixedThreadPool()使用了这个队列。
    - SynchronousQueue：一个不存储元素的阻塞队列。每个插入操作必须等到另一个线程调用
      移除操作，否则插入操作一直处于阻塞状态，吞吐量通常要高于Linked-BlockingQueue，静态工
      厂方法Executors.newCachedThreadPool使用了这个队列。
    - PriorityBlockingQueue：一个具有优先级的无限阻塞队列。
  - 3）maximumPoolSize（线程池最大数量）：线程池允许创建的最大线程数。如果队列满了，并
    且已创建的线程数小于最大线程数，则线程池会再创建新的线程执行任务。值得注意的是，如
    果使用了无界的任务队列这个参数就没什么效果。
  - 4）ThreadFactory：用于设置创建线程的工厂，可以通过线程工厂给每个创建出来的线程设
    置更有意义的名字。使用开源框架guava提供的ThreadFactoryBuilder可以快速给线程池里的线
    程设置有意义的名字，代码如下。
    new ThreadFactoryBuilder().setNameFormat("XX-task-%d").build();
  - 5）RejectedExecutionHandler（饱和策略）：当队列和线程池都满了，说明线程池处于饱和状
    态，那么必须采取一种策略处理提交的新任务。这个策略默认情况下是AbortPolicy，表示无法
    处理新任务时抛出异常。在JDK 1.5中Java线程池框架提供了以下4种策略。
    - AbortPolicy：直接抛出异常。
    - CallerRunsPolicy：只用调用者所在线程来运行任务。
    - DiscardOldestPolicy：丢弃队列里最近的一个任务，并执行当前任务。
    - DiscardPolicy：不处理，丢弃掉。
      当然，也可以根据应用场景需要来实现RejectedExecutionHandler接口自定义策略。如记录
      日志或持久化存储不能处理的任务。
    - keepAliveTime（线程活动保持时间）：线程池的工作线程空闲后，保持存活的时间。所以，
      如果任务很多，并且每个任务执行的时间比较短，可以调大时间，提高线程的利用率。
    - TimeUnit（线程活动保持时间的单位）：可选的单位有天（DAYS）、小时（HOURS）、分钟
      （MINUTES）、毫秒（MILLISECONDS）、微秒（MICROSECONDS，千分之一毫秒）和纳秒
      （NANOSECONDS，千分之一微秒）。

#### 向线程池提交任务

- 可以使用两个方法向线程池提交任务，分别为execute()和submit()方法。
- execute()方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功。
  execute()方法输入的任务是一个Runnable类的实例。
- submit()方法用于提交需要返回值的任务。线程池会返回一个future类型的对象，通过这个
  future对象可以判断任务是否执行成功，并且可以通过future的get()方法来获取返回值，get()方
  法会阻塞当前线程直到任务完成，而使用get（long timeout，TimeUnit unit）方法则会阻塞当前线
  程一段时间后立即返回，这时候有可能任务没有执行完。

#### 关闭线程池

- 可以通过调用线程池的shutdown或shutdownNow方法来关闭线程池
- 它们的原理是遍历线程池中的工作线程，然后逐个调用线程的interrupt方法来中断线程，所以无法响应中断的任务可能永远无法终止。但是它们存在一定的区别，
  - shutdownNow首先将线程池的状态设置成STOP，然后尝试停止所有的正在执行或暂停任务的线程，并返回等待执行任务的列表，
  - 而shutdown只是将线程池的状态设置成SHUTDOWN状态，然后中断所有没有正在执行任务的线程。
  - 只要调用了这两个关闭方法中的任意一个，isShutdown方法就会返回true。
  - 当所有的任务都已关闭后，才表示线程池关闭成功，这时调用isTerminaed方法会返回true。
  - 至于应该调用哪一种方法来关闭线程池，应该由提交到线程池的任务特性决定，通常调用shutdown方法来关闭线程池，如果任务不一定要执行完，则可以调用shutdownNow方法。

#### 合理地配置线程池

- ·任务的性质：CPU密集型任务、IO密集型任务和混合型任务。
  ·任务的优先级：高、中和低。
  ·任务的执行时间：长、中和短。
  ·任务的依赖性：是否依赖其他系统资源，如数据库连接

- 性质不同的任务可以用不同规模的线程池分开处理。
  - CPU密集型任务应配置尽可能小的线程，如配置N cpu +1个线程的线程池。
  - 由于IO密集型任务线程并不是一直在执行任务，则应配置尽可能多的线程，如2*N cpu 。混合型的任务
- 如果一直有优先级高的任务提交到队列里，那么优先级低的任务可能永远不能执行。
- 执行时间不同的任务可以交给不同规模的线程池来处理，或者可以使用优先级队列，让
  执行时间短的任务先执行。
- 建议使用有界队列。有界队列能增加系统的稳定性和预警能力，可以根据需要设大一点儿，比如几千。

#### 线程池的监控

- 如果在系统中大量使用线程池，则有必要对线程池进行监控，方便在出现问题时，可以根
  据线程池的使用状况快速定位问题。可以通过线程池提供的参数进行监控，在监控线程池的
  时候可以使用以下属性。
  - taskCount：线程池需要执行的任务数量。
  - completedTaskCount：线程池在运行过程中已完成的任务数量，小于或等于taskCount。
  - largestPoolSize：线程池里曾经创建过的最大线程数量。通过这个数据可以知道线程池是
    否曾经满过。如该数值等于线程池的最大大小，则表示线程池曾经满过。
  - getPoolSize：线程池的线程数量。如果线程池不销毁的话，线程池里的线程不会自动销
    毁，所以这个大小只增不减。
  - getActiveCount：获取活动的线程数。

#### 线程池的参数

### Executor

#### Executor框架的两级调度模型

- 在HotSpot VM的线程模型中，Java线程（java.lang.Thread）被一对一映射为本地操作系统线
  程。Java线程启动时会创建一个本地操作系统线程；当该Java线程终止时，这个操作系统线程
  也会被回收。操作系统会调度所有线程并将它们分配给可用的CPU。

- 在上层，Java多线程程序通常把应用分解为若干个任务，然后使用用户级的调度器（Executor框架）将这些任务映射为固定数量的线程；在底层，操作系统内核将这些线程映射到硬件处理器上
- 应用程序通过Executor框架控制上层的调度；而下层的调度由操作系统
  内核控制，下层的调度不受应用程序的控制。
- ![1626006968611](面经.assets/1626006968611.png)

#### Executor框架的结构

- Executor框架主要由3大部分组成如下。
  - 任务。包括被执行任务需要实现的接口：Runnable接口或Callable接口。
  - 任务的执行。包括任务执行机制的核心接口Executor，以及继承自Executor的ExecutorService接口。Executor框架有两个关键类实现了ExecutorService接口（ThreadPoolExecutor和ScheduledThreadPoolExecutor）。
  - 异步计算的结果。包括接口Future和实现Future接口的FutureTask类。

- Executor框架包含的主要的类与接口
  - Executor是一个接口，它是Executor框架的基础，它将任务的提交与任务的执行分离开来。
  - ThreadPoolExecutor是线程池的核心实现类，用来执行被提交的任务。
  - ScheduledThreadPoolExecutor是一个实现类，可以在给定的延迟后运行命令，或者定期执
    行命令。ScheduledThreadPoolExecutor比Timer更灵活，功能更强大。
  - Future接口和实现Future接口的FutureTask类，代表异步计算的结果。
  - Runnable接口和Callable接口的实现类，都可以被ThreadPoolExecutor或Scheduled-ThreadPoolExecutor执行。
  - ![1626007308478](面经.assets/1626007308478.png)

#### Executor框架的成员

- ThreadPoolExecutor、ScheduledThreadPoolExecutor、Future接口、Runnable接口、Callable接口和Executors。
- ThreadPoolExecutor
  - ThreadPoolExecutor通常使用工厂类Executors来创建。Executors可以创建3种类型的
    - FixedThreadPool
      - FixedThreadPool适用于为了满足资源管理的需求，而需要限制当前线程数量的应用场
        景，它适用于负载比较重的服务器。
    - SingleThreadExecutor
      - SingleThreadExecutor适用于需要保证顺序地执行各个任务；并且在任意时间点，不会有多
        个线程是活动的应用场景。
    - CachedThreadPool。
      - CachedThreadPool是大小无界的线程池，适用于执行很多的短期异步任务的小程序，或者
        是负载较轻的服务器。
- ScheduledThreadPoolExecutor
  - ScheduledThreadPoolExecutor通常使用工厂类Executors来创建
    - ScheduledThreadPoolExecutor。包含若干个线程的ScheduledThreadPoolExecutor。
      - ScheduledThreadPoolExecutor适用于需要多个后台线程执行周期任务，同时为了满足资源
        管理的需求而需要限制后台线程的数量的应用场景。
    - SingleThreadScheduledExecutor。只包含一个线程的ScheduledThreadPoolExecutor。
      - SingleThreadScheduledExecutor适用于需要单个后台线程执行周期任务，同时需要保证顺
        序地执行各个任务的应用场景。
- Future接口
  - Future接口和实现Future接口的FutureTask类用来表示异步计算的结果。当我们把Runnable
    接口或Callable接口的实现类提交（submit）给ThreadPoolExecutor或
    ScheduledThreadPoolExecutor时，ThreadPoolExecutor或ScheduledThreadPoolExecutor会向我们
    返回一个FutureTask对象
- Runnable接口和Callable接口
  - Runnable接口和Callable接口的实现类，都可以被ThreadPoolExecutor或Scheduled-ThreadPoolExecutor执行。它们之间的区别是Runnable不会返回结果，而Callable可以返回结果。
  - 除了可以自己创建实现Callable接口的对象外，还可以使用工厂类Executors来把一个Runnable包装成一个Callable。
  - 前面讲过，当我们把一个Callable对象（比如上面的Callable1或Callable2）提交给
    ThreadPoolExecutor或ScheduledThreadPoolExecutor执行时，submit（…）会向我们返回一个
    FutureTask对象。我们可以执行FutureTask.get()方法来等待任务执行完成。当任务成功完成后
    FutureTask.get()将返回该任务的结果

#### ThreadPoolExecutor详解

- Executor框架最核心的类是ThreadPoolExecutor，它是线程池的实现类，主要由下列4个组
  件构成。
  - ·corePool：核心线程池的大小。
  - ·maximumPool：最大线程池的大小。
  - ·BlockingQueue：用来暂时保存任务的工作队列。
  - ·RejectedExecutionHandler：当ThreadPoolExecutor已经关闭或ThreadPoolExecutor已经饱和
    时（达到了最大线程池大小且工作队列已满），execute()方法将要调用的Handler。
- ·通过Executor框架的工具类Executors，可以创建3种类型的ThreadPoolExecutor。
- ·FixedThreadPool。
  - FixedThreadPool被称为可重用固定线程数的线程池
  - FixedThreadPool的corePoolSize和maximumPoolSize都被设置为创建FixedThreadPool时指
    定的参数nThreads
  - 当线程池中的线程数大于corePoolSize时，keepAliveTime为多余的空闲线程等待新任务的
    最长时间，超过这个时间后多余的线程将被终止。这里把keepAliveTime设置为0L，意味着多余
    的空闲线程会被立即终止
  - FixedThreadPool的execute()方法的运行
    - 1）如果当前运行的线程数少于corePoolSize，则创建新线程来执行任务。
    - 2）在线程池完成预热之后（当前运行的线程数等于corePoolSize），将任务加入LinkedBlockingQueue。
    - 3）线程执行完1中的任务后，会在循环中反复从LinkedBlockingQueue获取任务来执行。
      FixedThreadPool使用无界队列LinkedBlockingQueue作为线程池的工作队列（队列的容量为
      Integer.MAX_VALUE）。使用无界队列作为工作队列会对线程池带来如下影响。
      - 1）当线程池中的线程数达到corePoolSize后，新任务将在无界队列中等待，因此线程池中
        的线程数不会超过corePoolSize。
      - 2）由于1，使用无界队列时maximumPoolSize将是一个无效参数。
      - 3）由于1和2，使用无界队列时keepAliveTime将是一个无效参数。
      - 4）由于使用无界队列，运行中的FixedThreadPool（未执行方法shutdown()或
        shutdownNow()）不会拒绝任务（不会调用RejectedExecutionHandler.rejectedExecution方法）。
  - ![1626011808205](面经.assets/1626011808205.png)
- ·SingleThreadExecutor。
  - SingleThreadExecutor是使用单个worker线程的Executor
  - ingleThreadExecutor的corePoolSize和maximumPoolSize被设置为1。
  - 其他参数与FixedThreadPool相同。SingleThreadExecutor使用无界队列LinkedBlockingQueue作为线程池的工作队列（队列的容量为Integer.MAX_VALUE）。SingleThreadExecutor使用无界队列作为工作队列对线程池带来的影响与FixedThreadPool相同，
  - SingleThreadExecutor的运行
    - 1）如果当前运行的线程数少于corePoolSize（即线程池中无运行的线程），则创建一个新线
      程来执行任务。
    - 2）在线程池完成预热之后（当前线程池中有一个运行的线程），将任务加入Linked-
      BlockingQueue。
    - 3）线程执行完1中的任务后，会在一个无限循环中反复从LinkedBlockingQueue获取任务来
      执行。
  - ![1626011792199](面经.assets/1626011792199.png)
- ·CachedThreadPool。
  - CachedThreadPool是一个会根据需要创建新线程的线程池
  - CachedThreadPool的corePoolSize被设置为0，即corePool为空；
  - maximumPoolSize被设置为Integer.MAX_VALUE，即maximumPool是无界的。
  - 这里把keepAliveTime设置为60L，意味着CachedThreadPool中的空闲线程等待新任务的最长时间为60秒，空闲线程超过60秒后将会被终止。
  - CachedThreadPool使用没有容量的SynchronousQueue作为线程池的工作队列，但
    CachedThreadPool的maximumPool是无界的。这意味着，如果主线程提交任务的速度高于
    maximumPool中线程处理任务的速度时，CachedThreadPool会不断创建新线程。极端情况下，
    CachedThreadPool会因为创建过多线程而耗尽CPU和内存资源。
  - CachedThreadPool的execute()
    - 1）首先执行SynchronousQueue.offer（Runnable task）。如果当前maximumPool中有空闲线程
      正在执行SynchronousQueue.poll（keepAliveTime，TimeUnit.NANOSECONDS），那么主线程执行offer操作与空闲线程执行的poll操作配对成功，主线程把任务交给空闲线程执行，execute()方
      法执行完成；否则执行下面的步骤2）。
    - 2）当初始maximumPool为空，或者maximumPool中当前没有空闲线程时，将没有线程执行
      SynchronousQueue.poll（keepAliveTime，TimeUnit.NANOSECONDS）。这种情况下，步骤1）将失败。此时CachedThreadPool会创建一个新线程执行任务，execute()方法执行完成。
    - 3）在步骤2）中新创建的线程将任务执行完后，会执行SynchronousQueue.poll（keepAliveTime，TimeUnit.NANOSECONDS）。这个poll操作会让空闲线程最多在SynchronousQueue中等待60秒钟。如果60秒钟内主线程提交了一个新任务（主线程执行步骤1）），那么这个空闲线程将执行主线程提交的新任务；否则，这个空闲线程将终止。由于空闲60秒的空闲线程会被终止，因此长时间保持空闲的CachedThreadPool不会使用任何资源。前面提到过，SynchronousQueue是一个没有容量的阻塞队列。每个插入操作必须等待另一个线程的对应移除操作，反之亦然。CachedThreadPool使用SynchronousQueue，把主线程提交的任务传递给空闲线程执行
  - ![1626011777468](面经.assets/1626011777468.png)



#### ScheduledThreadPoolExecutor详解

- ScheduledThreadPoolExecutor继承自ThreadPoolExecutor。它主要用来在给定的延迟之后运
  行任务，或者定期执行任务。ScheduledThreadPoolExecutor的功能与Timer类似，但
  ScheduledThreadPoolExecutor功能更强大、更灵活。Timer对应的是单个后台线程，而
  ScheduledThreadPoolExecutor可以在构造函数中指定多个对应的后台线程数。

##### ScheduledThreadPoolExecutor的运行机制

- DelayQueue是一个无界队列，所以ThreadPoolExecutor的maximumPoolSize在Scheduled-
  ThreadPoolExecutor中没有什么意义

- ScheduledThreadPoolExecutor的执行主要分为两大部分。
  - 1）当调用ScheduledThreadPoolExecutor的scheduleAtFixedRate()方法或者scheduleWith-
    FixedDelay()方法时，会向ScheduledThreadPoolExecutor的DelayQueue添加一个实现了
    RunnableScheduledFutur接口的ScheduledFutureTask。
  - 2）线程池中的线程从DelayQueue中获取ScheduledFutureTask，然后执行任务。

- ScheduledThreadPoolExecutor为了实现周期性的执行任务，对ThreadPoolExecutor做了如下的修改。
  - 使用DelayQueue作为任务队列。
  - 获取任务的方式不同（后文会说明）。
  - 执行周期任务后，增加了额外的处理

- ![1626012487464](面经.assets/1626012487464.png)

##### ScheduledThreadPoolExecutor的实现

- 前面我们提到过，ScheduledThreadPoolExecutor会把待调度的任务（ScheduledFutureTask）
  放到一个DelayQueue中。
- ScheduledFutureTask主要包含3个成员变量，如下。
  - long型成员变量time，表示这个任务将要被执行的具体时间。
  - long型成员变量sequenceNumber，表示这个任务被添加到ScheduledThreadPoolExecutor中的序号。
  - long型成员变量period，表示任务执行的间隔周期。
- DelayQueue封装了一个PriorityQueue，这个PriorityQueue会对队列中的ScheduledFutureTask进行排序。排序时，time小的排在前面（时间早的任务将被先执行）。如果两个ScheduledFutureTask的time相同，就比较sequenceNumber，sequenceNumber小的排在前面（也就是说，如果两个任务的执行时间相同，那么先提交的任务将被先执行）。
- ScheduledThreadPoolExecutor的任务执行步骤
  - 1）线程1从DelayQueue中获取已到期的ScheduledFutureTask（DelayQueue.take()）。到期任务
    是指ScheduledFutureTask的time大于等于当前时间。
    - 1）获取Lock。
    - 2）获取周期任务。
      - 如果PriorityQueue为空，当前线程到Condition中等待；否则执行下面的2.2。
      - 如果PriorityQueue的头元素的time时间比当前时间大，到Condition中等待到time时间；否
        则执行下面的2.3。
      - 获取PriorityQueue的头元素（2.3.1）；如果PriorityQueue不为空，则唤醒在Condition中等待
        的所有线程（2.3.2）。
    - 3）释放Lock。
      ScheduledThreadPoolExecutor在一个循环中执行步骤2，直到线程从PriorityQueue获取到一
      个元素之后（执行2.3.1之后），才会退出无限循环（结束步骤2）。
  - 2）线程1执行这个ScheduledFutureTask。
  - 3）线程1修改ScheduledFutureTask的time变量为下次将要被执行的时间。
  - 4）线程1把这个修改time之后的ScheduledFutureTask放回DelayQueue中（DelayQueue.add()）
    - 1）获取Lock。
    - 2）添加任务。
      - 向PriorityQueue添加任务。
      - 如果在上面2.1中添加的任务是PriorityQueue的头元素，唤醒在Condition中等待的所有线程。
    - 3）释放Lock。
  - ![1626013161298](面经.assets/1626013161298.png)



#### FutureTask详解

##### FutureTask简介

- Future接口和实现Future接口的FutureTask类，代表异步计算的结果

- FutureTask除了实现Future接口外，还实现了Runnable接口。因此，FutureTask可以交给
  Executor执行，也可以由调用线程直接执行（FutureTask.run()）。根据FutureTask.run()方法被执行
  的时机，FutureTask可以处于下面3种状态。

  - 1）未启动。FutureTask.run()方法还没有被执行之前，FutureTask处于未启动状态。当创建一
    个FutureTask，且没有执行FutureTask.run()方法之前，这个FutureTask处于未启动状态。
  - 2）已启动。FutureTask.run()方法被执行的过程中，FutureTask处于已启动状态。
  - 3）已完成。FutureTask.run()方法执行完后正常结束，或被取消（FutureTask.cancel（…）），或
    执行FutureTask.run()方法时抛出异常而异常结束，FutureTask处于已完成状态。

- 当FutureTask处于未启动或已启动状态时，执行FutureTask.get()方法将导致调用线程阻塞；

- 当FutureTask处于已完成状态时，执行FutureTask.get()方法将导致调用线程立即返回结果或抛出异常。

- 当FutureTask处于未启动状态时，执行FutureTask.cancel()方法将导致此任务永远不会被执行；

- 当FutureTask处于已启动状态时，执行FutureTask.cancel（true）方法将以中断执行此任务线程的方式来试图停止任务；

- 当FutureTask处于已启动状态时，执行FutureTask.cancel（false）方法将不会对正在执行此任务的线程产生影响（让正在执行的任务运行完成）；

- 当FutureTask处于已完成状态时，执行FutureTask.cancel（…）方法将返回false。

- 

  

  - ![1626013536439](面经.assets/1626013536439.png)



![1626013713432](面经.assets/1626013713432.png)



![1626013725932](面经.assets/1626013725932.png)



##### FutureTask的使用

- 可以把FutureTask交给Executor执行；也可以通过ExecutorService.submit（…）方法返回一个
  FutureTask，然后执行FutureTask.get()方法或FutureTask.cancel（…）方法。除此以外，还可以单独
  使用FutureTask。
- 当一个线程需要等待另一个线程把某个任务执行完后它才能继续执行，此时可以使用
  FutureTask。假设有多个线程执行若干任务，每个任务最多只能被执行一次。当多个线程试图
  同时执行同一个任务时，只允许一个线程执行任务，其他线程需要等待这个任务执行完后才
  能继续执行
- 当两个线程试图同时执行同一个任务时，如果Thread 1执行1.3后Thread 2执行2.1，那么接
  下来Thread 2将在2.2等待，直到Thread 1执行完1.4后Thread 2才能从2.2（FutureTask.get()）返回。

![1626013925031](面经.assets/1626013925031.png)



##### FutureTask的实现

- FutureTask的实现基于AbstractQueuedSynchronizer（以下简称为AQS）。java.util.concurrent中
  的很多可阻塞类（比如ReentrantLock）都是基于AQS来实现的。AQS是一个同步框架，它提供通
  用机制来原子性管理同步状态、阻塞和唤醒线程，以及维护被阻塞线程的队列。JDK 6中AQS
  被广泛使用，基于AQS实现的同步器包括：ReentrantLock、Semaphore、ReentrantReadWriteLock、
  CountDownLatch和FutureTask。

- 每一个基于AQS实现的同步器都会包含两种类型的操作，如下。
  - 至少一个acquire操作。这个操作阻塞调用线程，除非/直到AQS的状态允许这个线程继续
    执行。FutureTask的acquire操作为get()/get（long timeout，TimeUnit unit）方法调用。
  - 至少一个release操作。这个操作改变AQS的状态，改变后的状态可允许一个或多个阻塞
    线程被解除阻塞。FutureTask的release操作包括run()方法和cancel（…）方法。
  - 基于“复合优先于继承”的原则，FutureTask声明了一个内部私有的继承于AQS的子类Sync，对FutureTask所有公有方法的调用都会委托给这个内部子类。
  - AQS被作为“模板方法模式”的基础类提供给FutureTask的内部子类Sync，这个内部子类只
    需要实现状态检查和状态更新的方法即可，这些方法将控制FutureTask的获取和释放操作。具
    体来说，Sync实现了AQS的tryAcquireShared（int）方法和tryReleaseShared（int）方法，Sync通过这
    两个方法来检查和更新同步状态。

![1626014176371](面经.assets/1626014176371.png)



- 如图所示，Sync是FutureTask的内部私有类，它继承自AQS。创建FutureTask时会创建内部
  私有的成员对象Sync，FutureTask所有的的公有方法都直接委托给了内部私有的Sync。
- FutureTask.get()方法会调用AQS.acquireSharedInterruptibly（int arg）方法，
  - 1）调用AQS.acquireSharedInterruptibly（int arg）方法，这个方法首先会回调在子类Sync中实
    现的tryAcquireShared()方法来判断acquire操作是否可以成功。acquire操作可以成功的条件为：
    state为执行完成状态RAN或已取消状态CANCELLED，且runner不为null。
  - 2）如果成功则get()方法立即返回。如果失败则到线程等待队列中去等待其他线程执行release操作。
  - 3）当其他线程执行release操作（比如FutureTask.run()或FutureTask.cancel（…））唤醒当前线
    程后，当前线程再次执行tryAcquireShared()将返回正值1，当前线程将离开线程等待队列并唤
    醒它的后继线程（这里会产生级联唤醒的效果，后面会介绍）。
  - 4）最后返回计算的结果或抛出异常。

- FutureTask.run()的执行过程如下。
  - 1）执行在构造函数中指定的任务（Callable.call()）。
  - 2）以原子方式来更新同步状态（调用AQS.compareAndSetState（int expect，int update），设置
    state为执行完成状态RAN）。如果这个原子操作成功，就设置代表计算结果的变量result的值为
    Callable.call()的返回值，然后调用AQS.releaseShared（int arg）。
  - 3）AQS.releaseShared（int arg）首先会回调在子类Sync中实现的tryReleaseShared（arg）来执
    行release操作（设置运行任务的线程runner为null，然会返回true）；AQS.releaseShared（int arg），
    然后唤醒线程等待队列中的第一个线程。
  - 4）调用FutureTask.done()。
    当执行FutureTask.get()方法时，如果FutureTask不是处于执行完成状态RAN或已取消状态
    CANCELLED，当前执行线程将到AQS的线程等待队列中等待（见下图的线程A、B、C和D）。当
    某个线程执行FutureTask.run()方法或FutureTask.cancel（...）方法时，会唤醒线程等待队列的第一
    个线程（见图10-16所示的线程E唤醒线程A）。

![1626014474497](面经.assets/1626014474497.png)

- 假设开始时FutureTask处于未启动状态或已启动状态，等待队列中已经有3个线程（A、B和
  C）在等待。此时，线程D执行get()方法将导致线程D也到等待队列中去等待。
- 当线程E执行run()方法时，会唤醒队列中的第一个线程A。线程A被唤醒后，首先把自己从
  队列中删除，然后唤醒它的后继线程B，最后线程A从get()方法返回。线程B、C和D重复A线程
  的处理流程。最终，在队列中等待的所有线程都被级联唤醒并从get()方法返回。



















### java多线程的实现方式 

###  x

###  Collection 

### 实现生产者和消费者，一个长度100的buffer，10个生产者线程，10个消费者线程 

###  Java里的锁，有哪几种（synchronized和Reentrantlock） 

###  ReentrantLock有哪些特性（可重入，公平锁），可重入是如何实现的（有一个引用数，非可重入只有01值） 

###  当某个线程获取ReentrantLock失败时，是否会从内核态切换回用户态？ReentrantLock如何存储阻塞的线程的？（AQS，不断轮询前一个结点是否状态发生了变化）所以什么是自旋锁？ 



###  JVM，说一下最熟悉的GC（我说了CMS，讲了并行回收，浮动垃圾，最短STW等等），然后追问我CMS的整个回收流程，标记，清理等等，年轻代怎么回收等等 







## JVM

JVM字节码文件对象的结构（对象头有啥，对象体有啥...）

 JVM运行期内存空间，每块的作用 

 对象一定在堆中吗？（方法区里有静态变量和常量） 

 对象一定在堆和方法区中吗？（。。。） 

 字符串常量池（在堆中） 

 面试官讲：其实虚拟机栈里也是可以存放对象的balabala... 

 虚拟机的类加载机制，具体步骤及对应完成的事情 

 JVM的双亲委派模型

###  jvm内存结构 

- JVM字节码文件对象的结构

```
 对象头主要包含两部分数据：MarkWord、类型指针。MarkWord 用于存储哈希码（HashCode）、GC分代年龄、锁状态标志位、线程持有的锁、偏向线程ID等信息。类型指针即对象指向他的类元数据指针，如果对象是一个 Java 数组，会有一块用于记录数组长度的数据， 

 实例数据存储代码中所定义的各种类型的字段信息。 

 对齐填充起占位作用。HotSpot 虚拟机要求对象的起始地址必须是8的整数倍，因此需要对齐填充。

```

- JVM运行期内存空间，每块的作用

```
线程私有的运行时数据区: 程序计数器、Java 虚拟机栈、本地方法栈。

线程共享的运行时数据区:Java 堆、方法区。
```



- 对象一定在堆中吗

```
类(静态)变量也存储在方法区中。
```



- 字符串常量池

```
 运行时常量池存放常量池表，用于存放编译器生成的各种字面量与符号引用。一般除了保存 Class 文件中描述的符号引用外，还会把符号引用翻译的直接引用也存储在运行时常量池。除此之外，也会存放字符串基本类型。 

 JDK8之前，放在方法区，大小受限于方法区。JDK8将运行时常量池存放堆中

```



- 虚拟机的类加载机制

```
加载：

通过全类名获取类的二进制字节流. 将类的静态存储结构转化为方法区的运行时数据结构。在内存中生成类的Class对象，作为方法区数据的入口。验证：对文件格式，元数据，字节码，符号引用等验证正确性。

准备：在方法区内为类变量分配内存并设置为0值。

解析：将符号引用转化为直接引用。

初始化：执行类构造器clinit方法，真正初始化
```



- JVM的双亲委派模型

```
一个类加载器收到类加载请求之后，首先判断当前类是否被加载过。已经被加载的类会直接返回，如果没有被加载，首先将类加载请求转发给父类加载器，一直转发到启动类加载器，只有当父类加载器无法完成时才尝试自己加载。

加载类顺序：BootstrapClassLoader->ExtensionClassLoader->AppClassLoader->CustomClassLoader 检查类是否加载顺序：CustomClassLoader->AppClassLoader->ExtensionClassLoader->BootstrapClassLoader
```



- JVM垃圾收集每种算法

```


标记清除算法：先标记需清除的对象，之后统一回收。这种方法效率不高，会产生大量不连续的碎片。

标记整理算法：先标记存活对象，然后让所有存活对象向一端移动，之后清理端边界以外的内存

标记复制算法：将可用内存按容量划分为大小相等的两块，每次只使用其中一块。当使用的这块空间用完了，就将存活对象复制到另一块，再把已使用过的内存空间一次清理掉。
```



- 调用system.gc()一定会发生垃圾收集吗？为什么？

```
调用System.gc()的时候，其实并不会马上进行垃圾回收,只会把这次gc请求记录下来。需配合System.runFinalization()才会进行真正回收
```



## 源码

1. ArrayList和Linkedlist底层
2. ArrayList扩容机制

### 泛型

https://cloud.tencent.com/developer/article/1033693

### 注解

https://www.cnblogs.com/acm-bingzi/p/javaAnnotation.html

###  java hashmap[源码]

### java 常用集合和部分[源码]

### 抽象类与接口区别

### 深拷贝与浅拷贝

**浅拷贝**: 拷贝对象和原始对象的引用类型引用同一个对象。

**深拷贝**: 拷贝对象和原始对象的引用类型引用不同对象。

### Java有无符号类型吗？网络上传来一个64位可以用long接吗（可以但会溢出）

### Object里都有什么方法，equals，clone，wait，notify、hashcode、toString，讲下他们的作用

### Java有内存泄露吗

### 强弱引用是什么，区别在哪，使用场景

- ArrayList和Linkedlist底层

```


ArrayList 使用数组实现，是容量可变的非线程安全列表，随机访问快，集合扩容时会创建更大的数组，把原有数组复制到新数组。

LinkedList 本质是双向链表，与 ArrayList 相比插入和删除速度更快，但随机访问元素很慢。
```



- 红黑树

```


红黑树本身是有2-3树发展而来，红黑树是保持黑平衡的二叉树，其查找会比AVL树慢一点，添加和删除元素会比AVL树快一点。增删改查统计性能上讲，红黑树更优。红黑树主要特征是在每个节点上增加一个属性表示节点颜色，可以红色或黑色。红黑树和 AVL 树类似，都是在进行插入和删除时通过旋转保持自身平衡，从而获得较高的查找性能。红黑树保证从根节点到叶尾的最长路径不超过最短路径的 2 倍，所以最差时间复杂度是 O(logn)。红黑树通过重新着色和左右旋转，更加高效地完成了插入和删除之后的自平衡调整。
```



## 运行时数据区





## GC

###  概述

- 垃圾收集需要完成的事情
  - 哪些内存需要回收？
  - 什么时候回收？
  - 如何回收？
- 为什么我们还要去了解垃圾收集和内存分配？
  - 当需要排查各种内存溢出、内存泄漏问题时，当垃圾收集成为系统达到更高并发量的瓶颈时，我们就必须对这些“自动化”的技术实施必要的监控和调节
- 程序计数器、虚拟机栈、本地方法栈
  - 这几个区域的内存分配和回收都具备确定性，在这几个区域内就不需要过多考虑如何回收的问题，当方法结束或者线程结束时，内存自然就跟随着回收了
- 堆和方法区
  - 这两个区域则有着很显著的不确定性：一个接口的多个实现类需要的内存可能会不一样，一个方法所执行的不同条件分支所需要的内存也可能不一样，只有处于运行期间，我们才能知道程序究竟会创建哪些对象，创建多少个对象，这部分内存的分配和回收是动态的。垃圾收集器所关注的正是这部分内存该如何管理

### 如何判断对象已不在被引用?

- **引用计数算法**
  - 在对象中添加一个引用计数器，每当有一个地方引用它时，计数器值就加一；当引用失效时，计数器值就减一；任何时刻计数器为零的对象就是不可能再被使用的
  - 但是，在Java领域，至少主流的Java虚拟机里面都没有选用引用计数算法来管理内存，主要原因是，这个看似简单的算法有很多例外情况要考虑，必须要配合大量额外处理才能保证正确地工作，譬如单纯的引用计数就**很难解决对象之间相互循环引用**的问题。
- **可达性分析算法**
  - 通过一系列称为“**GC Roots**”的根对象作为起始节点集，从这些节点开始，根据引用关系向下搜索，搜索过
    程所走过的路径称为“**引用链**”（Reference Chain），如果某个对象到GC Roots间**没有任何引用链相连**，
    或者用图论的话来说就是从GC Roots到这个对象**不可达**时，则证明此对象是不可能再被使用的。
  - <img src="面经.assets/1625664166364.png" alt="1625664166364" style="zoom:50%;" />
  - 固定可作为**GC Roots**的对象
    - 在**虚拟机栈**（栈帧中的本地变量表）中引用的对象，譬如各个线程被调用的方法堆栈中使用到的
      **参数、局部变量、临时变量**等。
    - 在**方法区中类静态属性**引用的对象，譬如Java类的引用类型静态变量。
    - 在**方法区中常量**引用的对象，譬如字符串常量池（String Table）里的引用。
    - 在**本地方法栈中JNI**（即通常所说的Native方法）引用的对象。
    - **Java虚拟机内部的引**用，如**基本数据类型对应的Class对象**，一些**常驻的异常对象**（比如
      NullPointExcepiton、OutOfMemoryError）等，还有系统类加载器。
    - 所有被**同步锁（synchronized关键字）持有的对象**。
    - 反映**Java虚拟机内部情况的JMXBea**n、JVMTI中注册的回调、本地代码缓存等。
    - 除了这些固定的GC Roots集合以外，根据用户所选用的垃圾收集器以及当前回收的内存区域不
      同，还可以有其他对象“临时性”地加入
      - 某个区域里的对象完全有可能被位于堆中其他区域的对象所引用，这时候就需要将这些关联区域的对象也一并加入GC Roots集合中
      - 分代收集和局部回收, 只收集新生代的垃圾, 但是这些内存可能被老年代中的某些对象引用, 所以需要将这些其他关联区域的对象一起加入GC Root
- **引用**
  - (4种引用强度依次逐渐减弱)
  - 强引用（Strongly Re-ference）
    - 类似“Object obj=new Object()”这种引用关系。无论任何情况下，只要强引用关系还存在，垃圾收集器就永远不会回收掉被引用的对象。
  - 软引用（Soft Reference)
    - 一些还有用，但非必须的对象。只被软引用关联着的对象，在系统将要发生内存溢出异常前，会把这些对象列进回收范围之中进行第二次回收，如果这次回收还没有足够的内存，才会抛出内存溢出异常
  - 弱引用（Weak Reference）
    - 那些非必须对象，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾收集发生为止。当垃圾收集器开始工作，无论当前内存是否足够，都会回收掉只被弱引用关联的对象
  - 虚引用（Phantom Reference）
    - 最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例
- **finalize()**
  -  Java 技术允许使用 finalize() 方法在垃圾收集器将对象从内存中清除出去之前做必要的清理工作。这个方法是由垃圾收集器在确定这个对象没有被引用时对这个对象调用的。它是在 Object 类中定义的，因此所有的类都继承了它。子类覆盖 finalize() 方法以整理系统资源或者执行其他清理工作。finalize() 方法是在垃圾收集器删除对象之前对这个对象调用的。  
  - 任何一个对象的finalize()方法都只会被系统自动调用一次，如果对象面临下一次回收，它的finalize()方法不会被再次执行
  - 可达性分析算法中判定为不可达的对象，也不是“非死不可”的，这时候它们暂时还处于“缓刑”阶段，要真正宣告一个对象死亡，至少要经历两次标记过程：如果对象在进行可达性分析后发现没有与GC Roots相连接的引用链，那它将会被第一次标记，随后进行一次筛选，筛选的条件是此对象是否有必要执行finalize()方法。假如对象没有覆盖finalize()方法，或者finalize()方法已经被虚拟机调用过，那么虚拟机将这两种情况都视为“没有必要执行”
  - 如果这个对象被判定为确有必要执行finalize()方法，那么该对象将会被放置在一个名为F-Queue的
    队列之中，并在稍后由一条由虚拟机自动建立的、低调度优先级的Finalizer线程去执行它们的finalize()
    方法
  - finalize()方法是对象逃脱死亡命运的最后一次机会，稍后收集器将对F-Queue中的对象进行第二次小规模的标记，如果对象要在finalize()中成功拯救自己——只要重新与引用链上的任何一个对象建立关联即可，譬如把自己（this关键字）赋值给某个类变量或者对象的成员变量，那在第二次标记时它将被移出“即将回收”的集合



- 回收方法区
  - 主要回收两部分内容：废弃的常量和不再使用的类型
  - 废弃的常量
    - 与回收Java堆中的对象非常类似。举个常量池中字面量回收的例子，假如一个字符串“java”曾经进入常量池中，但是当前系统已经没有任何字符串对象引用常量池中的“java”常量，且虚拟机中也没有其他地方引用这个字面量
  - 不再使用的类型 (满足3个条件)
    - 该类所有的实例都已经被回收，也就是Java堆中不存在该类及其任何派生子类的实例。
    - 加载该类的类加载器已经被回收，
    - 该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方
      法。



### 垃圾收集算法

- 分代收集理论
  - 弱分代假说（Weak Generational Hypothesis）：绝大多数对象都是朝生夕灭的。
  - 强分代假说（Strong Generational Hypothesis) ：熬过越多次垃圾收集过程的对象就越难以消亡。
    - 收集器应该将Java堆划分出不同的区域，然后将回收对象依据其年龄（年龄即对象熬过垃圾收集过程的次数）分配到不同的区域之中存储
    - Java堆划分出不同的区域之后，垃圾收集器才可以每次只回收其中某一个或者某些部分的区域
      ——因而才有了“Minor GC”“Major GC”“Full GC”这样的回收类型的划分；也才能够针对不同的区域安
      排与里面存储对象存亡特征相匹配的垃圾收集算法——因而发展出了“标记-复制算法”“标记-清除算
      法”“标记-整理算法”等针对性的垃圾收集算法
    - 设计者一般至少会把Java堆划分为
      - 新生代（Young Generation）
        - 在新生代中，每次垃圾收集时都发现有大批对象死去
      - 老年代（Old Generation）
        - 而每次回收后存活的少量对象，将会逐步晋升到老年代中存放
  - 跨代引用假说（Intergenerational Reference Hypothesis）跨代引用相对于同代引用来说仅占极少数。
    - 存在互相引用关系的两个对象，是应该倾向于同时生存或者同时消亡的
    - 新生代上建立一个全局的数据结构（该结构被称为“记忆集”，Remembered Set），这个结构把老年代划分成若干小块，标识出老年代的哪一块内存会存在跨代引用。此后当发生Minor GC时，只有包含了跨代引用的小块内存里的对象才会被加入到GC Roots进行扫描
  - 部分收集（Partial GC）：指目标不是完整收集整个Java堆的垃圾收集，其中又分为：
  - 新生代收集（Minor GC/Young GC）：指目标只是新生代的垃圾收集。
  - 老年代收集（Major GC/Old GC）：指目标只是老年代的垃圾收集。目前只有CMS收集器会有单
    独收集老年代的行为。另外请注意“Major GC”这个说法现在有点混淆，在不同资料上常有不同所指，
    读者需按上下文区分到底是指老年代的收集还是整堆收集。
  - 混合收集（Mixed GC）：指目标是收集整个新生代以及部分老年代的垃圾收集。目前只有G1收
    集器会有这种行为。
  - 整堆收集（Full GC）：收集整个Java堆和方法区的垃圾收集。

- 标记-清除算法
  - 算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后，统一回收掉所有被标记的对象，也可以反过来，标记存活的对象，统一回收所有未被标记的对象。标记过程就是对象是否属于垃圾的判定过程
  - 缺点
    - 执行效率不稳定
      - 如果Java堆中包含大量对象，而且其中大部分是需要被回收的，这时必须进行大量标记和清除的动作，导致标记和清除两个过程的执行效率都随对象数量增长而降低
    - 内存空间的碎片化问题，
      - 标记、清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致当以后在程序运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作
- 标记-复制算法
  - 为了解决标记-清除算法面对大量可回收对象时执行效率低的问题提出了一种称为“半区复制”（Semispace Copying）的垃圾收集算法，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。如果内存中多数对象都是存活的，这种算法将会产生大量的内存间复制的开销，但对于多数对象都是可回收的情况，算法需要复制的就是占少数的存活对象，而且每次都是针对整个半区进行内存回收，分配内存时也就不用考虑有空间碎片的复杂情况，只要移动堆顶指针，按顺序分配即可
  - 缺点
    - 复制回收算法的代价是将可用内存缩小为了原来的一半，空间浪费
  - Appel式回收
    - HotSpot虚拟机的Serial、ParNew等新生代收集器均采用了这种策略来设计新生代的内存布局
    - 把新生代分为一块较大的Eden空间和两块较小的Survivor空间，每次分配内存只使用Eden和其中一块Survivor。发生垃圾搜集时，将Eden和Survivor中仍然存活的对象一次性复制到另外一块Survivor空间上，然后直接清理掉Eden和已用过的那块Survivor空间
    - HotSpot虚拟机默认Eden和Survivor的大小比例是8∶1，也即每次新生代中可用内存空间为整个新
      生代容量的90%（Eden的80%加上一个Survivor的10%），只有一个Survivor空间，即10%的新生代是会被“浪费”的
  - 分配担保
    - Appel式回收还有一个充当罕见情况的“逃生门”的安全设计，当Survivor空间不足以容纳一次Minor GC之后存活的对象时，就需要依赖其他内存区域（实际上大多就是老年代）进行分配担保（Handle Promotion）
    - 如果另外一块Survivor空间没有足够空间存放上一次新生代收集下来的存活对象，这些对象便将通过分配担保直接进入老年代
- 标记-整理算法
  - 标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向内存空间一端移动，然后直接清理掉边界以外的内存，
  - 标记-复制算法在对象存活率较高时就要进行较多的复制操作，效率将会降低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法
  - 标记-清除算法与标记-整理算法的本质差异在于前者是一种非移动式的回收算法，而后者是移动式的。是否移动回收后的存活对象是一项优缺点并存的风险决策
  - Stop the World
    - 如果移动存活对象，尤其是在老年代这种每次回收都有大量对象存活区域，移动存活对象并更新所有引用这些对象的地方将会是一种极为负重的操作，而且这种对象移动操作必须全程暂停用户应用程序才能进行

### HotSpot算法细节的实现

- 根结点枚举
  - 固定可作为GC Roots的节点主要在全局性的引用（例如常量或类静态属性）与执行上下文（例如栈帧中的本地变量表）中，尽管目标明确，但查找过程要做到高效并非一件容易的事情
  - 所有收集器在根节点枚举这一步骤时都是必须暂停用户线程的，因此毫无疑问根节点枚举与之前提及的整理内存碎片一样会面临相似的“Stop The World”的困扰
  - 在HotSpot的解决方案里，是使用一组称为OopMap的数据结构来达到这个目的。一旦类加载动作完成的时候，HotSpot就会把对象内什么偏移量上是什么类型的数据计算出来，在即时编译过程中，也会在特定的位置记录下栈里和寄存器里哪些位置是引用。这样收集器在扫描时就可以直接得知这些信息了，并不需要真正一个不漏地从方法区等GC Roots开始查找
- 安全点
  - HotSpot也的确没有为每条指令都生成OopMap，前面已经提到，只是在“特定的位置”记录了这些信息，这些位置被称为安全点（Safepoint）
  - 有了安全点的设定，也就决定了用户程序执行时并非在代码指令流的任意位置都能够停顿下来开始垃圾收集，而是强制要求必须执行到达安全点后才能够暂停。因此，安全点的选定既不能太少以至于让收集器等待时间过长，也不能太过频繁以至于过分增大运行时的内存负荷
  - 安全点位置的选取基本上是以“是否具有让程序长时间执行的特征”为标准进行选定的
    - 因为每条指令执行的时间都非常短暂，程序不太可能因为指令流长度太长这样的原因而
      长时间执行，
    - “长时间执行”的最明显特征就是指令序列的复用，例如方法调用、循环跳转、异常跳转
      等都属于指令序列复用，所以只有具有这些功能的指令才会产生安全点
  - 如何在垃圾收集发生时让所有线程（这里其实不包括执行JNI调用的线程）都跑到最近的安全点，然后停顿下来?
    - 抢先式中断: 不需要线程的执行代码主动去配合，在垃圾收集发生时，系统首先把所有用户线程全部中断，如果发现有用户线程中断的地方不在安全点上，就恢复这条线程执行，让它一会再重新中断，直到跑到安全点上。现在几乎没有虚拟机实现采用抢先式中断来暂停线程响应GC事件。
    - 主动式中断: 当垃圾收集需要中断线程的时候，不直接对线程操作，仅仅简单地设置一个标志位，各个线程执行过程时会不停地主动去轮询这个标志，一旦发现中断标志为真时就自己在最近的安全点上主动中断挂起。轮询标志的地方和安全点是重合的，另外还要加上所有创建对象和其他需要在Java堆上分配内存的地方，这是为了检查是否即将要发生垃圾收集，避免没有足够内存分配新对象。
    - 轮询操作在代码中会频繁出现，这要求它必须足够高效。HotSpot使用内存保护陷阱的方式，把轮询操作精简至只有一条汇编指令的程度
- 安全区域
  - 安全点机制保证了程序执行时，在不太长的时间内就会遇到可进入垃圾收集过程的安全点。但是，程序“不执行”的时候呢？所谓的程序不执行就是没有分配处理器时间，典型的场景便是用户线程处于Sleep状态或者Blocked状态，这时候线程无法响应虚拟机的中断请求，不能再走到安全的地方去中断挂起自己，虚拟机也显然不可能持续等待线程重新被激活分配处理器时间。对于这种情况，就必须引入安全区域（Safe Region）来解决。
  - 安全区域是指能够确保在某一段代码片段之中，引用关系不会发生变化，因此，在这个区域中任意地方开始垃圾收集都是安全的。我们也可以把安全区域看作被扩展拉伸了的安全点
  - 当用户线程执行到安全区域里面的代码时，首先会标识自己已经进入了安全区域，那样当这段时
    间里虚拟机要发起垃圾收集时就不必去管这些已声明自己在安全区域内的线程了。
  - 当线程要离开安全区域时，它要检查虚拟机是否已经完成了根节点枚举（或者垃圾收集过程中其他需要暂停用户线程的阶段），如果完成了，那线程就当作没事发生过，继续执行；否则它就必须一直等待，直到收到可以离开安全区域的信号为止。
- 记忆集
  - 记忆集是一种用于记录从非收集区域指向收集区域的指针集合的抽象数据结构。如果我们不考虑效率和成本的话，最简单的实现可以用非收集区域中所有含跨代引用的对象数组来实现这个数据结构，
    - 空间占用还是维护成本都相当高昂
  - 收集器只需要通过记忆集判断出某一块非收集区域是否存在有指向了收集区域的指针就可以了，并不需要了解这些跨代指针的全部细节,那设计者在实现记忆集的时候，便可以选择更为粗犷的记录粒度来节省记忆集的存储和维护成本，
    - 字长精度：每个记录精确到一个机器字长（就是处理器的寻址位数，如常见的32位或64位，这个
      精度决定了机器访问物理内存地址的指针长度），该字包含跨代指针。
    - 对象精度：每个记录精确到一个对象，该对象里有字段含有跨代指针。
    - 卡精度：每个记录精确到一块内存区域，该区域内有对象含有跨代指针。
  - 第三种“卡精度”所指的是用一种称为“卡表”（Card Table）的方式去实现记忆集 ，这也是目前最常用的一种记忆集实现形式, 卡表就是记忆集的一种具体实现，它定义了记忆集的记录精度、与堆内存的映射关系等
  - 卡表最简单的形式可以只是一个字节数组 ，而HotSpot虚拟机确实也是这样做的
  - 字节数组CARD_TABLE的每一个元素都对应着其标识的内存区域中一块特定大小的内存块，这个内存块被称作“卡页”（Card Page）。一般来说，卡页大小都是以2的N次幂的字节数
  - 一个卡页的内存中通常包含不止一个对象，只要卡页内有一个（或更多）对象的字段存在着跨代
    指针，那就将对应卡表的数组元素的值标识为1，称为这个元素变脏（Dirty），没有则标识为0。在垃
    圾收集发生时，只要筛选出卡表中变脏的元素，就能轻易得出哪些卡页内存块中包含跨代指针，把它
    们加入GC Roots中一并扫描。
- 写屏障
  - 我们已经解决了如何使用记忆集来缩减GC Roots扫描范围的问题，但还没有解决卡表元素如何维护的问题，例如它们何时变脏、谁来把它们变脏等。
  - 何时变脏的答案是很明确的——有其他分代区域中对象引用了本区域对象时，其对应的卡表元素就应该变脏，变脏时间点原则上应该发生在引用类型字段赋值的那一刻
  - HotSpot虚拟机里是通过写屏障（Write Barrier）技术维护卡表状态的
    - 写屏障可以看作在虚拟机层面对“引用类型字段赋值”这个动作的AOP切面
    - 在引用对象赋值时会产生一个环形（Around）通知，供程序执行额外的动作，也就是说赋值的前后都在写屏障的覆盖范畴内。在赋值前的部分的写屏障叫作写前屏障（Pre-Write Barrier），在赋值
      后的则叫作写后屏障（Post-Write Barrier）。HotSpot虚拟机的许多收集器中都有使用到写屏障，但直至G1收集器出现之前，其他收集器都只用到了写后屏障。
    - 应用写屏障后，虚拟机就会为所有赋值操作生成相应的指令，一旦收集器在写屏障中增加了更新
      卡表操作，无论更新的是不是老年代对新生代对象的引用，每次只要对引用进行更新，就会产生额外
      的开销
  - 伪共享问题
    - 处理并发底层细节时一种经常需要考虑的问题，现代中央处理器的缓存系统中是以缓存行（CacheLine）为单位存储的，当多线程修改互相独立的变量时，如果这些变量恰好共享同一个缓存行，就会彼此影响（写回、无效化或者同步）而导致性能降低
    - 为了避免伪共享问题，一种简单的解决方案是不采用无条件的写屏障，而是先检查卡表标记，只有当该卡表元素未被标记过时才将其标记为变脏
- 并发的可达性分析
  - 要知道包含“标记”阶段是所有追踪式垃圾收集算法的共同特征，如果这个阶段会随着堆变大而等
    比例增加停顿时间，其影响就会波及几乎所有的垃圾收集器，同理可知，如果能够削减这部分停顿时
    间的话，那收益也将会是系统性的。
  - 三色标记（Tri-color Marking）
    - 遍历对象图过程中遇到的对象，按照“是否访问过”这个条件标记成以下三种颜色：
      - 白色：表示对象尚未被垃圾收集器访问过。显然在可达性分析刚刚开始的阶段，所有的对象都是
        白色的，若在分析结束的阶段，仍然是白色的对象，即代表不可达。
      - 黑色：表示对象已经被垃圾收集器访问过，且这个对象的所有引用都已经扫描过。黑色的对象代
        表已经扫描过，它是安全存活的，如果有其他对象引用指向了黑色对象，无须重新扫描一遍。黑色对象不可能直接（不经过灰色对象）指向某个白色对象。
      - 灰色：表示对象已经被垃圾收集器访问过，但这个对象上至少存在一个引用还没有被扫描过
    - 如果用户线程与收集器是并发工作, 收集器在对象图上标记颜色，同时用户线程在修改引用关系——即修改对象图的结构，这样可能出现两种后果。
      - 一种是把原本消亡的对象错误标记为存活，这不是好事，但其实是可以容忍的，只不过产生了一点逃过本次收集的浮动垃圾而已，下次收集清理掉就好。
      - 另一种是把原本存活的对象错误标记为已消亡，这就是非常致命的后果了，程序肯定会因此发生错误，
  - 对象消失 (原本应该是黑色的对象被误标为白色)
    - 赋值器插入了一条或多条从黑色对象到白色对象的新引用；
    - 赋值器删除了全部从灰色对象到该白色对象的直接或间接引用。
    - 我们要解决并发扫描时的对象消失问题，只需破坏这两个条件的任意一个即可。由此分别
      产生了两种解决方案：增量更新（Incremental Update）和原始快照（Snapshot At The Beginning，SATB）
      - **增量更新**要破坏的是第一个条件，当黑色对象插入新的指向白色对象的引用关系时，就将这个新
        插入的引用记录下来，等并发扫描结束之后，再将这些记录过的引用关系中的黑色对象为根，重新扫描一次。这可以简化理解为，黑色对象一旦新插入了指向白色对象的引用之后，它就变回灰色对象了。
      - **原始快照**要破坏的是第二个条件，当灰色对象要删除指向白色对象的引用关系时，就将这个要删
        除的引用记录下来，在并发扫描结束之后，再将这些记录过的引用关系中的灰色对象为根，重新扫描一次。这也可以简化理解为，无论引用关系删除与否，都会按照刚刚开始扫描那一刻的对象图快照来进行搜索。

### 经典垃圾收集器

![1625673563340](面经.assets/1625673563340.png)

#### Serial

![1625673967565](面经.assets/1625673967565.png)

- 单线程工作的收集器，
  - 但它的“单线程”的意义并不仅仅是说明它只会使用一个处理器或一条收集线程去完成垃圾收集工作，
  - 更重要的是强调在它进行垃圾收集时，必须暂停其他所有工作线程，直到它收集结束
- 优点
  - 简单而高效
  - 对于内存资源受限的环境，它是所有收集器里额外内存消耗（Memory Footprint)最小的
  - 对于单核处理器或处理器核心数较少的环境来说，Serial收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率

#### ParNew收集器

![1625674095508](面经.assets/1625674095508.png)

- Serial收集器的多线程并行版本
- 目前只有它能与CMS收集器配合工作。
- ParNew收集器在单核心处理器的环境中绝对不会有比Serial收集器更好的效果，甚至由于存在线程
  交互的开销，该收集器在通过超线程（Hyper-Threading）技术实现的伪双核处理器环境中都不能百分
  之百保证超越Serial收集器
- **并行**（Parallel）：并行描述的是**多条垃圾收集器线程之间的关系**，说明同一时间有多条这样的线
  程在协同工作，通常默认此时用户线程是处于等待状态。
- **并发**（Concurrent）：并发描述的是**垃圾收集器线程与用户线程之间的关系**，说明同一时间垃圾
  收集器线程与用户线程都在运行。由于用户线程并未被冻结，所以程序仍然能响应服务请求，但由于
  垃圾收集器线程占用了一部分系统资源，此时应用程序的处理的吞吐量将受到一定影响。



#### Parallel Scavenge收集器

- 新生代收集器，
- 它同样是基于标记-复制算法实现的收集器，
- 也是能够并行收集的多线程收集器
- Parallel Scavenge收集器的目标则是达到一个可控制的吞吐量（Throughput）。
  - 所谓吞吐量就是处理器用于运行用户代码的时间与处理器总消耗时间的比值，
- 垃圾收集停顿时间缩短是以牺牲吞吐量和新生代空间为代价换取的：
  - 系统把新生代调得小一些，收集300MB新生代肯定比收集500MB快，但这也直接导致垃圾收集发生得
    更频繁，原来10秒收集一次、每次停顿100毫秒，现在变成5秒收集一次、每次停顿70毫秒。停顿时间
    的确在下降，但吞吐量也降下来了。



#### Serial Old收集器

![1625674424913](面经.assets/1625674424913.png)

- Serial收集器的老年代版本
- 同样是一个单线程收集器
- 使用标记-整理算法
- JDK 5以及之前的版本中与Parallel Scavenge收集器搭配使用 另外一种就是作为CMS收集器发生失败时的后备预案，在并发收集发生Concurrent Mode Failure时使用

#### Parallel Old收集器

![1625674509576](面经.assets/1625674509576.png)

- Parallel Scavenge收集器的老年代版本，
- 支持多线程并发收集，
- 基于标记-整理算法实现
- 在注重吞吐量或者处理器资源较为稀缺的场合，都可以优先考虑Parallel Scavenge加Parallel Old收集器这个组合

#### CMS收集器

![1625674622673](面经.assets/1625674622673.png)

- CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器,基于标记-清除算法实现
- 四个步骤
  1. 初始标记（CMS initial mark）
     - Stop The World
     - 标记一下GC Roots能直接关联到的对象，速度很快
  2. 并发标记（CMS concurrent mark）
     - GC Roots的直接关联对象开始遍历整个对象图的过程，这个过程耗时较长但是不需要停顿用户线程，可以与垃圾收集线程一起并发运行
  3. 重新标记（CMS remark）
     - Stop The World
     - 修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录
     - 这个阶段的停顿时间通常会比初始标记阶段稍长一些，但也远比并发标记阶段的时间短
  4. 并发清除（CMS concurrent sweep）
     - 清理删除掉标记阶段判断的已经死亡的对象，由于不需要移动存活对象，所以这个阶段也是可以与用户线程同时并发的
- 由于在整个过程中耗时最长的并发标记和并发清除阶段中，垃圾收集器线程都可以与用户线程一起工作，所以从总体上来说，CMS收集器的内存回收过程是与用户线程一起并发执行的
- 优点
  - 并发收集、低停顿
- 缺点
  - **对处理器资源非常敏感**
    - 并发阶段，它虽然不会导致用户线程停顿，但却会因为占用了一部分线程而导致应用程序变慢，降低总吞吐量
    - CMS默认启动的回收线程数是（处理器核心数量+3）/4，
      - 处理器核心数量 > 4 并发回收时垃圾收集线程只占用不超过25%
      - 处理器核心数量 < 4 并发回收时垃圾收集线程 占用比例很高
    - 增量式并发收集器
      - 并发标记、清理的时候让收集器线程、用户线程交替运行，尽量减少垃圾收集线程的独占资源的
        时间，这样整个垃圾收集的过程会更长，但对用户程序的影响就会显得较少一些，
  - **无法处理“浮动垃圾”**, 有可能出现“Con-current Mode Failure”失败进而导致另一次完全“Stop The World”的Full GC的产生
    - 浮动垃圾
      - 在CMS的并发标记和并发清理阶段，用户线程是还在继续运行的，程序在运行自然就还会伴随有新的垃圾对象不断产生，但这一部分垃圾对象是出现在标记过程结束以后，CMS无法在当次收集中处理掉它们，只好留待下一次垃圾收集时再清理掉。这一部分垃圾就称为“浮动垃圾”
    - 由于在垃圾收集阶段用户线程还需要持续运行，那就还需要预留足够内存空间提供给用户线程使用，因此CMS收集器不能像其他收集器那样等待到老年代几乎完全被填满了再进行收集，必须预留一部分空间供并发收集时的程序运作使用
      - JDK 5 : CMS收集器当老年代使用了68%的空间后就会被激活，
      - JDK 6 : CMS收集器当老年代使用了92%的空间后就会被激活，
    - 并发失败
      - 要是CMS运行期间预留的内存无法满足程序分配新对象的需要，就会出现一次“并发失败”（Concurrent Mode Failure）
      - 这时候虚拟机将不得不启动后备预案：冻结用户线程的执行，临时启用Serial Old收集器来重新进行老年代的垃圾收集，但这样停顿时间就很长了
  - 大量空间碎片产生。
    - 基于“标记-清除”算法实现的收集器
    - 空间碎片过多时，将会给大对象分配带来很大麻烦，往往会出现老年代还有很多剩余空间，但就是无法找到足够大的连续空间来分配当前对象，而不得不提前触发一次Full GC的情况
    - -XX：+UseCMS-CompactAtFullCollection
      - CMS收集器不得不进行Full GC时开启内存碎片的合并整理过程，由于这个内存整理必须移动存活对象，（在Shenandoah和ZGC出现前）是无法并发的, 这样空间碎片问题是解决了，但停顿时间又会变长
    - -XX：CMSFullGCsBefore-Compaction
      - 要求CMS收集器在执行过若干次（数量由参数值决定）不整理空间的Full GC之后，下一次进入Full GC前会先进行碎片整理

#### Garbage First收集器

![1625676105996](面经.assets/1625676105996.png)

- 基于Region的内存布局形式
- 提供并发的类卸载的支持
- 希望做出一款能够建立起“停顿时间模型”（PausePrediction Model）的收集器，停顿时间模型的意思是能够支持指定在一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间大概率不超过N毫秒这样的目标

- Mixed GC模式
  - 可以面向堆内存任何部分来组成回收集（Collection Set，一般简称CSet）进行回收，衡量标准不再是它属于哪个分代，而是哪块内存中存放的垃圾数量最多，回收收益最大，
- G1也仍是遵循分代收集理论设计的，但其堆内存的布局与其他收集器有非常明显的差异：
  - G1不再坚持固定大小以及固定数量的分代区域划分，而是把连续的Java堆划分为多个大小相等的独立区域（Region），每一个Region都可以根据需要，扮演新生代的Eden空间、Survivor空间，或者老年代空间。
  - 收集器能够对扮演不同角色的Region采用不同的策略去处理，这样无论是新创建的对象还是已经存活了一段时间、熬过多次收集的旧对象都能获取很好的收集效果
- Humongous区域
  - 专门用来存储大对象。G1认为只要大小超过了一个Region容量一半的对象即可判定为大对象
  - 而对于那些超过了整个Region容量的超级大对象，将会被存放在N个连续的Humongous Region之中，G1的大多数行为都把Humongous Region作为老年代的一部分来进行看待
- 处理思路
  - G1收集器去跟踪各个Region里面的垃圾堆积的“价值”大小，价值即回收所获得的空间大小以及回收所需时间的经验值，然后在后台维护一个优先级列表，每次根据用户设定允许的收集停顿时间（使用参数-XX：MaxGCPauseMillis指定，默认值是200毫秒），优先处理回收价值收益最大的那些Region，
- Region里面存在的跨Region引用对象如何解决?
  - 使用记忆集避免全堆作为GC Roots扫描，但在G1收集器上记忆集的应用其实要复杂很多，它的每个Region都维护有自己的记忆集，这些记忆集会记录下别的Region指向自己的指针，并标记这些指针分别在哪些卡页的范围之内。
  - G1的记忆集在存储结构的本质上是一种哈希表，Key是别的Region的起始地址，Value是一个集合，里面存储的元素是卡表的索引号。这种“双向”的卡表结构（卡表是“我指向谁”，这种结构还记录了“谁指向我”）比原来的卡表实现起来更复杂，同时由于Region数量比传统收集器的分代数量明显要多得多，因此G1收集器要比其他的传统垃圾收集器有着更高的内存占用负担
- 并发标记阶段如何保证收集线程与用户线程互不干扰地运行?
  - 首先要解决的是用户线程改变对象引用关系时，必须保证其不能打破原本的对象图结构，导致标记结果出现错误
    - 三色标记
    - CMS收集器采用**增量更新**算法实现，而G1收集器则是通过**原始快照（SATB）**算法来实现的
  - 垃圾收集对用户线程的影响还体现在回收过程中新创建对象的内存分配上，
    - 程序要继续运行就肯定会持续有新对象被创建，
    - G1为每一个Region设计了两个名为TAMS（Top at Mark Start）的指针，把Region中的一部分空间划分出来用于并发回收过程中的新对象分配，并发回收时新分配的对象地址都必须要在这两个指针位置以上。
    - G1收集器默认在这个地址以上的对象是被隐式标记过的，即默认它们是存活的，不纳入回收范围
  - 如果内存回收的速度赶不上内存分配的速度，G1收集器也要被迫冻结用户线程执行，导致Full GC而产生长时间“Stop The World”。
- 怎样建立起可靠的停顿预测模型?
  - G1收集器的停顿预测模型是以衰减均值（Decaying Average）为理论基础来实现的，在垃圾收集过程中，G1收集器会记录每个Region的回收耗时、每个Region记忆集里的脏卡数量等各个可测量的步骤花费的成本，并分析得出平均值、标准偏差、置信度等统计信息
  - 里强调的“衰减平均值”是指它会比普通的平均值更容易受到新数据的影响，平均值代表整体平均状态，但衰减平均值更准确地代表“最近的”平均状态。
- 四个步骤
  - 初始标记（Initial Marking）：
    - 仅仅只是标记一下GC Roots能直接关联到的对象，
    - 并且修改TAMS指针的值，让下一阶段用户线程并发运行时，能正确地在可用的Region中分配新对象。这个阶段需要停顿线程，但耗时很短，而且是借用进行Minor GC的时候同步完成的，所以G1收集器在这个阶段实际并没有额外的停顿。
  - 并发标记（Concurrent Marking）：
    - 从GC Root开始对堆中对象进行可达性分析，递归扫描整个堆里的对象图，找出要回收的对象，这阶段耗时较长，但可与用户程序并发执行。
    - 当对象图扫描完成以后，还要重新处理SATB记录下的在并发时有引用变动的对象。
  - 最终标记（Final Marking）：
    - 对用户线程做另一个短暂的暂停，用于处理并发阶段结束后仍遗留下来的最后那少量的SATB记录。
  - 筛选回收（Live Data Counting and Evacuation）：
    - 负责更新Region的统计数据，对各个Region的回收价值和成本进行排序，根据用户所期望的停顿时间来制定回收计划，可以自由选择任意多个Region构成回收集，然后把决定回收的那一部分Region的存活对象复制到空的Region中，再清理掉整个旧Region的全部空间。这里的操作**涉及存活对象的移动**，是必须暂停用户线程，由多条收集器线程并行完成的。
  - G1收集器除了并发标记外，其余阶段也是要完全暂停用户线程
    - 考虑到G1不是仅仅面向低延迟，停顿用户线程能够最大幅度提高垃圾收集效率，为了保证吞吐量所以才选择了完全暂停用户线程的实现方案
    - 可以由用户指定期望的停顿时间是G1收集器很强大的一个功能，设置不同的期望停顿时间，可使得G1在不同应用场景中取得关注吞吐量和关注延迟之间的最佳平衡。
- CMS VS G1
  - 可以指定最大停顿时间、分Region的内存布局、按收益动态确定回收集这些创新性设计带来的红利
  - 与CMS的“标记-清除”算法不同，G1从整体来看是基于“**标记-整理**”算法实现的收集器，但从局部（两个Region之间）上看又是基于“**标记-复制**”算法实现，无论如何，这两种算法都意味着**G1运作期间不会产生内存空间碎片**，垃圾收集完成之后能提供规整的可用内存。这种特性有利于程序长时间运行，在程序为大
    对象分配内存时不容易因无法找到连续内存空间而提前触发下一次收集
  - 用户程序运行过程中，G1无论是为了垃圾收集产生的内存占用（Footprint）还是程序运行时的额外执行负载（Overload）都要比CMS要高
    - 虽然G1和CMS都使用卡表来处理跨代指针，但G1的卡表实现更为复杂，而且堆中每个Region，无论扮演的是新生代还是老年代角色，都必须有一份卡表，这导致G1的记忆集（和其他内存消耗）可能会占整个堆容量的20%乃至更多的内存空间；
    - 相比起来CMS的卡表就相当简单，只有唯一一份，而且只需要处理老年代到新生代的引用，反过来则不需要，由于新生代的对象具有朝生夕灭的不稳定性，引用变化频繁，能省下这个区域的维护开销是很划算的
  - 在执行负载的角度上，同样由于两个收集器各自的细节实现特点导致了用户程序运行时的负载会
    有不同，
    - 譬如它们都使用到写屏障，CMS用写后屏障来更新维护卡表；
    - 而G1除了使用写后屏障来进行同样的（由于G1的卡表结构复杂，其实是更烦琐的）卡表维护操作外，为了实现原始快照搜索（SATB）算法，还需要使用写前屏障来跟踪并发时的指针变化情况
    - 相比起增量更新算法，原始快照搜索能够减少并发标记和重新标记阶段的消耗，避免CMS那样在最终标记阶段停顿时间过长的缺点，但是在用户程序运行过程中确实会产生由跟踪引用变化带来的额外负担
    - 由于G1对写屏障的复杂操作要比CMS消耗更多的运算资源，所以CMS的写屏障实现是直接的同步操作，而G1就不得不将其实现为类似于消息队列的结构，把写前屏障和写后屏障中要做的事情都放到队列里，然后再异步处理。











1. JVM垃圾收集每种[算法](https://www.nowcoder.com/jump/super-jump/word?word=算法)，实现方式，各自特点
2. 调用system.gc()一定会发生垃圾收集吗？为什么？
3. 谈一谈你对于垃圾收集机制在实际使用中的理解



###  被重新标记后，垃圾对象就一定会被回收吗？（不一定，清理时仍会进行判断，如重新可达就不会清理）  

###  说说引用计数器

###  知道安全点和安全区域吗 

###  根对象是什么 

 只说了虚拟机栈上对象、静态类变量，实际还有锁持有对象等等 



# Go

##  go协程是怎么实现的。 





# 微服务

##  微服务的意义/优点 

-  还问了一点对Golang协程的了解以及Channel在协程间通信的过程 

##  raft原理，一致性hash[算法](https://www.nowcoder.com/jump/super-jump/word?word=算法) 









# Spring

## Spring AOP实现原理，为什么要使用AOP

## 写一个 Controller，RESTful API，接受两个int 参数，返回相加结果，讲解涉及注解原理

##   谈谈springmvc的事务

（当时记不清了，就说和mysql事务差不多，又谈了事务传播策略，编程式事务和声明式事务）  

##   说说Tranctional注解，注解加在save方法上，方法执行失败时会发生什么  

##   spring bean的生命周期

（记不清了，就细说了下初始化流程，泛谈了存入容器等待调用和destory，提了下三级缓存和对象的提前暴露）

##  Spring，AOP是什么，IOC是什么 

# MySQL

1. MYSQL的事务隔离机制（3种问题，4种隔离机制）

2. MYSQL的A C I D怎样实现的？

3. MVCC原理

4. MYSQL redo_log原理（实现D）,undo_log原理（实现A）

5. sql注入

6. InnoDB和MyISAM的区别

   聚簇索引和非聚簇索引的区别

   MVCC

   非聚簇索引 回表（其实就是考B+树的优势，对叶子节点存数据这边问的很深入，答的不够完整）

   可重复读怎么解决不可重复读问题

##  数据库索引以及为什么要用这种索引 

##  问mysql有哪些存储引擎，你用到什么存储引擎，区别是什么？ 

##  sql优化策略 

##  聚集索引的底层 

## mvcc机制了解嘛

## mysql的表锁有哪些？

## 给了一段sql语句，问会上什么锁？详细解读一下

##  Mysql索引，联合索引，失效，左连接（八股文） 

##  什么是柔性事务 

##  跨库事务如何保证 

## 数据库三大范式

## 事务特性

## 事务隔离级别

##  mysql索引（B+和hash） 

##  聚集索引和非聚集索引 

##  Mysql的主从复制（从数据库依据redolog完成一致性） 

##  binlog和redolog的差异，以及记录写入的先后性（，binlog二进制数据文件，redolog逻辑命令。先后顺序，当时回答binlog先，redolog后，不知对错，面试官没纠正，应该对了） 

##  mysql事务特性（泛谈了ACID，和MVCC） 

##  谈谈锁（sync关键字和ReentrantLock） 

##  sync关键字和ReentrantLock的区别（层级，功能，重量级三个方面） 

##  说说分布式锁

（谈了Redis和Zoo[keep](https://www.nowcoder.com/jump/super-jump/word?word=keep)er的分布式锁实现原理， 



##  可重入锁在过期前续期失败会发生什么

（说了事务回滚和yeid让出） 



##  间隙锁是什么，具体什么时候会加锁

（具体什么时候加锁，这里要把所有情况都说清楚。。 

##  一级索引和二级索引之间是怎么作用的 

##  连接池，一些参数的含义(详细探讨了一下 removeAbandonedTimeout细节) 



## MYSQL的事务隔离机制

 读未提交：一个事务还没提交，它做的变更就能被别的事务看到。读提交：一个事务提交后，它做的变更才能被别的事务看到。可重复读：一个事务执行过程中看到的数据总是和事务启动时看到的数据是一致的。在这个级别下事务未提交，做出的变更其它事务也看不到。串行化：对于同一行记录进行读写会分别加读写锁，当发生读写锁冲突，后面执行的事务需等前面执行的事务完成才能继续执行。 

##  MYSQL的A C I D怎样实现的 

 利用undo log保障原子性。该log保存了事务发生之前的数据的一个版本，可以用于回滚，从而保证事务原子性。 

 利用redo log保证事务的持久性，该log关注于事务的恢复.在重启mysql服务的时候，根据redo log进行重做，从而使事务有持久性。 

 利用undo log+redo log保障一致性。事务中的执行需要redo log，如果执行失败，需要undo log 回滚。 

##  MVCC原理 

 MVCC为多版本并发控制，即同一条记录在系统中存在多个版本。其存在目的是在保证数据一致性的前提下提供一种高并发的访问性能。对数据读写在不加读写锁的情况下实现互不干扰,从而实现数据库的隔离性,在事务隔离级别为读提交和可重复读中使用到。 

 在InnoDB中，事务在开始前会向事务系统申请一个事务ID，该ID是按申请顺序严格递增的。每行数据具有多个版本，每次事务更新数据都会生成新的数据版本，而不会直接覆盖旧的数据版本。数据的行结构中包含多个信息字段。其中实现MVCC的主要涉及最近更改该行数据的事务ID（DBTRXID）和可以找到历史数据版本的指针（DBROLLPTR）。InnoDB在每个事务开启瞬间会为其构造一个记录当前已经开启但未提交的事务ID的视图数组。通过比较[链表]()中的事务ID与该行数据的值与对应的DBTRXID，并通过DBROLLPTR找到历史数据的值以及对应的DBTRXID来决定当前版本的数据是否应该被当前事务所见。最终实现在不加锁的情况下保证数据的一致性 

##  简述redo_log undo_log 

 redo log: 存储引擎级别的log（InnoDB有，MyISAM没有），该log关注于事务的恢复.在重启mysql服务的时候，根据redo log进行重做，从而使事务有持久性。 

 undo log：是存储引擎级别的log（InnoDB有，MyISAM没有）保证数据的原子性，该log保存了事务发生之前的数据的一个版本，可以用于回滚，是MVCC的重要实现方法之一。







# Redis

1. MYSQL分布式锁，Redis分布式锁了解吗
2. Redis的淘汰机制以及过期策略说一下
3. [redis](https://www.nowcoder.com/jump/super-jump/word?word=redis)除了作为中间键缓存还能用于什么功能，说一下你的理解

##  [redis](https://www.nowcoder.com/jump/super-jump/word?word=redis)底层数据结构，跳表，sds，渐进式哈希，[redis](https://www.nowcoder.com/jump/super-jump/word?word=redis)超级详细 

##  [redis](https://www.nowcoder.com/jump/super-jump/word?word=redis)数据结构，底层数据结构 

## 简述Redis的淘汰机制

1.  noeviction：默认禁止驱逐数据。内存不够使用时，对申请内存的命令报错。 
2.  volatile-lru：从设置了过期时间的数据集中淘汰最近没使用的数据。 
3.  volatile-ttl：从设置了过期时间的数据集中淘汰即将要过期的数据。 
4.  volatile-random：从设置了过期时间的数据中随机淘汰数据。 
5.  allkeys-lru：淘汰最近没使用的数据。 
6.  allkeys-random：随机淘汰数据。 

##  简述Redis过期策略 

1.  定期删除，[redis]()默认是每100ms就随机抽取一些设置了过期时间的key，并检查其是否过期，如果过期就删除。因此该删除策略并不会删除所有的过期key。 
2.  惰性删除，在[客户端]()需要获取某个key时，[redis]()将首先进行检查，若该key设置了过期时间并已经过期就会删除。

##  Redis的持久化如何做到的？（RDB+AOF） 

##  RDB具体是如何实现的，RDB生成快照的时候，Redis会阻塞掉吗？

（使用BgSave，fork一个子进程去并行生成快照，不会阻塞） 

##  既然生成快照的中途依然可以执行Redis，那么从节点获取到快照是不完整的，如何同步？

（主从同步，先建立连接，然后命令传播，两个结点中的buffer队列里存储一个offset，差值就是需要同步的值） 

## redis为什么快？

## redis知道多少说多少？

## redis的sorted set

## sorted set底层，越细越好。

##  如何解决Redis宕机后不可用 

## Redis 缓存击穿、缓存穿透和缓存雪崩区别

##  缓存数据时遇到过什么问题吗？

（没反应过来，好久没人问这些了，要求面试官描述详细情景，结果是缓存雪崩和缓存穿透，就说了些八股，随机过期、布隆过滤器、限流和在数据库操作上加分布式锁） 

##   删除缓存数据时，是先删数据库还是先删缓存数据）

（答了先删缓存，面试官纠正：先数据库，否则缓存会在二次查询时恢复









# 设计模式

- 设计模式几大基本原则
- 说一下你理解的几种设计模式

```


开放封闭原则：对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的代码，实现一个热插拔的效果。

单一职责原则：一个类、接口或方法只负责一个职责，降低代码复杂度以及变更引起的风险。

依赖倒置原则：针对接口编程，依赖于抽象类或接口而不依赖于具体实现类。

接口隔离原则：将不同功能定义在不同接口中实现接口隔离。

里氏替换原则：任何基类可以出现的地方，子类一定可以出现。

迪米特原则：每个模块对其他模块都要尽可能少地了解和依赖，降低代码耦合度。

合成复用原则：尽量使用组合(has-a)/聚合(contains-a)而不是继承(is-a)达到软件复用的目的。
```

##  说出单例模式几种实现方式以及区别 

##  单例模式的几种写法以及为啥双重校验锁。 

##  说下volatile底层，保证了什么？ 

##  指令重排是什么？ 

##  数据库怎么保证acid的，底层策略，说下undolog和redolog，next-key locks，那oracle呢？ 

##  一致性哈希 

##  设计一个线程池 

## 45个设计模式



# 算法

堆[排序](https://www.nowcoder.com/jump/super-jump/word?word=排序)+讲过程+复杂度分析（时间、空间） 

搜索二维矩阵 74

- https://leetcode-cn.com/problems/search-a-2d-matrix/

最大子序列和 53

- https://leetcode-cn.com/problems/maximum-subarray/

合并两个有序数组 88

- https://leetcode-cn.com/problems/merge-sorted-array/

二叉树的右视图

- https://leetcode-cn.com/problems/binary-tree-right-side-view/

[剑指 Offer 54. 二叉搜索树的第k大节点](https://leetcode-cn.com/problems/er-cha-sou-suo-shu-de-di-kda-jie-dian-lcof/)

[面试题 02.05. 链表求和](https://leetcode-cn.com/problems/sum-lists-lcci/)

[32. 最长有效括号](https://leetcode-cn.com/problems/longest-valid-parentheses/)

[235. 二叉搜索树的最近公共祖先](https://leetcode-cn.com/problems/lowest-common-ancestor-of-a-binary-search-tree/)

[56. 合并区间](https://leetcode-cn.com/problems/merge-intervals/)

[143. 重排链表](https://leetcode-cn.com/problems/reorder-list/)

[336. 回文对](https://leetcode-cn.com/problems/palindrome-pairs/)

[983. 最低票价](https://leetcode-cn.com/problems/minimum-cost-for-tickets/)

[1143. 最长公共子序列](https://leetcode-cn.com/problems/longest-common-subsequence/)

 股票买卖 

[121. 买卖股票的最佳时机](https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock/)

[1046. 最后一块石头的重量](https://leetcode-cn.com/problems/last-stone-weight/)

[1049. 最后一块石头的重量 II](https://leetcode-cn.com/problems/last-stone-weight-ii/)

 [链表](https://www.nowcoder.com/jump/super-jump/word?word=链表)去重 

[93. 复原 IP 地址](https://leetcode-cn.com/problems/restore-ip-addresses/)

[面试题 02.02. 返回倒数第 k 个节点](https://leetcode-cn.com/problems/kth-node-from-end-of-list-lcci/)

[141. 环形链表](https://leetcode-cn.com/problems/linked-list-cycle/)

[1,null,2,3]建成树，然后中序遍历





 手写快排 

 组合总和 II 

 环[链表](https://www.nowcoder.com/jump/super-jump/word?word=链表)的逻辑判断 

 LeetCode76 

 n*n的矩阵，只能向右或向下移动，从最左上方移动到[最右](https://www.nowcoder.com/jump/super-jump/word?word=最右)下方，把所有的路径输出（

 [二叉树](https://www.nowcoder.com/jump/super-jump/word?word=二叉树)深度（递归+非递归两种方法）、[买卖股票的最佳时机](https://www.nowcoder.com/jump/super-jump/word?word=买卖股票的最佳时机) 

 环形[链表](https://www.nowcoder.com/jump/super-jump/word?word=链表)、手写快排 

 子集 

 [二叉树](https://www.nowcoder.com/jump/super-jump/word?word=二叉树)的右视图 

 字符串转整数 

 旋转矩阵、

翻转[二叉树](https://www.nowcoder.com/jump/super-jump/word?word=二叉树)、

手写快排 

 手撕堆[排序](https://www.nowcoder.com/jump/super-jump/word?word=排序) 

 编辑距离、 

 环状[链表](https://www.nowcoder.com/jump/super-jump/word?word=链表)判断，找出交点 

最长不含重复字符的子字符串、

topk问题 

rand5 实现 rand7 

  dfs，[链表](https://www.nowcoder.com/jump/super-jump/word?word=链表)相交问题 

 K个升序[链表](https://www.nowcoder.com/jump/super-jump/word?word=链表)、滑雪场（dfs）

 有序数组用最快的方法找到重复数>1000的数字序列，直接秒 

 字符串[通配符匹配](https://www.nowcoder.com/jump/super-jump/word?word=通配符匹配)的填空题 

 逆序对 

 翻转[链表](https://www.nowcoder.com/jump/super-jump/word?word=链表) 

 荷兰国旗问题 

 Implement a data structure to support two functions add()/search() efficiently 

Given a string, find out the length of the longest substring which contains at most two distinct characters 

 一个[链表](https://www.nowcoder.com/jump/super-jump/word?word=链表)，先打印顺序奇数位，再逆序打印偶数位 

 计算字符串中的回文子串个数 

https://leetcode-cn.com/problems/simplify-path/

 **NC49** **最长的括号子串** 

 力扣138.复制带随机指针的[链表](https://www.nowcoder.com/jump/super-jump/word?word=链表) 

 [二叉树](https://www.nowcoder.com/jump/super-jump/word?word=二叉树)的锯齿形层序遍历 

[算法](https://www.nowcoder.com/jump/super-jump/word?word=算法)：问了分治和贪心的概念以及具体应用

分治在[排序](https://www.nowcoder.com/jump/super-jump/word?word=排序)[算法](https://www.nowcoder.com/jump/super-jump/word?word=算法)上有哪些应用（快排/归并）

快排和归并在实现上的区别

 力扣210课程表（拓扑[排序](https://www.nowcoder.com/jump/super-jump/word?word=排序)） 

 ①分割回文串
②手写LRU，并讲述原理讲的底层一些，为什么使用双向 [链表](https://www.nowcoder.com/jump/super-jump/word?word=链表)，别的方法实现怎么样
③ rand5()->rand7() 

 手写归并[排序](https://www.nowcoder.com/jump/super-jump/word?word=排序)，手写无限长[链表](https://www.nowcoder.com/jump/super-jump/word?word=链表)取中间结点值，手写链结点以及迭代器 

 无序数组的中位数（[排序](https://www.nowcoder.com/jump/super-jump/word?word=排序)、大顶堆小顶堆） 

 两个100000位的整数，计算相加和 

 寻找数组中重复的数字 

 全排列或者两个有序数组合并 

# 数据结构

 [二叉树](https://www.nowcoder.com/jump/super-jump/word?word=二叉树)的概念？[红黑树](https://www.nowcoder.com/jump/super-jump/word?word=红黑树)又是什么，[红黑树](https://www.nowcoder.com/jump/super-jump/word?word=红黑树)和其他平衡树的区别在哪 

1. [平衡二叉树](https://www.nowcoder.com/jump/super-jump/word?word=平衡二叉树)是什么？用普通二叉搜索树不行吗？ [红黑树](https://www.nowcoder.com/jump/super-jump/word?word=红黑树)和平衡树的区别 

1. 说一下对于树的理解
2. [二叉树](https://www.nowcoder.com/jump/super-jump/word?word=二叉树)，二叉查找树，[红黑树](https://www.nowcoder.com/jump/super-jump/word?word=红黑树)
3. 堆排 快排 
4. LRU 
5. B+ B树 时间复杂度 

#  自我介绍 

- 


# 项目



# LINUX

